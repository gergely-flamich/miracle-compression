Compression without Quantization

Gergely Flamich
Department of Engineering University of Cambridge

This dissertation is submitted for the degree of Master of Philosophy in Machine Learning and Machine Intelligence

St John's College

August 2019

Szüleimnek.

Declaration

I, Gergely Flamich of St John's College, being a candidate for the MPhil in Machine Learning and Machine Intelligence, hereby declare that this report and the work described in it are my own work, unaided except as may be specified below, and that the report does not contain material that has already been used to any substantial extent for a comparable purpose. Gergely Flamich August 2019

Acknowledgements

And I would like to acknowledge ...

Abstract

There has been renewed interest in machine learning (ML) based image compression techniques, with recently proposed techniques beating traditional lossy image codecs such as JPEG, WebP and BPG in perceptual quality on every compression rate. A key advantages of ML algorithms in this field are that a) they can adapt to the statistics of each individual image to increase compression efficiency much better than any hand-crafted method and b) they can be quickly adapted to develop codecs for new media such as lightfield cameras, 360 images, Virtual Reality (VR), where current methdos would struggle and where the develpment of new hand-crafted methods could take years. In this thesis we present an introduction to the field of neural image compression, first through lens of image compression, then through the lens of information theoretic neural compression. We will see how quantization is a fundamental block in the lossy image compression pipeline, and emphasize the difficulties it presents for gradient-based optimization techniques. We review recent influential developments in neural image compression and constrast them with each other and see how each method deals with the issues of quantization. Our approach is different: we propose a compression framework that allows us to forgo quantization completely. We use this to develop a novel image compression algorithm and evaluate its efficiency compare to both classical and ML-based approaches on two of the currently most popular perceptual quality metrics. Surprisingly, with no fine-tuning, we achieve close to state-of-the-art performance on low bitrates while slightly underperforming on higher bitrates. Finally, we present analysis of important characteristics of our method, such as coding time and the effectiveness of our chosen model, and discuss key areas where our method could be improved.

Table of contents
List of figures List of tables 1 Introduction 1.1 Motivation . . . . . . . . . . . . . . . 1.2 Our Contributions . . . . . . . . . . . 1.3 Thesis Outline . . . . . . . . . . . . . 1.4 Theoretical Foundations . . . . . . . 1.5 Notation and Basic Concepts . . . . . 1.6 Image Compression . . . . . . . . . . 1.7 The MDL, Bits-Back and MIRACLE . 1.7.1 MDL Principle . . . . . . . . 1.7.2 Bits-Back Argument . . . . . 1.7.3 MIRACLE . . . . . . . . . . 1.7.4 Our method . . . . . . . . . . xiii xvii 1 1 2 2 3 3 5 7 7 8 9 10 13 13 13 14 15 16 19 20 21 23 23

. . . . . . . . . . .

. . . . . . . . . . .

. . . . . . . . . . .

. . . . . . . . . . .

. . . . . . . . . . .

. . . . . . . . . . .

. . . . . . . . . . .

. . . . . . . . . . .

. . . . . . . . . . .

. . . . . . . . . . .

. . . . . . . . . . .

. . . . . . . . . . .

. . . . . . . . . . .

. . . . . . . . . . .

. . . . . . . . . . .

. . . . . . . . . . .

. . . . . . . . . . .

. . . . . . . . . . .

. . . . . . . . . . .

. . . . . . . . . . .

2

Related Works 2.1 Machine Learning-based Image Compression 2.2 Comparison of Recent Works . . . . . . . . . 2.2.1 Datasets and Input Pipelines . . . . . 2.2.2 Architectures . . . . . . . . . . . . . 2.2.3 Quantization . . . . . . . . . . . . . 2.2.4 Coding . . . . . . . . . . . . . . . . 2.2.5 Training . . . . . . . . . . . . . . . . 2.2.6 Evaluation . . . . . . . . . . . . . .

. . . . . . . .

. . . . . . . .

. . . . . . . .

. . . . . . . .

. . . . . . . .

. . . . . . . .

. . . . . . . .

. . . . . . . .

. . . . . . . .

. . . . . . . .

. . . . . . . .

. . . . . . . .

. . . . . . . .

. . . . . . . .

. . . . . . . .

. . . . . . . .

3

Method 3.1 Dataset and Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . .

xii 3.2 Architectures . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.1 VAEs . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.2 Probabilistic Ladder Network . . . . . . . . . . . . Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3.1 Learning the Variance of the Likelihood . . . . . . . Coded Sampling . . . . . . . . . . . . . . . . . . . . . . . . 3.4.1 Parallelized Rejection Sampling . . . . . . . . . . . 3.4.2 Refinement: Greedy Sampling . . . . . . . . . . . . 3.4.3 Second Refinement: Adaptive Importance Sampling Coding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.5.1 Coding the rejection sampled latents . . . . . . . . . 3.5.2 A note on the Arithmetic Coder . . . . . . . . . . . 3.5.3 Coding the greedy & importance sampled latents . . . . . . . . . . . . . . .

Table of contents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 24 27 31 31 32 34 35 37 38 38 38 39 41 41 42 43 48 49 51 55 57

3.3 3.4

3.5

4 Results 4.1 Experimental Setup . . . . . . . . . . . . . . . . 4.2 Comparison of our method with other algorithms 4.3 Analysis of the contribution of the second level . 4.4 Compression Speed . . . . . . . . . . . . . . . . 5 Conclusion and Future Work References Appendix A Appendix A Appendix B Appendix B: Images

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

List of figures
3.1 a) kodim21.png from the Kodak Dataset. b) A random sample from the VAE posterior. c) Posterior means in a randomly selected channel. d) Posterior standard deviations in the same randomly selected channel. We can see that there is a lot of structure in the latent space, on which the full indepenence assumption will have a detrimental effect. (We have examined several random channels and observed the similarly high structure. We present the above cross-section without preference.) . . . . . . . . . . . . . . . . . . . PLN network architecture. The blocks signal data transformations, the arrows signal the flow of information. Block descriptions: Conv2D: 2D convolutions along the spatial dimensions, where the  ×  × / implies a  ×  convolution kernel, with  target channels and  gives the downsampling rate (given a preceding letter "d") or the upsampling rate (given a preceding letter "u"). If the slash is missing, it means that there is no up/downsampling. All convolutions operate in same mode with mirror padding. GDN / IGDN: these are the non-linearities described in Ballé et al. (2016b). Leaky ReLU: elementwise non-linearity defined as max{, }, where we set  = 0.2. Sigmoid: Elementwise non-linearity defined as 1 . We ran all experiments presented here with  = 196,  = 1+exp{-} 128,  = 128,  = 24. . . . . . . . . . . . . . . . . . . . . . . . . . . . .

26

3.2

28

xiv 3.3

List of figures We continue the analysis of the latent spaces induced by kodim21 from the Kodak Dataset. Akin to Figure 3.1, we have selected a random channel for both the first and second levels each and present the spatial cross-sections along these channels. a) Level 1 prior means. b) Level 1 posterior means. c) Level 1 prior standard deviations. d) Level 1 posterior standard deviations. e) Random sample from the Level 1 posterior. f) The sample from e) standardized according to the level 1 prior. Most structure from the sample is removed, hence we see that the second level has successfully learned a lot of the dependencies between the latents. We have checked cross-sections along several randomly selected channels and observed the same phenomenon. We present the above with no preference. . . . . . . . . . . . . . . . . . . . . . We continue the analysis of the latent spaces induced by kodim21 from the Kodak Dataset. Akin to Figures 3.1 and 3.3, we have selected a random channel for both the first and second levels each and present the spatial crosssections along these channels. a) Level 1 prior means. b) Level 1 posterior means. c) Level 1 prior standard deviations. d) Level 1 posterior standard deviations. e) Random sample from the Level 1 posterior. f) The sample from e) standardized according to the level 1 prior. We observe the same phenomenon, with no significant difference, as in Figure 3.3. We note that while the posterior sample may seem like it has more significant structure than the one in the previous Figure. This is only coincidence; some of the regular PLN's channels contain similar structure, and some of the  -PLN's channels contain more noisy elements. . . . . . . . . . . . . . . . . . . . ladder on kodim21 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Rate-Distorsion curves of several relevant methods. Please see Section 4 for the description of how we obtained each curve. We note that the MSSSIM results are presented in decibels, where the conversion is done using the formula -10  log10 (1 - MS-SSIM(x, x)). The PSNR is computed from the mean squared error, using the formula -10  log10 MSE(x, x). . . . . . . Contribution of the second level to the rate, plotted agains the actual rate. Left: Contribution in BPP, Right: Contribution in percentages. We see that for lower bitrates there is more contribution from the second level and it quickly decreases for higher rates. It is also clear that on the same bitrates, the  -PLN requires less contribution from the second level than regular PLN. ladder on kodim21 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ladder on kodim21 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

30

3.4

33 43

4.3 4.1

44

4.2

4.4 4.5

45 45 46

List of figures 4.6 4.7 4.8 4.9 ladder on kodim21 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ladder on kodim21 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ladder on kodim21 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Coding times of models plotted agains their rates. Left: Regular PLNs. Right:  -PLNs. The striped lines indicate the concrete positions of our models in the rate line. While it seems that there is a linear relationship between rate and coding time, we do not have enough datapoints to conclude this. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

xv 46 47 47

48

List of tables
4.1 haha . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48

Chapter 1 Introduction
1.1 Motivation

There have been several exciting developments in neural image compression recently, demonstrating methods that consistently outperform classical methods such as JPEG, WebP and BPG Toderici et al. (2017), Theis et al. (2017), Rippel and Bourdev (2017), Ballé et al. (2018), Johnston et al. (2018), Mentzer et al. (2018). The first advantage of ML-based image codecs, is that they can adapt to the statistics of each individual image much better than even the best hand-crafted methods. This allows them to code images in much fewer bits, while retaining good perceptual quality. A second advantage is that they are generally far easier to adapt to new media formats, such as lightfield cameras, 360 images, Virtual Reality (VR), video streaming etc. The purposes of compression and the devices on which the encoding and decoding is performed varies greatly, from archiving gigabytes of genetic data for later research on a supercomputer, through compressing images to be displayed on a blog or a news article, to streaming video on a mobile device. Classical methods are usually "one-size-fits-all", and their compression efficiency can severly degrade when attempting to compress media for which they were not designed. Designing good hand-crafted codecs for these is difficult, can take several years, and requires the knowledge of many experts. ML techniques on the other hand allow to create equally or better performing, much more flexible codecs within a few months. The chief limitation of current neural image compression methods is while most models these days are trained using gradient-based optmizers, quantization, a key step in the (lossy) image compression pipeline, is an inherently non-differentiable operation. Hence, all current methods need to resort to "tricks" and approximations so that the learning signal can still be passed through the whole model. A review of these methods will be presented in Chapter 2.

2

Introduction

Our approach differs from virtually all previous methods in that we take inspiration from information theory Rissanen (1986), Harsha et al. (2007) and neural network compression Hinton and Van Camp (1993), Havasi et al. (2018) to develop a general compression framework that allows us to forgo the quantization step in our compression pipeline completely. We then apply these ideas to image compression and demonstrate that our codec achieves close to state-of-the-art performance on the Kodak Dataset Company (1999) with no finetuning of our architecture.

1.2 Our Contributions
The contributions of our thesis are as follows: 1. A comparative review of recent influential works in the field of neural image compression. 2. The development of a machine learning-based general compression framework that forgos the quantization step in the compression pipeline, thus allowing end-to-end optimization of models using of gradient-based methods. 3. A novel image compression algorithm using our framework, that achieves close to state-of-the-art performance on the Kodak Dataset Company (1999) without any finetuning of model hyperparameters. 4. An approximate sampling algorithm for multivariate Gaussian distributions, that can be readily used in our compression framework.

1.3 Thesis Outline
Our thesis begins with an introduction to the field of neural image compression (Section 1.4). We first review concepts in image compression, such as lossless versus lossy compression, the rate-distortion trade-off and linear and non-linear transform coding. We emphasize the fundamental role quantization plays in virtually all previous approaches in image compression. We then shift our focus to information theory, where we introduce the Minimum Description Length (MDL) Principle Rissanen and Langdon (1981) and the Bits-back Argument Hinton and Van Camp (1993). Taking inspiration from these, as well as from Harsha et al. (2007) and Havasi et al. (2018), we develop a general framework for compressing data.

1.4 Theoretical Foundations

3

Next, in Chapter 2 we give a comparative review of recent influential developments in neural image compression. We examine their whole pipeline: the datasets used, their architectures, the "trick" used to circumvent the non-differentiability of quantization, their coding methds, training procedures and evaluation methods. In Chapter 3 we describe our proposed method. We explain our choice of dataset, and preprocessing steps. We give a detailed description of our model and why we ended up choosing it. We then walk the reader through the training procedure, based on ideas from Sønderby et al. (2016), Higgins et al. (2017), Ballé et al. (2018) and Dai and Wipf (2019). Next, we present 3 "codable" sampling techniques, that can be used in our compression framework andpoint out their strengths and weaknesses. Finally, in Chapter 4 we compare our trained models to current compression algorithms, both classical such as JPEG and BPG, and the current state-of-the-art neural methods Ballé et al. (2018). In particulare, we compare these methods by their compression rates for a given perceptual quality as measured by the two most popular perceptual metrics, Peak Signal-toNoise Ratio (PSNR) Huynh-Thu and Ghanbari (2008) and the Multi-scale Structural Similarity Index (MS-SSIM) Wang et al. (2003), and show that we achieve close to state-of-the-art performance, with no fine-tuning of model hyperparameters. We also present some further analysis of our chosen models, to empyrically justify their use, as well as to analyze some of the aspects that were not of primary concern of this work, such as coding speed.

1.4

Theoretical Foundations

As compression is not a standard topic in machine learning, it will be useful to first spend some time establishing the main concepts and familiarize ourselves with the jargon. We first go through some notation that will be used throughout this work. Then we go through a brief introduction to compression in general, and lossy compression and transform coding in particular. Third, we examine the motivating line of research to our project, the MDL framework, the bits-back argument and MIRACLE. Although our work is inspired by and based on earlier work, it differs from them in several significant ways. We will point out these differences throughout.

1.5

Notation and Basic Concepts

It will be useful to clarify some of the notation throughout this work.

4 · Vectors will be denoted by boldface lowercase letters: u, x, ... · Matrices will be denoted by uppercase letters: , , ...

Introduction

· Probability mass functions will be denoted by uppercase letters:  (), (), ... · Probability density functions will be denoted by lowercase letters: (), (), ... · In general, exact/continuous values will be denoted by unannotated letters (e.g.  , w), their quantized counterparts denoted by a hat (  ,w  ) and their approximate counterparts by a tilde (  ,w  ). · () [ ()] denotes the expected value of  () with respect to the mass / density (), i.e.: () [ ()] =  () d(),   where  is the sample space. As  will usually denote  or will be understood from context, it will be omitted, and the integral will be rewritten as () [ ()] =  ()() d.



· [] denotes the Shannon entropy of the random variable  . If  is discrete, then it is defined as -  ( = ) log  ( = ). 
=

If it is continuous, then it will refer to the differential entropy of  , namely - log () d(),

 

where  denotes the support of  . Note: we used the natural logarithm in our definition of entropy, and hence its units are nats. If we used the base 2 logarithm instead, the units would be bits. · KL[ () || () ] denotes the Kullback-Leibler divergence between two distributions and is defined as () KL[ () || () ] = () log . [ () ]

1.6 Image Compression

5

· [   ] denotes the mutual information between random variables  and  and is defined as [   ] = KL[ (, ) || ()() ], where (,  )  (, ) and () and () denote the marginals.

1.6
with

Image Compression

Source Coding From a theoretical point of view, given some source  , a sender and a receiver, compression may be described as the aim of the sender communicating an arbitrary sequence 1 , 2 , ... ,  taken from  to the receiver in as few bits as possible such that the receiver may recover relevant information from the message. If the receiver can always recover all the information from the message of the sender, we call the algorithm lossless, otherwise we call it lossy. At first it might seem non-sensical to allow for lossy compression, and in some domains this is definitely true, e.g. in text compression. However, human's audio-visual perception is neither completely aligned with the range of what can be digitally represented, nor does it always scale the same way. Hence, there is a huge opportunity for compressing media in a lossy way by discarding information with the change being imperceptible for a human observer, while making huge gains in size reduction. Lossy Compression As the medium of interest in lossy compression is generally assumed to be a real-valued vectory x   , such as RGB pixel intensities in an image or frequency coefficients in an audio file, the usual pipeline consists of an encoder   Enc map a point x   to a string of bits and a decoder mapping from bitstrings to reconstruction x  . The  factors of the encoder Enc and  can be understood as a map from  to a finite symbol set  , called a lossy encoder and a map from  to a string of bits called a lossless code Goyal (2001). We will examine both Enc and  in more detail shortly. The decoder then can be thought of as inverting the code first and then using an approximate inverse of Enc to get the reconstruction x  : Dec   -1 . It is important to be able to quantify · the distortion of the compressor: on average, how closely does x  resemble x? · the rate of the compressor: on average, how many bits are required to communicate x? We want this to be as low as possible of course.

6

Introduction

Distortion In order to measure "closeness" in the space of interest  , a distance metric (, )   ×    is introduced. Then, the distortion  is is defined as  = (x ) [(x, x  )]. A popular choice of  , across many domains of compression is the normalized 2 metric or MSE, defined as (x, x )= 1 ( -  )2 ,   


 =  .

It is a popular metric as it is simple, easy to implement and has nice interpretations in both a Bayesian Bishop (2013) and the MDL (Hinton and Van Camp (1993), to be introduced in Section 1.7.1) settings. In the image compression setting, however, the MSE is problematic, since it is optimizing for such metric does not necessarily translate to obtaining pleasantlooking reconstructions Zhao et al. (2015), and hence more appropriate, so-called perceptual metrics were developed. The ones relevant to our discussion are Peak Signal-to-Noise Ratio (PSNR) Huynh-Thu and Ghanbari (2008), Gupta et al. (2011) and the Structural Similarity Index (SSIM) Wang et al. (2004) and its multiscale version (MS-SSIM) Wang et al. (2003). Crucially, these two metrics are not only the most popular, but are also differentiable, which means they lend themselves for gradient-based optimization. Rate We noted above that the code used after the lossy encoder is lossless. To further elaborate, in virtually all cases it is an entropy code Goyal (2001). This means that we assume that each symbol in the representation z = Enc(x) has some probability mass  ( ). A fundamental result by Shannon states that z may not be encoded losslessly in fewer than [z] nats Shannon and Weaver (1998). Entropy codes, such as Huffman codes Huffman (1952) or Arithmetic Coding Rissanen and Langdon (1981) can get very close to this lower bound. We will discuss coding methods further in Sections 3.4 and 3.5. The rate (in nats) of the compression algorithm is defined as the average number of nats required to code a single dimension of the input, i.e. 1  = [z].  Transform Coding The issue with source coding is that coding x might have a lot of dependencies across its dimensions. For images, this manifests on multiple scales and semantic levels, e.g. a pixel being blue might indicate that most pixels around it are blue as the scene is depicting the sky or a body of water; a portrait of a face will also imply that eyes, a nose and mouth are probably present, etc. Modelling and coding this dependence structure in

1.7 The MDL, Bits-Back and MIRACLE

7

very high dimensions is highly non-trivial or perhaps even impossible, and hence we need to make simplifying assumptions about it to proceed. Transform coding attempts to solve the above problem by decomposing the encoder function Enc =    into a so-called analysis transform  and a quantizer . The idea is that to transform the input into a domain, such that the dependencies between the dimensions are removed, and hence they can be coded individually. The decoder inverts the steps of the encoder, where the inverse operation of  is called the synthesis transform Gupta et al. (2011). In linear transform coding,  is an invertible linear trasformation, such as a discrete cosine transformation (DCT), as it is in the case of JPEG Wallace (1992), or discrete wavelet transforms in JPEG 2000 Rabbani and Joshi (2002). While simple, fast and elegant, linear transform coding has the key limitation that it can only at most remove correlations (i.e. firstorder dependencies), and this can severly limit its efficiency Ballé et al. (2016a). Instead, Ballé et al. (2016a) propose a method for non-linear transform coding, where  is replaced by a highly non-linear transformation, and its inverse is now replaced by an approximate inverse, which is a separate non-linear transformation. Both  and its approximate inverse are learnt, and the authors show that with a more complicated transformation they can easily surpass the performance of the much more fine-tuned JPEG codecs. Our work also falls into this line of research, although with signifcant differences, which will be pointed out later.

1.7

The MDL, Bits-Back and MIRACLE

Here we overview the theoretical foundation of our project.

1.7.1

MDL Principle

Our approach is based on the Minimum Description Length (MDL) Principle Rissanen (1986). In essence, it is a formalization of Occam's Razor, i.e. the simplest model that describes the data well is the best model of the data Grünwald et al. (2007). Here, "simple" and "well" need to be defined, and these definitions are precisely what the MDL principle gives us. Informally, it asserts that given a class of hypotheses  (e.g. a certain statistical model and its parameters) and some data  , if a particular hypothesis    can be described with at most () bits and the using the hypothesis the data can be described with

8 at most (  ) bits, then the minimum description length of the data is  = min {() + (  )},


Introduction

(1.1)

and the best hypothesis is the  that minimizes the above quantity. Crucially, the MDL principle can thus be interpreted as telling us that the best model of the data is the one that compresses it the most. This makes Eq 1.1 a very appealing learning objective for optimization-based compression methods, ours included. Below, we briefly review how this has been applied so far and how it translates to our case.

1.7.2 Bits-Back Argument
First, we begin with the bits-back argument, introduced in Hinton and Van Camp (1993), which is a direct application of the above. The main goal of this work was to develop a regularisation technique for neural networks by framing the training of a neural network as a communication problem, where the training input and the fixed network architecture is public, but the weights, the network's output given a particular input, and the training targets are only available to the sender, and the task is to communicate the training targets with minimal bits. Concretely, they train a Bayesian Neural Network, by equipping the weights w with a prior  (w) and a posterior  (w) (parameterized by  and , respectively) and maximize the evidence lower bound (ELBO) given a likelihood (  w):  [log (  w)] - KL[  ||  ]. (1.2)

Given a sufficiently finely quantized likelihood, the minimum description length of the data given this model is  [- log (  w)] Shannon and Weaver (1998), and hence the first term in Eq 1.2 corresponds to -(  ). In their work then, Hinton and Van Camp (1993) show that the second term in Eq 1.2 is equal to -(), which establishes a link between the variational training objective of the BNN and the MDL principle. To do this, the encoder 1. trains the neural network, optimising Eq 1.2. 2. draws a random sample w    (w). This represents a message of  [- log  ] nats. 3. w  is then used to calculate the residuals r between the network's output and the targets.

1.7 The MDL, Bits-Back and MIRACLE

9

4. r is coded with w  and then w  is coded using its prior  . The total length of the message is hence  [- log (  w  )] +  [- log  ]. Once everything has been communicated, the decoder can recover the true training targets, but then they can also run the same training algorithm that the encoder used to then recover the posterior  . This means that the code of w  is "free bits" in the sense that the decoder can recover them exactly given what they already have. Hence, the whole cost of drawing w  should be subtracted from the original cost, yielding  [- log  ] -  [- log  ] = KL[  ||  ] nats. This "recovery" is the namesake for the bits-back argument.

1.7.3

MIRACLE

Inspired by the above idea, Havasi et al. (2018) asked a natural question: is it possible to communicate only the weights of a network at bits-back efficiency? If the above were true, it would give a method for compressing neural networks rather efficiently. It is clear that the coding must be different than it was in Hinton and Van Camp (1993), as their method focused on the regularisation aspect of the KL-divergence and is very inefficient for actual communication of the model parameters. A second, important question that arises in conjunction with the first, natural for compression algorithms: is it possible trade off accuracy of a fixed neural network architecture for better compression rates, and vice versa? Luckily, the answer to both of the above questions is yes, and we shall begin by addressing the latter first. Fix a network architecture, and some data likelihood given a weight set (  w  ). Akin to Hinton and Van Camp (1993), we will actually train a BNN with weight prior (w) and posterior (w). Then, given a budget of  nats, we hope to maximize the following constrained objective:  [log (  w)] subject to KL[  ||  ] < . (1.3)

We can rewrite Eq 1.3 as its Lagranagian relaxation under the KKT conditions Karush (2014), Kuhn and Tucker (2014), Higgins et al. (2017) and get:  (, , ,  , w  ) =  [log (  w  )] - (KL[  ||  ] - ). By the KKT conditions if   0 then   0, hence discarding the last term in the above equation will provide a lower bound for it:  (, , ,  , w  )   (, ,  , w  ) =  [log (  w  )] -  KL[  ||  ]. (1.4)

10

Introduction

Notice, that this is the same as Eq 1.2, but with the addition of the parameter  that will control the regularisation term and eventually the compression cost of the weights. It is also intimately related to the training target of  -VAEs Higgins et al. (2017), except for where they regularise the distributions of activations on a stochastic layer, here the regularisation is for the distributions of weights. Now, to answer the first question, we first need to establish the right setting for the task, which will be another communications problem. Concretely, given a dataset  sampled from a distribution (), and  (w), our trained weight posterior for a given  , what are the bounds on the minimum description length for the posterior ( )? Under some mild assumptions, it can be shown Harsha et al. (2007) that in fact () [( )]  () [KL[  ||  ]], i.e. in this probabilistic setting bits-back efficiency is the best we can hope for. Now, if we make the further assumption that the sender and the receiver are allowed to share a source of randomness (e.g. a random number generator and a seed for it), then a rather tight upper bound can also be derived, also due to Harsha et al. (2007): () [( )]  [  w] + 2 log ([  w] + 1) + (1) (1.5)

where [  w] = () [KL[  ||  ]] is the mutual information between the distribution of datasets and the weights. Note: Hence, tuning  in 1.4 directly controls the rate of the compression algorithm. Eq 1.5 is proven by exposing an algorithm that achieves the postualted coding efficiency, an adaptive rejection sampling algorithm, which we detail in the Appendix A. This turns out to be infeasible in the case of MIRACLE, and instead the authors propose an importance sampling-based approximate sampling algorithm, which is also discussed in further detail in Appendix A. They are important, as we have used both in our project.

1.7.4 Our method
Our project is based upon the simple observation, that the MIRACLE framework may be utilised for compression of any data where in our model a public prior distribution  and a learned  is available and we are allowed to share a source of randomness. We have already noted the extreme similarity of the original BNN training objective to that of the

1.7 The MDL, Bits-Back and MIRACLE

11

 -VAE Higgins et al. (2017), and indeed they are precisely the model that we shall use for the compression of images in our case.

Chapter 2 Related Works
Here we give a brief overview of the history of using machine learning for image compression. Then, we focus on recent advances in lossy image compression and describe and compare their methods to each other as well as ours.

2.1

Machine Learning-based Image Compression

First attempt by Bottou et al. (1998) DjVu focused on segmenting foreground and background in magazines and using K-means clustering to analize and code the background. Neural network based image compression has been theorized about for a long time now Mougeot et al. (1991) Jiang (1999) Toderici et al. (2015) focused only 32x32 thumbnails Image reconstruction through compressive representations Denton et al. (2015), Gregor et al. (2015)

2.2

Comparison of Recent Works

There have been several recent advances in neural network-based compression techniques, most notably Toderici et al. (2015), Ballé et al. (2016b), Toderici et al. (2017), Theis et al. (2017), Rippel and Bourdev (2017), Ballé et al. (2018), Johnston et al. (2018), Mentzer et al. (2018). An interesting commonality between these approaches is that there is very little commonality between them, for a multidue of reasons, on which we hope to shed some lite in this section. Instead of analyzing them in a historical order, we will instead go through the compression pipeline and compare them head-to-head in each compartment separately.

14

Related Works

2.2.1 Datasets and Input Pipelines
Somewhat surprisingly it appears that there is no canonical dataset (yet) for the task at hand, namely a set of high-resolution, variable-sized losslessly encoded colour images, although CLIC CLIC (2018) seems to be an emerging one. Perhaps the reason is that generally in other domains, such as image-based classification cropping and rescaling images can effectively side-step the need to deal with variable-sized images. However, when it comes to compression, if we hope to build anything useful, we must account for this. On the other hand most authors have used the Kodak dataset Company (1999) for testing / reporting results. · Ballé et al. (2016b) trained on 6507 images, selected from ImageNet Deng et al. (2009). They removed images with excessive saturation and since their method is based on dithering, they added unifrom noise to the remaining images to imitate the noise introduced by quantization. Finally, they downsampled and cropped images to be 256 × 256 pixels in size. They only kept images whise resampling factor was 0.75 or less, in order to avoid high frequency noise. · Toderici et al. (2017) used two datasets, first the one they described in Toderici et al. (2015) and the second one (they call the "High Entropy" dataset) was created by first scraping 6 million images from the web, then resizing them to 1280 × 720 pixels. Then, they decomposed these into 32 × 32 pixel tiles and selected the 100 tiles from each with the worst compression ratio under the PNG algorithm. · Theis et al. (2017) used 434 high resolution images from flickr.com under the creative commons license. As flickr store its images as JPEGs, they downsampled all images to be below 1536 × 1536 in resolution and saved them as PNGs in order to reduce the effects of the lossy compression. Then, they extracted several 128 × 128 patches from each image and trained on those. · Rippel and Bourdev (2017) took images from the Yahoo Flickr Creative Commons 100 Million dataset, with 128 × 128 patches randomly sampled from the images. They do not state whether they used the whole dataset or just a subset, neither do they describe further preprocessing steps. · Ballé et al. (2018) scraped  1 million colour JPEG images of dimensions at most 3000 × 5000. They filtered out images with excessive saturation similarly to Ballé et al. (2016b). They also downsampled images by random factors such that the image's height and width stayed above 640 and 1200 pixels, respectively. Finally, they use several randomly cropped 256 × 256 pixel patches extracted from each image.

2.2 Comparison of Recent Works

15

2.2.2

Architectures

This is the most diverse aspect of recent approaches, and so we will only discuss them on a very high level. We took inspiration from most of these papers as well as others, these will be emphasized in Section 3. · Ballé et al. (2016b) Build a relatively shallow autoencoder (5 layers). There are several non-standard techniques they use, however. Firstly, their architecture is fully convolutional, i.e. all linear transformations in their network are convolutions in the encoder and deconvolutions in the decoder. They also downsample after the linear transformations, however, this is not elaborated upon in the work, neither is whether they padded the convolutions and if so how. This leads to the very natural consequence that their latent space and hence the code of an image grows with its size. Secondly they do not use any standard non-linearity or batch normalization. Instead, they propose their own activation function, custom tailored for image compression. These non-linearities are a form of adaptive local gain control for the images, called Generalized Divisive Nor() malization (GDN). At the th layer for channel  at position (, ), for input  (, ), the GDN transform is defined as
(+1)  (, )

=

 (, )
2 () , +  ,, ( (, )) ) ( ()

()

.

(2.1)

Its approximate inverse, IGDN for input  (, ) is defined as
() ()

  (, ) =  (, ) 

(

 + ,

  (, ))   .  ,, (  ) 

()

2

1 2

(2.2)

 , ,, Here, the set , , ,, , ,  are learned during training and fixed at test time. · Toderici et al. (2017) · Theis et al. (2017) define a Compressive Autoencoder (CAE) as a regular autoencoder with the quantization step between the encoding and decoding step. (In this sense, the architectures of Ballé et al. (2016b) and Ballé et al. (2018) are also CAEs.) They mirror pad the input first and then they follow it up by a deep, fully convolutional, residual architecture He et al. (2016). They use valid convolutions and downsample by using a stride of 2. Between convolutions they use leaky ReLUs as nonlinearities,

16 which are defined as  () = max{, },   [0, 1].

Related Works

The decoder mirrors the encoder. When upsampling is required, they use what they term subpixel convolutions, where they perform a regular convolution operation with an increased number of filters, and then reshape the resulting tensor into one with larger spatial extent but fewer channels. · Rippel and Bourdev (2017) use a fully convolutional encoder/decoder pair, downscaling between convolutions. They also add in an additional residual connection from every layer to the last, summing at the end. They call this pyramidal decomposition and interscale alignment, with the rationale behind it being that the residual connections extract features at different scales, and so the latent representations can take advantage of this. · Ballé et al. (2018) extend the architecture presented in Ballé et al. (2016b). In particular, the encoder and decoder remain the same, and they add an additional stochastic layer on top of the architecture. However, it is important to note, that this is not a hierarchical VAE, it resembles instead a probabilistic ladder network Sønderby et al. (2016). The layers leading to the second level are more standard, it is still fully convlutional with downsampling after convolutions, however, instead of GDN they use ReLUs. A slightly strange design choice on their part is since they will wish to force the second stage activations to be positive (it will be predicting a scale parameter), instead of using an exponential or softplus (log(1 + exp{})) activation at the end, they take the absolute value of the input to the first layer, and rely on the ReLUs never giving negative values. We are not sure if this was meant to be a computational saving, as taking absolute values is certainly cheaper then either of the aforementioned standard ways of forcing positive values, or it if it gave better results.

2.2.3 Quantization
As all methods surveyed here are trained using gradient-based methods, a crucial question that needs to be answered is how they dealt with the issue of quantization. This is because encoding an image, quantizing the result and then decoding the dequantized representation is problematic as the quantization step yields 0 derivatives almost everywhere. There are two particular items that need to be circumvented: first, the quantization operation itself,

2.2 Comparison of Recent Works

17

and second, the rate estimator [ (z)]. In the next section, we will see how our method suffers from neither of these issues, as we forego the quantization step altogether. · Quantization Ballé et al. (2016b) Quantize the output of their encoder z as   = [ ], where [] denotes the rounding operation. They model this quantization error as dither, i.e. they replace their quantized latents   by   =  +  ,    (0, 1) .

Rate To model the rate, they also require to learn a distribution over the   s. They assume that the latents are independent, and hence they can model the the joint as a fully factorized distribution. They use linear splines to do this, whose parameters  () they update separately every 106 iterations using SGD to maximize its log likelihood on the latents. · Toderici et al. (2017) · Quantization Theis et al. (2017) also use rounding as their quantization step. However, instead of using uniform noise to model the quantization error, they simply replace the derivative of the rounding operation in the backpropagation chain by the constant function 1:  [] = 1.  This is a smooth approximation of rounding and they report that empirically it gave good results. However, as quantization itself creates an important bottleneck in the flow of information, it is key that only the derivative is replaced and not the operation itself. Rate They note that  (z) =  [- 1 , 1 )
2 2

(z + u) du

for some appropriate density  , where the integral is taken over the centered  dimensional hypercube. Then, they replace the the rate estimator with an upper bound using Jensen's inequality: - log2  (z) = - log2 (z + u) du  - log2 (z + u) du.

 [- 1 , 1 )
2 2

 [- 1 , 1 )
2 2

18

Related Works This upper bound is now differentiable. They pick a Gaussian Scale Mixtures for  , with  = 6 components, with the mixing proportions fixed across spatial dimensions, which gives the negative log likelihood - log2 (z + u) =  log2 
 2 ,  (,, + ,,  0, , ),

,,

where ,  iterate through the spatial dimensions and  indexes the filters. · Quantiazation Rippel and Bourdev (2017) use the following formula:   = 1  2    2

with  = 6. For  = 1 this gives a similar error as rounding, and as  is increased, the quantization gets finer. They do not report how they circumvented the non-differentiability of the quantization step, however, they do cite both Ballé et al. (2016b) and Theis et al. (2017), so our guess is that they used a method from one of these papers. Rate they do not train for the rate-distrotion trade-off directly and in particular omit the rate estimator from the loss. Hence there was no need for them to approximate it to make it differentiable. · Quantization Ballé et al. (2018) the quantization scheme remains the same as in Ballé et al. (2016b), extended to the second stochastic layer as well. Rate As for the rate, they us a non-parametric, fully factorized prior for the second stage: 1 1 (2) (z  (2)  ) =      )   (- , - )) .  ( (  2 2  Then, they model the first stage as dithered zero-mean Gaussians with variable scale depending on the second stage, thereby relaxing the initial independence assumption on the latent space to a more general conditional indepenence assumption Bishop (1998): 1 1 (1) (z  (1)  z  (2) ) =  (   0, 2 )   (- , - )) . (  2 2  Crucially, the rate term in our optimization objective is different from previous approaches, in that in previous approaches the distribution of the codewords was estimated separately. In our case, however, it is an integral part of the optimization process.

2.2 Comparison of Recent Works

19

In Theis et al. (2017), they demonstrate that it is precisely the quantization step that introduces the noisy artifacts into the image, and not the reconstruction procedure. This is fundamentally different from our case, where the majority of the quality degradation comes from the fact that VAEs generally produce blurry reconstructions.

2.2.4

Coding

Another important part of the examined methods is the coding. In particular, an interesting caveat of entropy codes is that they tend to perform slightly worse than the predicted rate, due to neglected constant factors in the algorithm Rissanen and Langdon (1981). Hence, it is always more informative to present results where the actual coding has been performed and not just the theoretical rate reported. All examined works have implemented their own coding algorithms, and we briefly review them here. · Ballé et al. (2016b) Context adaptive binary arithmetic coding (CABAC), they supply new information in raster-scan order, which means it does not improve much over non-adaptive coding, but there might be potential · Toderici et al. (2017) · Theis et al. (2017) used their estimated probabilities (z) and used an off-the-shelf publicly available range coder to compress their latents. · Rippel and Bourdev (2017) treat each bit of their  -bit precision quantized representations individually, because they want to utilize the sparsity of more significant bits. They train a separate binary classifier to predict probabilities for each individual bit based on a set of features (they call it a context) to use in an adaptive arithmetic coder. They further add a regularizing term during training based on the codelength of a batch to match a length target. This is to encourage sparsity for high-resolution, but low entropy images and a longer codelength for low resolution but high entropy images. · Ballé et al. (2018) use a non-adaptive arithmetic coder as their entropy code. As they have two stochastic levels, with the first depending on the second, they have to code them sequentially. For the second level, they get their frequency estimates for z  (2) from the non-parametric prior:
(2) (  )

=

  +1 2
(2)   - 1  2

(2)

(    ) d .

20 Then, on the first level, their probaibilities are given by: (
(1)

Related Works

  ) = (

(2)

(1)



2 )

=

1   +2 (1)    - 1  2

(1)

 (   0, 2 ) d .

2.2.5 Training
· Ballé et al. (2016b) Set out to train for the rate-distrotion trade-off directly, i.e.  = [z  ] + [(x, x  )], where the expectation is taken over training batches. As this is a non-differentiable loss function due to the quantization, they replace the discrete entropy term with a differential entropy term:  =  - log2 ( +    () ) + (x, x ) . [  ]  Since they use MSE as the distance metric, they note that their architecture could be considered as an, albeit somewhat strange, VAE with Gaussian likelihood (x  z  , ) =  (x  x  , (2)-1 1) , mean-field prior (z    1 , ... ,   ) = and mean-field posterior (z   x) = 





(    () )

 (    , 1) ,

where  (    , 1) , is the uniform distribution centered on  of width 1. They train their model using Adam Kingma and Ba (2014), with learning rate decay. They do not state how long they trained their architecture. · Toderici et al. (2017)

2.2 Comparison of Recent Works

21

· Theis et al. (2017) also optimize an approximation of the rate-distrotion trade-off, replacing the rate estimator with its upper bound:  = -[log2 (z + u)] + [(x, x  )]. They performed the training incrementally, in the sense that they masked most latents at the start, such that their contribution to the loss was 0. Then, as the training performance saturated, they unmasked them incrementally. They trained the model with Adam, with a small learning rate decay. · Rippel and Bourdev (2017) use the MS-SSIM metric as the training objective and they use an adversarial Goodfellow et al. (2014) loss use GANs · Ballé et al. (2018) in the same vein as they laid out their VAE-based training objective in Ballé et al. (2016b), the data log likelihood term stays, but now the regularizing term is the KL divergence between the joint posterior (z  (1) , z  (2)  x) and the joint prior (z  (1) , z  (2) ). Here, as due to the dithering assumption, the joint posterior works out to be  (z  (1) , z  (2)  x) = 


 (   (1)  , 1) 

(1)




 (   (2)  , 1) .

(2)

The prior is as explained in the previous section Then, taking the KL between these, the full traning objective works out to be  =  - log2 (    () ) - log (   2 ) + (x, x ) .  2 [  ]  
(2) (1)

(2.3)

Eq 2.3 is very important, as it will be directly translated to our learning objective. They train 32 models, half using the architecture from Ballé et al. (2016b) and half using the current one, half optimized for MSE and half of MS-SSIM, with 8 different  s. They use Adam to optimize their model, and they report that neither batch normalization nor learning rate decay gave better results. The former they attribute to GDN.

2.2.6

Evaluation

· Ballé et al. (2016b)

22 · Toderici et al. (2017) · Theis et al. (2017) · Rippel and Bourdev (2017) · Ballé et al. (2018)

Related Works

Notably, all previous VAE-based approaches have addressed the non-differentiablility of quantization indirectly. Theis et al. (2017) use an approximation for the derivative of the rounding operator and optimize an upper bound on the error term introduced by the quantiztion. In Ballé et al. (2016b),Ballé et al. (2018) they model the quantizer by adding uniform noise to the samples

Chapter 3 Method
In this section we describe the models we have tried. We present it in a similar way as the layout of Section 2.2. Within a section we persent ideas we tried in chronological order At a high level, our strategy for compressing images is going to be as follows: 1. Pick an appropriate VAE architecture for image reconstruction. (Section 3.2) 2. Train the VAE on a reasonably selected dataset for this task. (Section 3.1) 3. Once the VAE is trained, we can compress an image by using the methods proposed for MIRACLE Havasi et al. (2018). Concretely, for an image x, we can use the latent posteriors (z  x) and priors (z) for our coded sampling algorithm. (Section 3.4) 4. We may consider to use entropy codes to further increase the efficiency of our method. (Section 3.5) A rather pleasing aspect of this is the modularity that is allowed by the removal of quantization from the training pipeline: our method is reusable with virtually any regular VAE architecture, which opens up the possibility of creating efficient compression algorithms for any domain where a VAE can be used to reconstruct the objects of interest. (More generally, any method where we can obtain a prior and approximate posterior on the latent representaion will do.)

3.1

Dataset and Preprocessing

We trained our image on the CLIC 2018 dataset CLIC (2018), as it seemed sufficiently extensive for our project, as well as "bad" images have been filtered out, which reduced the amount of preprocessing required on our side. A further advantage is that we can compare

24

Method

our results against the constestants, although it is not obvious how representative this comparison is, compared to the results reported in papers. The dataset contains high-resolution PNG encoded photographs, namely 585 in the training set and 41 photos in the validation set. The test set is not publicly available, as it was reserved for the competition. To make training tractable, similarly to previous works, we randomly extracted 256 × 256 pixel patches from each image. The number of patches  was based on their size of the image, according to the formula   × ,  ( , ) =  ×   256 256  where  ,  are the width and height of the current image, respectively and  is an integer constant we set. We used  = 15, which yielded us a training set of 93085 patches. We note that all image data we used for learning was in RGB format. It is possible to achieve better compression rates using the YCbCr format Ballé et al. (2016b), Rippel and Bourdev (2017), however, for simplicity's sake as well as due to time constraints we leave investigating this for later work.

3.2 Architectures
In this section we describe the various architectures that we experimented with. The basis of all our architectures were inspired by the ones used in Ballé et al. (2016b) and Ballé et al. (2018). In particular, we use the General Divisive Normalization (GDN) layer for encoding and its approximate inverse, the IGDN layer for decoding Ballé et al. (2015) Ballé et al. (2016b).

3.2.1 VAEs
As a baseline, we started by attempting to replicate the exact architecture presented in Ballé et al. (2016b), but with a Gaussian prior and posterior instead. Although they do not describe it, we chose mirror padding, as it seems to be the standard for this task Theis et al. (2017). Luckily, the most error-prone part, the implementation of the GDN and IGDN layers was already available in Tensorflow1 . While VAEs are by now fairly standard and we assume that the reader is at least somewhat familiar with them, our later models build on them and are non-standard, hence it will be

3.2 Architectures

25

useful to briefly go over them and introduce notation that we will extend in the following sections. In a regular VAE, we have a first level encoder, that is given some input x predicts the posterior  (1) (z(1)  x) =  (z(1)   ,(1) (x),  ,(1) (x)) , where  ,(1) (x) = (   )(x) predicts the mean and  ,(1) (x) = (exp    )(x) predicts the standard deviation of the posterior. Here  is a highly nonlinear mapping of the input, in reality these correspond to several layers of neural network layers. Notice that  is shared for the two statistics. Then,  and  are custom linear transformations, and finally we take    (1) . the exponential of    to force the standard deviation to be positive. We sample z(1) The first level prior is usually assumed to be a diagonal Gaussian (1) (z(1) ) =  (z(1)  0,  ) . Finally, the first level decoder predicts the statistics of the data likelihood, (x  z(1) ). A note on the latent distributions We have chosen to use Gaussian latent distributions due to their simplicity, as well as their extenesibility to PLNs (see Section 3.2.2). On the other hand, we note that Gaussians are inappropriate, as it has been shown that the filter responses of natural images usually follow a heavy-tailed distribution, usually assumed to be a Laplacian Jain (1989), as used directly in Zhou et al. (2018), but can also be approximated reasonably well by Gaussian Scale Mixtures Portilla et al. (2003), as adopted by Theis et al. (2017). While it would be interesting to investigate incorporating these into our model, as they do not extend trivially to our more complex model settings (in particular PLNs, as we formulated here requre the latent posterior distribution's family to be self-conjugate), we leave this for future work. Data Likelihood and Training Objective As is standard for VAEs, we aim to maximize the the (weighted) Evidence Lower Bound (ELBO): (1) [log (x  z(1) )] -  KL[  (1) || (1) ]. (3.1)

As the latent posterior and prior are both Gaussians, the KL can be computed analytically, and there is no need for a Monte Carlo estimation. A popular and simple choice for the
1

https://www.tensorflow.org/api_docs/python/tf/contrib/layers/gdn

26

Method

Fig. 3.1 a) kodim21.png from the Kodak Dataset. b) A random sample from the VAE posterior. c) Posterior means in a randomly selected channel. d) Posterior standard deviations in the same randomly selected channel. We can see that there is a lot of structure in the latent space, on which the full indepenence assumption will have a detrimental effect. (We have examined several random channels and observed the similarly high structure. We present the above cross-section without preference.)

3.2 Architectures

27

likelihood to be chosen a Gaussian, in which case the expectectation of the log-likelihood corresponds to the mean squared error between the original image and its reconstruction. This also corresponds to optimizing for the PSNR as the perceptual metric. However, it has been shown that PSNR correlates badly with the HVS's perception of image quality Girod (1993) Eskicioglu et al. (1994). This is mainly because an MSE training objective is tolerant to small deviations regardless of the structure in the image, and hence this leads to blurry colour patch artifacts in low-textured regions, which the HVS quickly picks up as unpleasant. A thorough survey of different training objectives for image reconstruction was performed by Zhao et al. (2015). As far as we are aware, they were also the first to propose MS-SSIM as a training objective as well. However, they also show another interesting result: Mean Absolute Error (MAE) already significantly reduces and in some cases completely removes the unpleasant artifacts introduced by MSE. This is because MAE no longer underestimates small deviations, at the cost of somewhat blurrier edges, which MSE penalized more. The MAE corresponds to a diagonal Laplacian log-likelihood with unit scale, which is what we ended up using. This results in efficient training (an MS-SSIM training loss is very expensive to compute) as well as it will enable us for a further enchancement, see Section 3.3.1. Concretely, our likelihood is going to be  ),  ) , (x  z(1) ) =  (x   ,(1) (z(1) where  ,(1) is the reverse operation of  ,(1) . (3.2)

3.2.2

Probabilistic Ladder Network

As mentioned in Ballé et al. (2018), their scale hyperprior architecture closely resembles a probabilistic ladder network (PLN), as defined by Sønderby et al. (2016). We quickly present here a brief overview of them. In order to understand PLNs, we start by a slightly simpler model family: hieararchical VAEs (H-VAEs). For simplicity's sake, we consider two stochastic level H-VAEs and PLNs only, the ideas here extend trivially to more stochastic levels. On a basic level, we will be merely just stacking VAEs on top of each other. Hence, for reference we define level 0 as the input - output layer pairs. The  is sampled, we use it to predict the statistics of the To get a 2-level H-VAE, once z(1) second level posterior  ),  ,(2) (z(1)  )) ,  (2) (z(2)  z(1) ) =  (z(2)   ,(2) (z(1)

28

Method

Fig. 3.2 PLN network architecture. The blocks signal data transformations, the arrows signal the flow of information. Block descriptions: Conv2D: 2D convolutions along the spatial dimensions, where the  ×  × / implies a  ×  convolution kernel, with  target channels and  gives the downsampling rate (given a preceding letter "d") or the upsampling rate (given a preceding letter "u"). If the slash is missing, it means that there is no up/downsampling. All convolutions operate in same mode with mirror padding. GDN / IGDN: these are the non-linearities described in Ballé et al. (2016b). Leaky ReLU: elementwise non-linearity defined as max{, }, where we set  = 0.2. Sigmoid: Ele1 . We ran all experiments presented here with mentwise non-linearity defined as 1+exp {-}  = 196,  = 128,  = 128,  = 24.

3.2 Architectures

29

 ) and  ,(2) (z(1)  ) are analogous to their first level counterparts. Next the where  ,(2) (z(1)    (2) . The second level prior (2) (z(2) ) is now the diagonal second level is sampled z(2)  : unit-variance Gaussian, and the first level priors' statistics are predicted using z(2)  ),  ,(2) (z(2)  )) . (1) (z(1)  z(2) ) =  (z(1)   ,(2) (z(2)  as before Sønderby et al. (2016). The data likelihood's mean is predicted using z(1) The issue with H-VAEs is that the flow of information is limited by the bottleneck of the final stochastic layer. PLNs resolve this issue by allowing the flow of information between lower levels as well. To arrive at them, we make the following modification to our H-VAE: first, once  (1) is known, instead of sampling it immediately, we instead use its mean to predict the statistics of the second level posterior:  x ),  ,(2) (  x )) ,  (2) (z(2)  z(1) ) =  (z(2)   ,(2) (    (2) is sampled. The first level prior (1) is calculated as where  x =  ,(1) (x). Now, z(2) before. Finally, we allow the flow information on the first level by combining  (1) and (1) , inspired by the self-conjugacy of the Normal distribution in Bayesian inference2 :  (z
(1) (1)

 z , x) = 

2

(

z

(1)

|

 -1  +  -1 x  z(1) z(1) x
-1  -1 x +  z(1)

,

1 -1  -1 x +  z(1) )

   (1) (z(1)   z2  , x), and predict the mean of the likelihood using it. We sample z(1) The reason why H-VAEs and PLNs are more powerful models than regular VAEs, is because regular VAEs make an independence assumption between the latents to make the model tractable to compute, while H-VAEs and PLNs relax this assumption to a conditional indpendence assumption. This is the same way Ballé et al. (2018). Finally we need to update the regularizing term of the ELBO to incorporate the joint posterior and priors over the latents. Luckily, we can break it up as KL[ (z(1) , z(2)  x) || (z(1) , z(2) ) ] = KL[ (z(2)  x) || (z(2) ) ]+KL[ (z(1)  z(2) , x) || (z(1)  z(2) ) ], which we can now compute analytically again.
We note that the formula we used is the actual combination rule for a Gaussian likelihood and Gaussian prior. The formula given in Sønderby et al. (2016) is slightly different. We are not sure if it is a typo or it is what they actually used. We found our combination rule worked quite well in practice.
2

30

Method

Fig. 3.3 We continue the analysis of the latent spaces induced by kodim21 from the Kodak Dataset. Akin to Figure 3.1, we have selected a random channel for both the first and second levels each and present the spatial cross-sections along these channels. a) Level 1 prior means. b) Level 1 posterior means. c) Level 1 prior standard deviations. d) Level 1 posterior standard deviations. e) Random sample from the Level 1 posterior. f) The sample from e) standardized according to the level 1 prior. Most structure from the sample is removed, hence we see that the second level has successfully learned a lot of the dependencies between the latents. We have checked cross-sections along several randomly selected channels and observed the same phenomenon. We present the above with no preference.

3.3 Training

31

3.3

Training

Sønderby et al. (2016) give two key advices on training PLNs: · use batch normalization Ioffe and Szegedy (2015) · and use a warmup on the coefficient of the KL term in the loss. Concretely, given a target coefficient 0 , the actual coefficient they recommend should be () = min {  , 1 × 0 ,  }

where  is the number of current batch and  is the warmup period. We ended up not utilizing point 1, due to an argument of Ballé et al. (2018), namely that GDN already performs a similar kind of normalization as BN, and in the same way as it did not impact their training results, we did not expect it to do for us either.

3.3.1

Learning the Variance of the Likelihood

As we have noted, the reconstructions are blurry. A solution offered by Dai and Wipf (2019) is to introduce a new parameter  to the model, that will be the scale of the data likelihood. In our case, since we are using a Laplace likelihood, we will have  , )) . (x  z(1) ) =  (x   ,(1) (z(1) In the case of a Gaussian,  would be the variance of the distribution. Then, it is suggested that instead of predicting gamma (i.e. using a heteroscedastic model), or setting it as a hyperparameter, we learn it. In Dai and Wipf (2019) this is used in conjunction with another novel technique to achieve generative results with VAEs that are competitive with state-of-the-art GANs. In this work however, as the second technique is more irrelevant to us, we focus on learning  only. Let us examine this concept a bit more: let  be the (original) log-likelihood term with unit scale, and  the (original) regularizing term, already multiplied by our target coefficient  . Then, our new loss is going to be 1  =  + .  Multiplying this through by  does not change the minimizer of the expression, but we get the new loss  =  + .

32

Method

In Dai and Wipf (2019) it is shown that if  is learned, then it is always true that   0 as   . This means, that if we set some target  , and use   = max{,  }, as the scale of the data likelihood, we actually get a dual effect to the warmup recommended by Sønderby et al. (2016), but the scaling is automatic!

3.4 Coded Sampling
Once the appropriate network has been trained, this means that for any image x we are able to produce a latent posterior (z  x) and prior (z). Hence, as alluded to in Section 1.7.3, we can use the MIRACLE algorithm to code x in approximately KL[  ||  ] nats! The question is how MIRACLE can be adapted to our setting. There are several key differences in our situation compared to the setting in Havasi et al. (2018): · Variable sized latent space: The original method has been developed for compressing the weight distribution of a BNN, whose dimension is fixed. In our case, due to our fully convolutional architecture, our rendition of the algorithm must be able to adapt gracefully to a range of latent space dimensions which can be as much as 4 magnitudes different for each other. · Different posterior effects: as our latent spaces will carry much more information about the topological nature of the coded image, the distribution of informative versus non-informative posteriors, and their properties will be different from the original setting, and we will need to adapt to these effects. · Practicality / Resource efficiency: Since the original method has been proposed to compress neural networks after they have been trained, the algorithm was proposed with people who have sufficient resources to train them in mind. In particular, the original method incorporates several rounds of incremental retraining during compression to guarantee their efficiency, which might require several GPU hours to complete. As our aim in this work to present a practical, universal compression algorithm, we must also design our method with a much broader audience in mind. Though we still assume the presence of a GPU, our requirements and coding times are much less harsh than that of the original work. In the rest of this section we present the ways we have attempted to address the above points.

3.4 Coded Sampling

33

Fig. 3.4 We continue the analysis of the latent spaces induced by kodim21 from the Kodak Dataset. Akin to Figures 3.1 and 3.3, we have selected a random channel for both the first and second levels each and present the spatial cross-sections along these channels. a) Level 1 prior means. b) Level 1 posterior means. c) Level 1 prior standard deviations. d) Level 1 posterior standard deviations. e) Random sample from the Level 1 posterior. f) The sample from e) standardized according to the level 1 prior. We observe the same phenomenon, with no significant difference, as in Figure 3.3. We note that while the posterior sample may seem like it has more significant structure than the one in the previous Figure. This is only coincidence; some of the regular PLN's channels contain similar structure, and some of the  -PLN's channels contain more noisy elements.

34

Method

3.4.1 Parallelized Rejection Sampling
Our initial goal was to perform parallelization in a way that preserves image quality, i.e. draws exact samples and also achieves the postulated upper bound. As mentioned earlier, the rejection sampling algorithm given in Harsha et al. (2007) achieves this, and hence it served as a good baseline. As pointed out in Havasi et al. (2018), however, this algorithm quickly becomes intractable once we leave the univariate setting. Fortunately, as we are working with (conditional) independence assumptions between the latents, sampling each dimension exactly by itself and then aggregating them will also give an exact multivariate sample. Algorithm 1 Parallelized, bit-budgeted rejection sampling procedure Rej-Sampler(,  , ,       )   dim( ) ,0 ()  0    ,  = 1, ... .  ,0  0,  = 1, ... .  = 0  {0, 1}  Keeps track of whether a dimension has been accepted or not  = 0    Sample we are "building"   = -1    MIRACLE index vector for each dimension for   1, ... 2 do for   1, ...  do if  = 1 then Skip end if , ()  min  () - ,-1 (), (1 -  ,-1 ) ()    , ()  ,-1 () + , ()  ,   , () , ( )  Draw    (0, 1) if  < , ( ) then   1       end if end for end for return ,  end procedure
, () (1- , ) ()

 Indicate we accepted the sample

We modify Algorithm 4 in two ways to make it more efficient. While their algorithm is Las Vegas, i.e. it always gives the right answer, but in random time, this can get very

3.4 Coded Sampling

35

inefficient if we have a few dimensions with very high KL. Instead, to circumvent this issue and fix the runtime of the algorithm, by allocating a bit budget  to each dimension, and only allowing 2 samples to be examined. If the sample is accepted within these draws, then we carry on with their sample indices. The dimensions where every sample was rejected, we sample from the true target, and quantize the samples to 16 bits. We then communicate these quantized samples. The concrete details can be seen in Algorithm 1. Issues Sadly, sampling each dimension individually leads to an (1) cost per dimension, as we will produce an index for each dimension, as opposed to one index for the whole multivariate distribution. In other words, the length of the concatenated indices will be much longer than the single index that would be produced by rejection sampling the whole distribution. In Section 3.5.1 we discuss how we reduced these costs using arithmetic coding.

3.4.2

Refinement: Greedy Sampling

Algorithm 2 Greedy sampler procedure Greedy-Sampler(, ,   ,  2  , , 1 , ...  )  0 z0  Initialize the sample  = ()  Initialize the index set to an empty list for  = 1, ... ,  do    Draw s,   (  ,  ) for  = 1, ... ,   c, = z-1 + s,   arg maxc, {log (c, )} z  Create new sample   arg max {log (c, )}  Store the index of the shard sample that we used Append  to  . end for  ,  return z end procedure To solve the issue of dimensionality, we would need a way to code the sample from the whole multivariate distribution. One way of doing this, as we have seen, was using the indpendence assumption of the latents and break it up per dimension. But there is another way. The strategy will be to progressively "build" a reasonable sample from the posterior. A well known fact about Gaussian distributed random variables is that they are closed under 2 2 addition. Concretely, if    (()  ,  ),    (()  ,  ), then
2 2  +    (()  +  ,  +  ).

36

Method

Assuming that the normals are diagonal multivariate Gaussians, the extension of the above to it is straight forward. Using this simple fact, we may now do the following: pick an integer,  , that we will call the number of shards. Then, given the prior (z(1)   ,  2 ), we can break 2   it up into  equal shards  (z(1)   ,  ). Now, for each individual shard, we may allocate a bit budget  . Then, we draw 2 samples from each shard. Note, if we assign a different, but preagreed sequence of random seeds to each shard, then each sample can be coded in  = 0. Now, draw 2 samples s1, from the first shard  bits. Start with an initial sample z0  + s1, and calculate their log-likelihoods under 1 and create "candidate samples"c1, = z0  = arg maxc1, log (c1, ), and repeat until we reach z  , at which the target. Finally, set z1 point we return it. Then the returned vector will be approximately from the target. More precisely, this is a "guided" random walk, where we bias the trajectory towards the median of the distribution. The algorithm is described in more detail in Algorithm 2. A note on implementation We note that empirically the greedy sampler is sensitive to small variances on the priors. To ameliorate this, we standardize the prior, and scale the posterior according to the standardization, i.e. we set
2  -    (z  x) =  z| , 2 ,  (  )  2 2 where  ,  are the statistics of the original prior and  ,  are the statistics of the original  posterior. We communicate the approximate sample z from   instead of  . This is not problematic, as Gaussian distributed random variables are closed under linear transformations, i.e. given    (, 2 ), we have

 +  =    ( + ,  2 2 ) . Hence, the decoder may recover an approximate sample from  , by calculating z =  z + . Issues While the greedy sampler makes sampling efficient and tractable from the posterior, it comes at the cost of reduced sample quality. In particular, it gives blurrier images. This also means that if we use a PLN to compress an image and we use the greedy technique to code the latents on the second level, the first level priors' statistics derived from the biased sample will be off, and (z(1)  z(2) , x)(z(1)  z(2) ) will be higher. We have verified empirically, that while using a biased sample on the second level does not degrade image quality (possibly due to the noise tolerance of VAEs), it does significantly increase the compression size (by

3.4 Coded Sampling

37

a factor of 1.2 - 1.5) of the first level, which is very significant. This motivated the final sampling algorithm presented here, only used on the second level of our PLNs.

3.4.3

Second Refinement: Adaptive Importance Sampling

Algorithm 3 Adaptive Importance Sampler procedure Adaptive-Importance-Sampler(, , ,  , ,       )   ()  Group sizes   KL[  ||  ] = 1, ...   Get KLs for each dimension   Where( >  )  Outlier indices in the vector Sample      Quantize(O)     Target distribution restricted to the dimensions defined by  .      Remove outlier dimensions   0  Current group size   0  Current group KL  for   1, ... dim( ) do if  +  >  or  + 1 >  then Append  to       1 else    +     + 1 end if end for Append  to   Append the last group size  = ()  Importance samples for the groups  = ()  MIRACLE sample indices   0  Current group index for  in  do  Now importance sample each group ,   Importance-Sample(+ , + ,       ) Append  to  Append  to  end for return I, S end procedure The adaptive importance sampler uses the importance sampler described in Algorithm 5. The idea is to use the block based importance sampling, as proposed in Havasi et al. (2018), however, unlike them we allocate the block sizes dynamically. In particular, we set

38

Method

a bit budget  per group, a maximum group size  and an individual limit  . We first begin by discarding individual dimensions where the KL is larger than  . Then, we flatten the remaining dimensions in raster-scan order and iteratively add dimensions into the current group so long as the total KL of the group reaches either the bit budget  or the number of dimensions added to the group reaches . At this point we start with a new group. Once the whole vector has been partitioned, we importance sample each group using Algorithm 5. The removed dimensions are sampled directly from the posterior and then the samples are quantized to 16 bits. The complete algorithm can be seen in Algorithm 3. For ease of referral, since we would perform Adaptive Importance Sampling on the second level, followed by the Greedy Sampling on the first, We will refer to this way of sample coding as IS-GS.

3.5 Coding
3.5.1 Coding the rejection sampled latents
Simply writing the indices of the individual dimensions given by the rejection sampler would be very inefficient, because without additional assumptions the way to uniquely decode them would be to block code them (i.e. code all indices in 8 bits, say). This would however would add an (1) cost per dimension on the coding length, which is very undesirable. Hence, we implemented a simple non-adaptive arithmetic coder Rissanen and Langdon (1981) to compress the indices even further. The probabilities for the symbol table have been estimated by encoding the entire training set and using the empirical probability distribution. For unseen indices we used Laplace smoothing. In particular, given the empirical distribution of the sample indices  , the probability distribution used is  (1 - ) ()   () =      if    otherwise

where  is the allowed index set, in our case since we allocated  = 8 bits for each individual dimension,  = {0, ... 255}. Since we quantized the outliers to 16 bits,  = 216 - 28 . We found that choosing   0.01 worked reasonably well.

3.5.2 A note on the Arithmetic Coder
While for small alphabets the naive implementation of the arithmetic coder is very fast, the decoding time actually grows as (| |) in the size of the alphabet. In particular, decoding

3.5 Coding

39

the arithmetic code of a reasonably large image would take up to 30 minutes using the naive implementation. The inefficiency is due to a bottleneck where we need to find in a partition of [0, 1] into which partition the currently coded interval fits. In the naive implementation this is simply determined using a linear search over the partitions. This can be made more efficient by using height-balanced binary search trees (BSTs). In particular, we need to find the least upper bounding item in the BST for the given point, which can be done in (log2 | |). Using this additional trick, we can decode large images' code in a few seconds. In particular we implemented an AVL-tree to serve as the BST, which is always guaranteed to be height balanced Adel'son-Vel'skii and Landis (1962).

3.5.3

Coding the greedy & importance sampled latents

Importance Sampler For the importance sampler, we note that the inefficiency of block coding the sample index goes away, as the (1) cost is now shared across the dimensions and is going to negligible, as well as estimating empirical probabilities for indices would have been very costly and would not have increased efficiency significantly. On the other hand, we also have to communicate the groups as well. We note, that instead of communicating group indices, we can instead order the latents and communicate consecutive group sizes, from which the indices can be easily reconstructed, but each group's size takes up at most  log2  bits. We also note that in the best case we will have  2  groups, but probably more (where 2 is the dimensionality of the second level). This means that there is still a huge inefficiency here still, however so long as 2 is sufficiently small, compared to the codelength for the indices, it will be manageable. We also note that the group size is likely to be biased towards higher values, and hence building an empyrical distribution them and arithmetic coding the sequence could lead to a big reduction in the inefficiency, however, we found this not to be too important to focus on. Greedy Sampler It is easy to see that the greedy sampler is already as efficient as it could be. The sample indices for each shard are as compactly represented, as we expect the maximum of the samples to be uniformly distributed. Hence, the only other things that needs to be coded is the number of shards and the number of samples for each shard for the cumulative sample to be decodable. Hence, for the greedy sampler we just wrote the indices straight to the binary file.

Chapter 4 Results
In this section we detail how we setup and empiricall show the correctness and the efficiency of our model. We compare our results against JPEG, the most widely used lossy compression method Bull (2014), and the current state-of-the-art, the results of Ballé et al. (2018)1 . Zhao et al. (2015) Note: All experiments were run on a GeForce GTX 1080 GPU.

4.1

Experimental Setup

As we based our models on that of Ballé et al. (2016b) and Ballé et al. (2018), we mirror a lot of their training setup as well (See Section 3.1 for the dataset and preprocessing). We trained all our models with Adam with a starting learning rate of 0 = 3 × 10-5 and trained all of our models for 20 epochs or equivalently, approximately 200,000 iterations.. We used a smooth exponential learning rate decay schedule except in the case of the  -VAEs, according to the formula  () = 0 ×   . Where  is the decay rate,  is the decay step size and  is the current batch number. We found  = 0.96 and  = 1500 worked well for our experiments. We note, however, that we did not notice signifcant performance gains by using this schedule compared to just using a fixed one. A surprising result is that even though we have not trained our models for nearly as long, or on nearly as much data as Ballé et al. (2018), our method still gets reasonably close to
1

We thank the authors of the paper for making their data available to us.

42

Results

their results. We compare our results to theirs, which as far as we are aware are the current state-of-the-art on both the MS-SSIM and PSNR perceptual metrics.

4.2 Comparison of our method with other algorithms
We present the rate-distorsion curves for the following: · JPEG, with quality settings from 1 to 92, with increments of 7 between settings. As this is the most widely used lossy image compression codec, it is crucial to demonstrate that our method is at least competitive with it, and ideally beats it. · BPG2 with 4:4:4 chroma sampling, as we are comparing against RGB-based compression techniques. We used quantization settings between 51 to 33 wiht decrements of 3 between settings. · Two models with the same architecture from Ballé et al. (2018), one optimized for a MSE training objective, and one optimized for the MS-SSIM perceptual metric. · Two of our models, all of which were optimized with Laplacian likelihoods, one PLN and one  -PLN. We plot both their theoretically optimal performance as well as their actual performance, with the differences explained below. For our method, for each model we present two results: the theoretically optimal performance, and the actual performance. The theoretically optimal BPP was calculated using the theoretically achievable upper bound for the compression size in bits as given by Harsha et al. (2007), without the constant term: [x  z] + 2 log ([x  z] + 1) . The optimal reconstruction error was calculated by passing an image through the VAE regularly, instead of using the coded approximate sample. Thus, any actual method's performance using the same setup must apper to the right of (less efficient compression) or below (worse reconstruction quality) the theoretical position. We trained the PLNs using  = {0.01, 0.03, 0.1, 0.3, 1} and the  -PLNs using  = {10, 3, 1, 0.3, 0.1}. Our results can be seen in Figure 4.1. We observe a similar phenomenon as Ballé et al. (2018): there is a mismatch in the comparison of models according to different perceptual
2

We used the implementation available at http://bellard.org/bpg

4.3 Analysis of the contribution of the second level

43

metrics, depending on what objective they have been optimized for. In particular, JPEG and BPG have both been optimized so that they give high PSNR (thus, low MSE), whereas they underperform on the newer MS-SSIM metric. We note that as MS-SSIM correlates better with what the HVS perceives, we find it more important to do well on that comparison. In interesting note, that also justifies our choice of the MAE as the training objective, is the fact that our model optimized for it does well on this metric.

4.3

Analysis of the contribution of the second level

An important part of verifying the validity of using PLNs is to analyze the contribution of the second level. Here we look at · its contribution to the codelength · its efficiency in capturing dependencies between the first level latents

Fig. 4.3 ladder on kodim21

44

Results

Fig. 4.1 Rate-Distorsion curves of several relevant methods. Please see Section 4 for the description of how we obtained each curve. We note that the MS-SSIM results are presented in decibels, where the conversion is done using the formula -10 log10 (1 - MS-SSIM(x, x)). The PSNR is computed from the mean squared error, using the formula -10log10 MSE(x, x).

4.3 Analysis of the contribution of the second level

45

Fig. 4.2 Contribution of the second level to the rate, plotted agains the actual rate. Left: Contribution in BPP, Right: Contribution in percentages. We see that for lower bitrates there is more contribution from the second level and it quickly decreases for higher rates. It is also clear that on the same bitrates, the  -PLN requires less contribution from the second level than regular PLN.

Fig. 4.4 ladder on kodim21

46

Results

Fig. 4.5 ladder on kodim21

Fig. 4.6 ladder on kodim21

4.3 Analysis of the contribution of the second level

47

Fig. 4.7 ladder on kodim21

Fig. 4.8 ladder on kodim21

48

Results

Fig. 4.9 Coding times of models plotted agains their rates. Left: Regular PLNs. Right:  -PLNs. The striped lines indicate the concrete positions of our models in the rate line. While it seems that there is a linear relationship between rate and coding time, we do not have enough datapoints to conclude this.  Encoding Time (s) Decoding Time (s) PLNs 1 0.3 0.1 0.03 55.91 64.95 98.85 145.38 24.85 26.61 33.34 44.85 Table 4.1 haha  -PLNs 10 3 1 71.40 120.54 172.34 27.81 38.87 54.86 0.1 452.49 140.52

4.4 Compression Speed
Although not a focus of our project, we now briefly examine the the encoding and decoding speed of our method. We have plotted the compression ratios of our models against the time it took them to encode / decode them using IS-GS in Figure 4.9. As increasing the reconstruction quality leads to higher KL divergences between the latent posteriors and priors, both the importance sampler and the greedy sampler will need to split up a higher total KL, and thus we expect the coding to become slower. This is precisely what we observe, with a seemingly approximately linear growth, although we do not have data to conclude this. We also see that encoding consistently takes around 3 times as long as decoding. It is clear that our method is not yet practical: even the fastest case takes around a minute to encode and about 20 seconds to decode, which very far away for real-time applications for now. The precise values are reported in Table 4.1.

Chapter 5 Conclusion and Future Work
Several metrics to optimise for: - compression quality - compression size - compression time - compressor size - compressor power consumption - robustness of compressor (i.e. resistance to errors / adversarial attacks) - security / privacy of compression - scalability: image size, image quality

References
Adel'son-Vel'skii, G. M. and Landis, E. M. (1962). An algorithm for organization of information. In Doklady Akademii Nauk, volume 146, pages 263­266. Russian Academy of Sciences. Ballé, J., Laparra, V., and Simoncelli, E. P. (2015). Density modeling of images using a generalized normalization transformation. arXiv preprint arXiv:1511.06281. Ballé, J., Laparra, V., and Simoncelli, E. P. (2016a). End-to-end optimization of nonlinear transform codes for perceptual quality. In 2016 Picture Coding Symposium (PCS), pages 1­5. IEEE. Ballé, J., Laparra, V., and Simoncelli, E. P. (2016b). End-to-end optimized image compression. arXiv preprint arXiv:1611.01704. Ballé, J., Minnen, D., Singh, S., Hwang, S. J., and Johnston, N. (2018). Variational image compression with a scale hyperprior. arXiv preprint arXiv:1802.01436. Bishop, C. (2013). Pattern Recognition and Machine Learning. Information science and statistics. Springer (India) Private Limited. Bishop, C. M. (1998). Latent variable models. In Learning in graphical models, pages 371­403. Springer. Bottou, L., Haffner, P., Howard, P. G., Simard, P., Bengio, Y., and LeCun, Y. (1998). High quality document image compression with djvu. Bull, D. (2014). Communicating Pictures: A Course in Image and Video Coding. Elsevier Science. CLIC (2018). Workshop and challenge on learned image compression. compression.cc. Accessed: 2019-03-25. https://www.

Company, E. K. (1999). Kodak lossless true color image suite. http://r0k.us/graphics/kodak/. Dai, B. and Wipf, D. (2019). Diagnosing and enhancing vae models. arXiv preprint arXiv:1903.05789. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009). Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248­255. Ieee.

52

References

Denton, E. L., Chintala, S., Fergus, R., et al. (2015). Deep generative image models using a laplacian pyramid of adversarial networks. In Advances in neural information processing systems, pages 1486­1494. Eskicioglu, A. M., Fisher, P. S., and Chen, S.-Y. (1994). Image quality measures and their performance. Girod, B. (1993). What's wrong with mean-squared error? Digital images and human vision, pages 207­220. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. (2014). Generative adversarial nets. In Advances in neural information processing systems, pages 2672­2680. Goyal, V. K. (2001). Theoretical foundations of transform coding. IEEE Signal Processing Magazine, 18(5):9­21. Gregor, K., Danihelka, I., Graves, A., Rezende, D. J., and Wierstra, D. (2015). Draw: A recurrent neural network for image generation. arXiv preprint arXiv:1502.04623. Grünwald, P., Grunwald, A., and Rissanen, J. (2007). The Minimum Description Length Principle. Adaptive computation and machine learning. MIT Press. Gupta, P., Srivastava, P., Bhardwaj, S., and Bhateja, V. (2011). A modified psnr metric based on hvs for quality assessment of color images. In 2011 International Conference on Communication and Industrial Application, pages 1­4. IEEE. Harsha, P., Jain, R., McAllester, D., and Radhakrishnan, J. (2007). The communication complexity of correlation. In Twenty-Second Annual IEEE Conference on Computational Complexity (CCC'07), pages 10­23. IEEE. Havasi, M., Peharz, R., and Hernández-Lobato, J. M. (2018). Minimal random code learning: Getting bits back from compressed model parameters. arXiv preprint arXiv:1810.00440. He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770­778. Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M., Mohamed, S., and Lerchner, A. (2017). beta-vae: Learning basic visual concepts with a constrained variational framework. In International Conference on Learning Representations. Hinton, G. and Van Camp, D. (1993). Keeping neural networks simple by minimizing the description length of the weights. In in Proc. of the 6th Ann. ACM Conf. on Computational Learning Theory. Citeseer. Huffman, D. A. (1952). A method for the construction of minimum-redundancy codes. Proceedings of the IRE, 40(9):1098­1101. Huynh-Thu, Q. and Ghanbari, M. (2008). Scope of validity of psnr in image/video quality assessment. Electronics letters, 44(13):800­801.

References

53

Ioffe, S. and Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167. Jain, A. K. (1989). Fundamentals of digital image processing. Englewood Cliffs, NJ: Prentice Hall,. Jiang, J. (1999). Image compression with neural networks­a survey. Signal processing: image Communication, 14(9):737­760. Johnston, N., Vincent, D., Minnen, D., Covell, M., Singh, S., Chinen, T., Jin Hwang, S., Shor, J., and Toderici, G. (2018). Improved lossy image compression with priming and spatially adaptive bit rates for recurrent networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Karush, W. (2014). Minima of functions of several variables with inequalities as side conditions. In Traces and Emergence of Nonlinear Programming, pages 217­245. Springer. Kingma, D. P. and Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. Kuhn, H. W. and Tucker, A. W. (2014). Nonlinear programming. In Traces and emergence of nonlinear programming, pages 247­258. Springer. Mentzer, F., Agustsson, E., Tschannen, M., Timofte, R., and Van Gool, L. (2018). Conditional probability models for deep image compression. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Mougeot, M., Azencott, R., and Angeniol, B. (1991). Image compression with back propagation: improvement of the visual restoration using different cost functions. Neural networks, 4(4):467­476. Portilla, J., Strela, V., Wainwright, M. J., and Simoncelli, E. P. (2003). Image denoising using scale mixtures of gaussians in the wavelet domain. IEEE Trans Image Processing, 12(11). Rabbani, M. and Joshi, R. (2002). An overview of the jpeg 2000 still image compression standard. Signal processing: Image communication, 17(1):3­48. Rippel, O. and Bourdev, L. (2017). Real-time adaptive image compression. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 2922­2930. JMLR. org. Rissanen, J. (1986). Stochastic complexity and modeling. The annals of statistics, pages 1080­1100. Rissanen, J. and Langdon, G. (1981). Universal modeling and coding. IEEE Transactions on Information Theory, 27(1):12­23. Shannon, C. E. and Weaver, W. (1998). The mathematical theory of communication. University of Illinois press.

54

References

Sønderby, C. K., Raiko, T., Maaløe, L., Sønderby, S. K., and Winther, O. (2016). How to train deep variational autoencoders and probabilistic ladder networks. In 33rd International Conference on Machine Learning (ICML 2016). Theis, L., Shi, W., Cunningham, A., and Huszár, F. (2017). Lossy image compression with compressive autoencoders. arXiv preprint arXiv:1703.00395. Toderici, G., O'Malley, S. M., Hwang, S. J., Vincent, D., Minnen, D., Baluja, S., Covell, M., and Sukthankar, R. (2015). Variable rate image compression with recurrent neural networks. arXiv preprint arXiv:1511.06085. Toderici, G., Vincent, D., Johnston, N., Jin Hwang, S., Minnen, D., Shor, J., and Covell, M. (2017). Full resolution image compression with recurrent neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5306­5314. Wallace, G. K. (1992). The jpeg still picture compression standard. IEEE transactions on consumer electronics, 38(1):xviii­xxxiv. Wang, Z., Bovik, A. C., Sheikh, H. R., Simoncelli, E. P., et al. (2004). Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600­612. Wang, Z., Simoncelli, E. P., and Bovik, A. C. (2003). Multiscale structural similarity for image quality assessment. In The Thirty-Seventh Asilomar Conference on Signals, Systems & Computers, 2003, volume 2, pages 1398­1402. Ieee. Zhao, H., Gallo, O., Frosio, I., and Kautz, J. (2015). Loss functions for neural networks for image processing. arXiv preprint arXiv:1511.08861. Zhou, L., Cai, C., Gao, Y., Su, S., and Wu, J. (2018). Variational autoencoder for low bit-rate image compression. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops.

Appendix A Appendix A
Rejection Sampling
The rejection sampling algorithm presented here is due to Harsha et al. (2007). Algorithm 4 Rejection sampling presented in Harsha et al. (2007).
1:

procedure Rej-Sampler( , ,       )

  is the prior   is the posterior   are i.i.d. samples from 

2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14:

0 ()  0    .  0  0. for   1, ...  do  ()  min  () - -1 (), (1 -  -1 )()     ()  -1 () +  ()      ()  ()  ( )  (1- )() Draw    (0, 1) if  <  ( ) then return ,  end if end for end procedure


56

Appendix A

Algorithm 5 Importance sampling algorithm proposed by Havasi et al. (2018) procedure Importance-Sampler( , ,       )   exp{KL[  ||  ]} ( )     ()  = 1, ...    Sample   () return ,  end procedure   is the prior   is the posterior   are i.i.d. samples from 

Appendix B Appendix B: Images

