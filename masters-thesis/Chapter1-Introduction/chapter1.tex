%!TEX root = ../thesis.tex
%*******************************************************************************
%*********************************** First Chapter *****************************
%*******************************************************************************
\chapter{Introduction}

\section{Motivation}
\par
There have been several exciting developments in neural image compression
recently, and there are now methods that consistently outperform classical methods
such as JPEG, WebP and BPG (\cite{toderici2017full}, \cite{theis2017lossy},
\cite{rippel2017real}, \cite{balle2018variational}, \cite{johnston2018cvpr},
\cite{mentzer2018cvpr}).

\par
The first advantage of ML-based image codecs is that they can adapt to the
statistics of individual images much better than even the best hand-crafted
methods. This allows them to compress images to much fewer bits while retaining good
perceptual quality. A second advantage is that they are generally far easier to
adapt to new media formats, such as light-field cameras, $360^\circ$ images,
Virtual Reality (VR), video streaming etc. The purposes of compression and the
devices on which the encoding and decoding is performed varies greatly, from
archiving terabytes of genetic data for later research on a supercomputer,
through compressing images to be displayed on a blog or a news article to
improve their loading time in a user's web browser, to
streaming video on a mobile device. Classical methods are usually
``one-size-fits-all'', and their compression efficiency can severely degrade when
attempting to compress media for which they were not designed. Designing good
hand-crafted codecs is difficult, can take several years, and
requires the knowledge of many experts. ML techniques, on the other hand, allow to
create equal or better performing, and much more flexible codecs within a few
months at a significantly lower cost.

\par
The chief limitation of current neural image compression methods is while most
models these days are trained using gradient-based optimizers, quantization, a
key step in the (lossy) image compression pipeline, is an inherently
non-differentiable operation. Hence, all current methods need to resort to
``tricks'' and approximations so that the learning signal can still be passed
through the whole model. A review of these methods will be presented in Chapter 
\ref{chapter:related_works}.

\par
Our approach differs from virtually all previous methods in that we take
inspiration from information theory (\cite{rissanen1986stochastic} and
\cite{harsha2007communication}) and neural network compression
(\cite{hinton1993keeping} and \cite{havasi2018minimal}) to develop a general
lossy compression
framework that allows us forgoing the quantization step in our compression
pipeline completely. We then apply these ideas to image compression and
demonstrate that our codec using Probabilisitic Ladder Networks 
(\cite{sonderby2016train}), an extension of Variational Auto-Encoders
(\cite{kingma2013auto}), achieves close to state-of-the-art performance on the
Kodak Dataset (\cite{kodakdataset}) with no fine-tuning of our architecture.

\section{Thesis Contributions}
\par
The contributions of our thesis are as follows:
\begin{enumerate}
\item A \textbf{comparative review} of recent influential works in the field of
  neural image compression.

\item The development of a \textbf{general lossy compression
    framework} that allows forgoing the quantization step in the compression pipeline,
  thus allowing end-to-end optimization of models using gradient-based methods.

\item A \textbf{novel image compression algorithm} using our framework, that 
  achieves close to state-of-the-art performance on the Kodak Dataset
  \cite{kodakdataset} without any fine-tuning of model hyperparameters.

\item Three \textbf{sampling algorithms} for multivariate Gaussian
  distributions, that can be readily used in our compression framework.
\end{enumerate}

\section{Thesis Outline}
\par 
Our thesis begins with an introduction to the field of neural image compression.
We first review concepts in image compression in Chapter
\ref{chapter:background}, such as lossless versus lossy
compression, the rate-distortion trade-off and linear and non-linear transform
coding. We emphasize the fundamental role quantization plays in virtually all
previous approaches in image compression. In Section
\ref{sec:intro_theoretical_foundations}, we shift our focus to information
theory, where we introduce the Minimum Description Length (MDL) Principle
(\cite{rissanen1981universal}) and the Bits-back Argument
(\cite{hinton1993keeping}). Taking inspiration from these, as well as from
\cite{harsha2007communication} and \cite{havasi2018minimal}, in Section
\ref{sec:compression_without_quantization}
we develop a general framework for the lossy compression of data and show how
it is related to quantization.

\par
In Chapter \ref{chapter:related_works}, we give a comparative review of recent
influential developments in neural image compression. We examine their whole
pipeline: the datasets used, their architectures, the ``tricks'' and
approximations used to circumvent the non-differentiability of quantization,
their coding methods, training procedures and evaluation methods.

\par
In Chapter \ref{chapter:method}, we describe our proposed method. We explain our
choice of the dataset, and preprocessing steps. We give a detailed description of
our model and why we chose it. We describe our
training procedure, based on ideas from \cite{sonderby2016train},
\cite{higgins2017beta}, \cite{balle2018variational} and \cite{dai2019diagnosing}.
Next, we present 3 ``codable'' sampling techniques, that can be used in our
compression framework and point out their strengths and weaknesses.

\par
Finally, in Chapter \ref{chapter:experiments} we compare our trained models to
current compression algorithms, both classical such as JPEG and BPG, and
the current state-of-the-art neural methods (\cite{balle2018variational}). In
particular, we compare these methods by their compression rates for a given
perceptual quality as measured by the two most popular perceptual metrics, Peak
Signal-to-Noise Ratio (PSNR) (\cite{psnr}) and the Multi-scale Structural
Similarity Index (MS-SSIM) (\cite{msssim}). We achieve close to
state-of-the-art performance, with no fine-tuning of model hyperparameters.
We also present some further analysis of our chosen models, to empirically
justify their use, as well as to analyze some of the aspects that were not of
primary concern of this work, such as coding speed.
\paragraph{}

