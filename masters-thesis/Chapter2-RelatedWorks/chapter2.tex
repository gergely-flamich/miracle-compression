%!TEX root = ../thesis.tex
%*******************************************************************************
%****************************** Second Chapter *********************************
%*******************************************************************************

\chapter{Related Works}
\label{chapter:related_works}

\graphicspath{{../img/related_works/}}

\par
Here we give a brief overview of the history of using machine learning for image
compression. Then, we focus on recent advances in lossy image compression and
describe and compare their methods to each other as well as ours.
\section{Machine Learning-based Image Compression}
\par

First attempt by \cite{bottou1998high} DjVu focused on segmenting foreground and
background in magazines and using K-means clustering to analize and code the
background.

Neural network based image compression has been theorized about for a long time
now \cite{mougeot1991image} \cite{jiang1999image} 
\cite{toderici2015variable} focused only 32x32 thumbnails
Image reconstruction through compressive representations \cite{denton2015deep},
\cite{gregor2015draw}

\section{Comparison of Recent Works}
\label{sec:lit_comparison}
\par

There have been several recent advances in neural network-based compression
techniques, most notably \cite{toderici2015variable}, \cite{balle2016end},
\cite{toderici2017full}, \cite{theis2017lossy}, \cite{rippel2017real},
\cite{balle2018variational}, \cite{johnston2018cvpr}, \cite{mentzer2018cvpr}. 
An interesting commonality between these approaches is that there is very little
commonality between them, for a multidue of reasons, on which we hope to shed
some lite in this section. Instead of analyzing them in a historical order, we
will instead go through the compression pipeline and compare them head-to-head
in each compartment separately. 
\subsection{Datasets and Input Pipelines}
\par
Somewhat surprisingly it appears that there is no canonical dataset (yet) for
the task at hand, namely a set of high-resolution, variable-sized losslessly
encoded colour images,
although CLIC \cite{clic2018} seems to be an emerging one. Perhaps the reason is
that generally in other domains, such as image-based classification cropping and
rescaling images can effectively side-step the need to deal with variable-sized
images. However, when it comes to compression, if we hope to build anything
useful, we must account for this.
\par
On the other hand most authors have used the Kodak dataset \cite{kodakdataset}
for testing / reporting results.
\begin{itemize}
\item \cite{balle2016end} trained on 6507 images, selected from ImageNet
  \cite{deng2009imagenet}. They removed images with excessive saturation and
  since their method is based on dithering, they added unifrom noise to the
  remaining images to imitate the noise introduced by quantization. Finally,
  they downsampled and cropped images to be $256 \times 256$ pixels in size.
  They only kept images whise resampling factor was 0.75 or less, in order to
  avoid high frequency noise.
\item \cite{toderici2017full}
  used two datasets, first the one they described in \cite{toderici2015variable}
  and the second one (they call the ``High Entropy'' dataset) was created by
  first scraping 6 million images from the web, then resizing them to $1280
  \times 720$ pixels. Then, they decomposed these into $32 \times 32$ pixel
  tiles and selected the 100 tiles from each with the worst compression ratio under
  the PNG algorithm.
\item \cite{theis2017lossy} used 434 high resolution images from \url{flickr.com}
  under the creative commons license. As \texttt{flickr} store its images as
  JPEGs, they downsampled all images to be below $1536 \times 1536$ in
  resolution and saved them as PNGs in order to reduce the effects of the lossy
  compression. Then, they extracted several $128 \times 128$ patches from each
  image and trained on those.
\item \cite{rippel2017real} took images from the Yahoo Flickr Creative Commons
  100 Million dataset, with $128 \times 128$ patches randomly sampled from the
  images. They do not state whether they used the whole dataset or just a
  subset, neither do they describe further preprocessing steps.
\item \cite{balle2018variational} scraped $\approx$ 1 million colour JPEG images
  of dimensions at most $3000 \times 5000$. They filtered out images with
  excessive saturation similarly to \cite{balle2016end}. They also
  downsampled images by random factors such that the image's height and width
  stayed above 640 and 1200 pixels, respectively. Finally, they use several
  randomly cropped $256 \times 256$ pixel patches extracted from each image.
% \item \cite{johnston2018cvpr} similarly to the ``High Entropy'' dataset in
%   \cite{toderici2017full}, they scrape 6 million JPEG images from the web and
%   extract $128 \times 128$ patches from them to train on.
% \item \cite{mentzer2018cvpr} train on the ILSVRC2012 ImageNet dataset 
%   \cite{russakovsky2015imagenet}. They take extract $160 \times 160$ pixel image
%   patches and randomly flipped them.
\end{itemize}

\subsection{Architectures}
\par
This is the most diverse aspect of recent approaches, and so we will only
discuss them on a very high level. We took inspiration from most of these papers
as well as others, these will be emphasized in Section \ref{sec:our_method}.

\begin{itemize}
\item \cite{balle2016end} Build a relatively shallow autoencoder (5 layers).
  There are several non-standard techniques they use, however. Firstly, their
  architecture is fully convolutional, i.e. all linear transformations in their
  network are convolutions in the encoder and deconvolutions in the decoder.
  They also downsample after the linear transformations, however, this is not
  elaborated upon in the work, neither is whether they padded the convolutions
  and if so how.
  This leads to the very natural consequence that their latent space and hence
  the code of an image grows with its size. Secondly they do not use any
  standard non-linearity or batch normalization. Instead, they propose their own
  activation function, custom tailored for image compression. These
  non-linearities are a form of adaptive local gain control for the images,
  called Generalized Divisive Normalization (GDN). At the $k$th layer for
  channel $i$ at position $(m, n)$, for input $w_i^{(k)}(m, n)$, the GDN
  transform is defined as
  \begin{equation}
    \label{eq:gdn_def}
    u_i^{(k + 1)}(m, n) = \frac{w_i^{(k)}(m, n)}{
      \left( \beta_{k, i} + \sum_j \gamma_{k, i, j}
        \left( w_{j}^{(k)}(m, n)\right)^2 \right)}.
  \end{equation}
  Its approximate inverse, IGDN for input $\hat{u}_i^{(k)}(m, n)$ is defined as
  \begin{equation}
    \label{eq:igdn_def}
    \hat{w}_i^{(k)}(m, n) = \hat{u}_i^{(k)}(m, n) \cdot \left(
      \hat{\beta}_{k, i} + \sum_{j} \hat{\gamma}_{k, i, j} \left(
      \hat{u}_j^{(k)}(m, n)\right)^2 \right)^{\frac{1}{2}}.
  \end{equation}
  Here, the set $\beta_{k, i}, \gamma_{i, j, k}, \hat{\beta_{k, i}},
  \hat{\gamma_{i, j, k}}$ are learned during training and fixed at test time.
  
\item \cite{toderici2017full}
\item \cite{theis2017lossy} define a Compressive Autoencoder (CAE) as a regular
  autoencoder with the quantization step between the encoding and decoding step.
  (In this sense, the architectures of \cite{balle2016end} and
  \cite{balle2018variational} are also CAEs.) They mirror pad the input first
  and then they follow it up by a deep, fully convolutional, residual
  architecture \cite{he2016deep}. They use valid convolutions and downsample by
  using a stride of 2. Between convolutions they use leaky ReLUs as
  nonlinearities, which are defined as 
  \[
    f_\alpha(x) = \max\{x, \alpha x\}, \quad \alpha \in [0, 1].
  \]
  The decoder mirrors the encoder. When upsampling is required, they use what
  they term \textit{subpixel} convolutions, where they perform a regular
  convolution operation with an increased number of filters, and then reshape
  the resulting tensor into one with larger spatial extent but fewer channels.
\item \cite{rippel2017real} use a fully convolutional encoder/decoder pair,
  downscaling between convolutions. They also add in an additional residual
  connection from every layer to the last, summing at the end. They call this
  \textit{pyramidal decomposition} and \textit{interscale alignment}, with the
  rationale behind it being that the residual connections extract features at
  different scales, and so the latent representations can take advantage of this.
\item \cite{balle2018variational} extend the architecture presented in
  \cite{balle2016end}. In particular, the encoder and decoder remain the same,
  and they add an additional stochastic layer on top of the architecture.
  However, it is important to note, that this is not a hierarchical VAE, it
  resembles instead a probabilistic ladder network \cite{sonderby2016train}.
  The layers leading to the second level are more standard, it is still fully
  convlutional with downsampling after convolutions, however, instead of GDN
  they use ReLUs.
  \par
  A slightly strange
  design choice on their part is since they will wish to force the second stage
  activations to be positive (it will be predicting a scale parameter), instead
  of using an exponential or softplus ($\log (1 + \exp\{x\})$) activation at the
  end, they take the absolute value of the input to the first layer, and rely on
  the ReLUs never giving negative values. We are not sure if this was meant to
  be a computational saving, as taking absolute values is certainly cheaper then
  either of the aforementioned standard ways of forcing positive values, or it
  if it gave better results.
\end{itemize}

\subsection{Quantization}
\label{sec:comp_quant}
\par
As all methods surveyed here are trained using gradient-based methods, a crucial
question that needs to be answered is how they dealt with the issue of
quantization. This is because encoding an image, quantizing the result and then
decoding the dequantized representation is problematic as the quantization step
yields 0 derivatives almost everywhere. There are two particular items that need
to be circumvented: first, the quantization operation itself, and second, the
rate estimator $H[P(\vec{z})]$. In the next section, we will see how our method
suffers from neither of these issues, as we forego the quantization step altogether.
\begin{itemize}
\item
  \textbf{Quantization }
  \cite{balle2016end} Quantize the output of their encoder $\vec{z}$ as
  \[
    \hat{z}_i = [z_i], 
  \]
  where $[\cdot]$ denotes the rounding operation. They model this 
  quantization error as dither, i.e. they replace their quantized latents
  $\hat{z}_i$ by
  \[
    \tilde{z}_i = z_i + \delta z_i, \quad z_i \sim \Unif{0, 1}. 
  \]
  \par \textbf{Rate }
  To model the rate, they also require to learn a distribution over the
  $\tilde{z}_i$s. They assume that the latents are independent, and hence they
  can model the the joint as a fully factorized distribution. They use linear
  splines to do this, whose parameters $\psi^{(i)}$ they update separately every
  $10^6$ iterations using SGD to maximize its log likelihood on the latents.

\item \cite{toderici2017full}
\item
  \textbf{Quantization }
  \cite{theis2017lossy} also use rounding as their quantization step.
  However, instead of using uniform noise to model the quantization error, they
  simply replace the derivative of the rounding operation in the backpropagation
  chain by the constant function 1:
  \[
    \frac{d}{d y} [y] = 1.
  \]
  This is a smooth approximation of rounding and they report that empirically it
  gave good results. However, as quantization itself creates an important
  bottleneck in the flow of information, it is key that only the derivative is
  replaced and not the operation itself.
  \par \textbf{Rate }
  They note that
  \[
    P(\vec{z}) = \int_{[-\frac{1}{2}, \frac{1}{2})^M} q(\vec{z} + \vec{u}) \d \vec{u}
  \]
  for some appropriate density $q$,
  where the integral is taken over the centered $M$ dimensional hypercube.
  Then, they replace the the rate estimator with an upper bound using Jensen's
  inequality:
  \[
    -\log_2P(\vec{z}) = -\log_2\int_{[-\frac{1}{2}, \frac{1}{2})^M} q(\vec{z} +
    \vec{u}) \d \vec{u} \leq -\int_{[-\frac{1}{2}, \frac{1}{2})^M} \log_2q(\vec{z} +
    \vec{u}) \d \vec{u}.
  \]
  This upper bound is now differentiable. They pick a Gaussian Scale Mixtures for
  $q$, with $s = 6$ components, with the mixing proportions fixed across spatial
  dimensions, which gives the negative log likelihood
  \[
    -\log_2 q(\vec{z} + \vec{u}) =
    \sum_{i, j, k} \log_2 \sum_s \pi_{k, s} \Norm{z_{k,i,j} + u_{k, i, j} \mid
    0, \sigma_{k, s}^2},
  \]
  where $i,j$ iterate through the spatial dimensions and $k$ indexes the filters.
  
\item
  \textbf{Quantiazation }
  \cite{rippel2017real} use the following formula:
  \[
    \hat{z}_i = \frac{1}{2^B}\left\lceil 2^Bz_i \right\rceil
  \]
  with $B = 6$. For $B = 1$ this gives a similar error as rounding, and as $B$
  is increased, the quantization gets finer. They do not report how they
  circumvented the non-differentiability of the quantization step, however, they
  do cite both \cite{balle2016end} and \cite{theis2017lossy}, so our guess is
  that they used a method from one of these papers.

  \textbf{Rate } they do not train for the rate-distrotion trade-off directly
  and in particular omit the rate estimator from the loss. Hence there was no
  need for them to approximate it to make it differentiable.
\item
  \textbf{Quantization }
  \cite{balle2018variational} the quantization scheme remains the same as in
  \cite{balle2016end}, extended to the second stochastic layer as well.

  \textbf{Rate } 
  As for the rate, they us a non-parametric, fully factorized prior for the
  second stage:
  \[
    p(\vec{\tilde{z}}^{(2)} \mid \psi) =
    \prod_i \left(  p\left(\tilde{z}^{(2)}_i \mid \psi_i\right) *
      \Unif{-\frac{1}{2}, -\frac{1}{2}\right)}.
  \]
  Then, they model the first stage as dithered zero-mean Gaussians with variable
  scale depending on the second stage, thereby relaxing the initial independence
  assumption on the latent space to a more general \textit{conditional
    indepenence} assumption \cite{bishop1998latent}:
  \[
    p(\vec{\tilde{z}}^{(1)} \mid \vec{\tilde{z}}^{(2)}) = 
    \prod_i \left(  \Norm{\tilde{z}^{(1)}_i \mid 0, \tilde{\sigma}^2_i\right)*
      \Unif{-\frac{1}{2}, -\frac{1}{2}}}.
  \]
\end{itemize}

Crucially, the rate term in our optimization objective is different from
previous approaches, in that in previous approaches the distribution of the
codewords was estimated separately. In our case, however, it is an integral part
of the optimization process.

In \cite{theis2017lossy}, they demonstrate that it is precisely the quantization
step that introduces the noisy artifacts into the image, and not the
reconstruction procedure. This is fundamentally different from our case, where
the majority of the quality degradation comes from the fact that VAEs generally
produce blurry reconstructions.

\subsection{Coding}
\par
Another important part of the examined methods is the coding. In particular, an
interesting caveat of entropy codes is that they tend to perform slightly worse
than the predicted rate, due to neglected constant factors in the algorithm
\cite{rissanen1981universal}. Hence, it is always more informative to present
results where the actual coding has been performed and not just the theoretical
rate reported. All examined works have implemented their own coding algorithms,
and we briefly review them here.
\begin{itemize}
\item \cite{balle2016end} Context adaptive binary arithmetic coding (CABAC),
  they supply new information in raster-scan order, which means it does not
  improve much over non-adaptive coding, but there might be potential
\item \cite{toderici2017full}
\item \cite{theis2017lossy} used their estimated probabilities $q(\vec{z})$ and
  used an off-the-shelf publicly available range coder to compress their latents.
\item \cite{rippel2017real} treat each bit of their $B$-bit precision quantized
  representations individually, because they want to utilize the sparsity of
  more significant bits. They train a separate binary classifier to predict
  probabilities for each individual bit based on a set of features (they call
  it a \textit{context}) to use in an adaptive arithmetic coder. They further
  add a regularizing term during training based on the codelength of a batch to
  match a length target. This is to encourage sparsity for high-resolution, but
  low entropy images and a longer codelength for low resolution but high entropy
  images.
\item \cite{balle2018variational} use a non-adaptive arithmetic coder as their
  entropy code. As they have two stochastic levels, with the
  first depending on the second, they have to code them sequentially. For the
  second level, they get their frequency estimates for $\vec{\hat{z}}^{(2)}$
  from the non-parametric prior:
  \[
    p(\hat{z}^{(2)}_i) =
    \int_{\hat{z}^{(2)}_i-\frac{1}{2}}^{\hat{z}^{(2)}_i+\frac{1}{2}}p(\tilde{z}_i \mid \psi_i) \d \tilde{z}_i.
  \]
  Then, on the first level, their probaibilities are given by:
  \[
    p(\hat{z}^{(1)} \mid \tilde{z}^{(2)}) = p(\hat{z}^{(1)} \mid
    \tilde{\sigma}^2_i) = 
    \int_{\hat{z}^{(1)}_i-\frac{1}{2}}^{\hat{z}^{(1)}_i + \frac{1}{2}}\Norm{\tilde{z}_i \mid 0, \tilde{\sigma}^2_i} \d \tilde{z}_i.
  \]

\end{itemize}

\subsection{Training}
\par
\begin{itemize}
\item \cite{balle2016end} Set out to train for the rate-distrotion trade-off
  directly, i.e.
  \[
    L = H[\vec{\hat{z}}] + \beta \Exp[d(\vec{x}, \vec{\hat{x}})],
  \]
  where the expectation is taken over training batches.
  As this is a non-differentiable loss function due to the quantization, they
  replace the discrete entropy term with a differential entropy term:
  \[
   L = \Exp\left[ -\sum_i \log_2 p(z_i + \delta z_i \mid \psi^{(i)}) +
                     \beta d(\vec{x}, \vec{\hat{x}})\right].
  \]
  \par Since they use MSE as the distance metric, they note that their
  architecture could be considered as an, albeit somewhat
  strange, VAE with Gaussian likelihood
  \[
    p(\vec{x} \mid \vec{\tilde{z}}, \beta) =
    \Norm{\vec{x} \mid \vec{\hat{x}}, (2\beta)^{-1}\vec{1}},
  \]
  mean-field prior
  \[
    p(\vec{\tilde{z}} \mid \psi^{1}, \hdots, \psi^{N}) =
    \prod_i p(\tilde{z}_i \mid \psi^{(i)})
  \]
  and mean-field posterior
  \[
    q(\vec{\tilde{z}} \mid \vec{x}) =
    \prod_i \Unif{\tilde{z}_i \mid z_i, 1},
  \]
  where $\Unif{\tilde{z}_i \mid z_i, 1},$ is the uniform distribution centered
  on $z_i$ of width 1. They train their model using Adam \cite{kingma2014adam},
  with learning rate decay. They do not state how long they trained their
  architecture.
  
\item \cite{toderici2017full}
\item \cite{theis2017lossy}
  also optimize an approximation of the rate-distrotion trade-off, replacing the
  rate estimator with its upper bound:
  \[
    L = -\Exp[\log_2 q(\vec{z} + \vec{u})] + \beta \Exp[d(\vec{x}, \vec{\hat{x}})].
  \]
  They performed the training incrementally, in the sense that they masked most
  latents at the start, such that their contribution to the loss was 0. Then, as
  the training performance saturated, they unmasked them incrementally.
  They trained the model with Adam, with a small learning rate decay.
\item \cite{rippel2017real} use the MS-SSIM metric as the training objective and
  they use an adversarial \cite{goodfellow2014generative} loss use GANs 
\item \cite{balle2018variational} in the same vein as they laid out their
  VAE-based training objective in \cite{balle2016end}, the data log likelihood
  term stays, but now the regularizing term is the KL divergence between the
  joint posterior $q(\vec{\tilde{z}}^{(1)}, \vec{\tilde{z}}^{(2)} \mid \vec{x})$
  and the joint prior 
  $q(\vec{\tilde{z}}^{(1)}, \vec{\tilde{z}}^{(2)})$. Here, as due to the
  dithering assumption, the joint posterior works out to be
  \[
    q\left(\vec{\tilde{z}}^{(1)}, \vec{\tilde{z}}^{(2)} \mid \vec{x}\right) =
    \prod_i \Unif{\tilde{z}^{(1)}_i \mid \hat{z}^{(1)}, 1} \cdot
    \prod_i \Unif{\tilde{z}^{(2)}_i \mid \hat{z}^{(2)}, 1}.
  \]
  The prior is as explained in the previous section
  \par
  Then, taking the KL between these, the full traning objective works out to be
  \begin{equation}
    \label{eq:balle_var_train_objective}
    L = \Exp\left[ -\sum_i \log_2 p(\tilde{z}^{(2)}_i \mid \psi^{(i)}) 
      -\sum_i \log_2 p(\tilde{z}^{(1)}_i \mid \tilde{\sigma}^2_i) +
      \beta d(\vec{x}, \vec{\hat{x}})\right].
  \end{equation}
  Eq \ref{eq:balle_var_train_objective} is very important, as it will be
  directly translated to our learning objective.
  \par
  They train 32 models, half using the architecture from \cite{balle2016end} and
  half using the current one, half optimized for MSE and half of MS-SSIM, with 8
  different $\beta$s. They use Adam to optimize their model, and they report
  that neither batch normalization nor learning rate decay gave better results.
  The former they attribute to GDN.
\end{itemize}

\subsection{Evaluation}
\par
\begin{itemize}
\item \cite{balle2016end} 
\item \cite{toderici2017full}
\item \cite{theis2017lossy}
\item \cite{rippel2017real}
\item \cite{balle2018variational}
\end{itemize}


Notably, all previous VAE-based approaches have addressed the
non-differentiablility of quantization indirectly.
\paragraph{}
\cite{theis2017lossy} use an approximation for the derivative of the rounding
operator and optimize an upper bound on the error term introduced by the
quantiztion.
\paragraph{}
In \cite{balle2016end},\cite{balle2018variational} they model the quantizer by
adding uniform noise to the samples 
