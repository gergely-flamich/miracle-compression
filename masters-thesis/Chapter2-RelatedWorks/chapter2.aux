\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{mougeot1991image}
\citation{jiang1999image}
\citation{jiang1999image}
\citation{mougeot1991image}
\citation{jiang1999image}
\citation{bottou1998high}
\citation{denton2015deep}
\citation{gregor2015draw}
\citation{krizhevsky2009learning}
\citation{toderici2015variable}
\citation{toderici2017full}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Related Works}{17}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:related_works}{{2}{17}{Related Works}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Machine Learning-based Image Compression}{17}{section.2.1}}
\citation{balle2016end}
\citation{toderici2017full}
\citation{theis2017lossy}
\citation{rippel2017real}
\citation{balle2018variational}
\citation{johnston2018cvpr}
\citation{mentzer2018cvpr}
\citation{balle2016end}
\citation{theis2017lossy}
\citation{rippel2017real}
\citation{balle2018variational}
\citation{clic2018}
\citation{balle2016end}
\citation{deng2009imagenet}
\citation{theis2017lossy}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Comparison of Recent Works}{18}{section.2.2}}
\newlabel{sec:lit_comparison}{{2.2}{18}{Comparison of Recent Works}{section.2.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Note on Notation:}{18}{section*.11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Datasets and Input Pipelines}{18}{subsection.2.2.1}}
\newlabel{sec:related_works_datasets}{{2.2.1}{18}{Datasets and Input Pipelines}{subsection.2.2.1}{}}
\@writefile{toc}{\contentsline {paragraph}{\cite  {balle2016end}}{18}{section*.12}}
\@writefile{toc}{\contentsline {paragraph}{\cite  {theis2017lossy}}{18}{section*.13}}
\citation{rippel2017real}
\citation{balle2018variational}
\citation{balle2016end}
\citation{kodakdataset}
\citation{asuni2014testimages}
\citation{theis2017lossy}
\@writefile{toc}{\contentsline {paragraph}{\cite  {rippel2017real}}{19}{section*.14}}
\@writefile{toc}{\contentsline {paragraph}{\cite  {balle2018variational}}{19}{section*.15}}
\@writefile{toc}{\contentsline {paragraph}{}{19}{section*.16}}
\@writefile{toc}{\contentsline {paragraph}{Datasets for testing}{19}{section*.17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Architectures}{19}{subsection.2.2.2}}
\citation{balle2016end}
\citation{theis2017lossy}
\citation{balle2016end}
\citation{balle2018variational}
\citation{he2016deep}
\citation{theis2017lossy}
\citation{theis2017lossy}
\citation{theis2017lossy}
\citation{theis2017lossy}
\@writefile{toc}{\contentsline {paragraph}{\cite  {balle2016end}}{20}{section*.18}}
\newlabel{eq:gdn_def}{{2.1}{20}{\cite {balle2016end}}{equation.2.2.1}{}}
\newlabel{eq:igdn_def}{{2.2}{20}{\cite {balle2016end}}{equation.2.2.2}{}}
\@writefile{toc}{\contentsline {paragraph}{\cite  {theis2017lossy}}{20}{section*.19}}
\citation{rippel2017real}
\citation{rippel2017real}
\citation{rippel2017real}
\citation{rippel2017real}
\citation{rippel2017real}
\citation{balle2018variational}
\citation{balle2016end}
\citation{sonderby2016train}
\citation{balle2016end}
\citation{balle2018variational}
\citation{balle2018variational}
\citation{balle2016end}
\citation{balle2018variational}
\citation{balle2018variational}
\citation{rippel2017real}
\citation{toderici2017full}
\citation{balle2016end}
\citation{balle2018variational}
\citation{theis2017lossy}
\citation{toderici2017full}
\citation{balle2016end}
\citation{balle2018variational}
\citation{theis2017lossy}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Compressive Auto-Encoder architecture used by \cite  {theis2017lossy}. Note that for visual clarity only 2 residual blocks are displayed, in their experiments they used 3. They use a 6-component Gaussian Scale Mixture model (GSM) to model the quantization noise during the training of the architecture. The normalization layer performs batch normalization separately for each channel, denormalization is the analogous inverse operation. (Image taken from their \cite  {theis2017lossy}.)\relax }}{21}{figure.caption.20}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:comp_auto_arch}{{2.1}{21}{Compressive Auto-Encoder architecture used by \cite {theis2017lossy}. Note that for visual clarity only 2 residual blocks are displayed, in their experiments they used 3. They use a 6-component Gaussian Scale Mixture model (GSM) to model the quantization noise during the training of the architecture. The normalization layer performs batch normalization separately for each channel, denormalization is the analogous inverse operation. (Image taken from their \cite {theis2017lossy}.)\relax }{figure.caption.20}{}}
\@writefile{toc}{\contentsline {paragraph}{\cite  {rippel2017real}}{21}{section*.21}}
\@writefile{toc}{\contentsline {paragraph}{\cite  {balle2018variational}}{21}{section*.23}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Encoder architecture used by \cite  {rippel2017real}. All circular blocks denote convolutions. (Image taken from \cite  {rippel2017real}.)\relax }}{22}{figure.caption.22}}
\newlabel{fig:rippel_arch}{{2.2}{22}{Encoder architecture used by \cite {rippel2017real}. All circular blocks denote convolutions. (Image taken from \cite {rippel2017real}.)\relax }{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Analysis and synthesis transforms $g_a$ and $g_s$ along with first level quantizer $Q(\mathbf {y})$ used in \cite  {balle2016end}. This architecutre was then extended by \cite  {balle2018variational} with second level analysis and synthesis transforms $h_a$ and $h_s$, along with second level quantizer $Q(\mathbf {z})$. This full architecture is also the basis of our model. A slightly strange design choice on their part is since they will wish to force the second stage activations to be positive (it will be predicting a scale parameter), instead of using an exponential or softplus ($\qopname  \relax o{log}(1 + \qopname  \relax o{exp}\{x\})$) activation at the end, they take the absolute value of the input to the first layer, and rely on the ReLUs never giving negative values. We are not sure if this was meant to be a computational saving, as taking absolute values is certainly cheaper then either of the aforementioned standard ways of forcing positive values, or it if it gave better results. (Image taken from \cite  {balle2018variational})\relax }}{22}{figure.caption.24}}
\newlabel{fig:balle_ladder_arch}{{2.3}{22}{Analysis and synthesis transforms $g_a$ and $g_s$ along with first level quantizer $Q(\vec {y})$ used in \cite {balle2016end}. This architecutre was then extended by \cite {balle2018variational} with second level analysis and synthesis transforms $h_a$ and $h_s$, along with second level quantizer $Q(\vec {z})$. This full architecture is also the basis of our model. A slightly strange design choice on their part is since they will wish to force the second stage activations to be positive (it will be predicting a scale parameter), instead of using an exponential or softplus ($\log (1 + \exp \{x\})$) activation at the end, they take the absolute value of the input to the first layer, and rely on the ReLUs never giving negative values. We are not sure if this was meant to be a computational saving, as taking absolute values is certainly cheaper then either of the aforementioned standard ways of forcing positive values, or it if it gave better results. (Image taken from \cite {balle2018variational})\relax }{figure.caption.24}{}}
\citation{balle2016end}
\citation{balle2018variational}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Comparison of quantization error and its relaxations. \textbf  {A)} Original image. \textbf  {B)} Artifacts that result from using rounding as the quantizer. \textbf  {C)} Stochastic rounding used by \cite  {toderici2017full}. \textbf  {D)} Uniform additive noise used by \cite  {balle2016end} and \cite  {balle2018variational}. (Image taken from \cite  {theis2017lossy}.)\relax }}{23}{figure.caption.25}}
\newlabel{fig:quantization_models}{{2.4}{23}{Comparison of quantization error and its relaxations. \textbf {A)} Original image. \textbf {B)} Artifacts that result from using rounding as the quantizer. \textbf {C)} Stochastic rounding used by \cite {toderici2017full}. \textbf {D)} Uniform additive noise used by \cite {balle2016end} and \cite {balle2018variational}. (Image taken from \cite {theis2017lossy}.)\relax }{figure.caption.25}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Addressing Non-Differentiability}{23}{subsection.2.2.3}}
\newlabel{sec:comp_quant}{{2.2.3}{23}{Addressing Non-Differentiability}{subsection.2.2.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{Quantization}{23}{section*.26}}
\newlabel{eq:quantization_step}{{2.3}{23}{Quantization}{equation.2.2.3}{}}
\citation{theis2017lossy}
\citation{rippel2017real}
\citation{balle2016end}
\citation{toderici2017full}
\citation{rippel2017real}
\citation{balle2016end}
\citation{theis2017lossy}
\@writefile{toc}{\contentsline {paragraph}{\cite  {balle2016end} and \cite  {balle2018variational}}{24}{section*.27}}
\@writefile{toc}{\contentsline {paragraph}{\cite  {theis2017lossy}}{24}{section*.28}}
\@writefile{toc}{\contentsline {paragraph}{\cite  {rippel2017real}}{24}{section*.29}}
\@writefile{toc}{\contentsline {subsubsection}{Rate Estimation}{24}{section*.30}}
\@writefile{toc}{\contentsline {paragraph}{\cite  {balle2016end}}{24}{section*.31}}
\citation{balle2018variational}
\citation{bishop1998latent}
\citation{balle2016end}
\citation{rissanen1981universal}
\@writefile{toc}{\contentsline {paragraph}{\cite  {theis2017lossy}}{25}{section*.32}}
\@writefile{toc}{\contentsline {paragraph}{\cite  {balle2018variational}}{25}{section*.33}}
\citation{balle2016end}
\citation{theis2017lossy}
\citation{rippel2017real}
\citation{balle2018variational}
\citation{balle2016end}
\citation{theis2017lossy}
\citation{balle2018variational}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Coding}{26}{subsection.2.2.4}}
\@writefile{toc}{\contentsline {paragraph}{\cite  {balle2016end}}{26}{section*.34}}
\@writefile{toc}{\contentsline {paragraph}{\cite  {theis2017lossy}}{26}{section*.35}}
\@writefile{toc}{\contentsline {paragraph}{\cite  {rippel2017real}}{26}{section*.36}}
\@writefile{toc}{\contentsline {paragraph}{\cite  {balle2018variational}}{26}{section*.37}}
\citation{rippel2017real}
\citation{kingma2014adam}
\citation{balle2016end}
\citation{theis2017lossy}
\citation{rippel2017real}
\citation{rippel2017real}
\citation{rippel2017real}
\citation{rippel2017real}
\citation{rippel2017real}
\citation{balle2018variational}
\citation{balle2016end}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.5}Training}{27}{subsection.2.2.5}}
\@writefile{toc}{\contentsline {paragraph}{\cite  {balle2016end}}{27}{section*.38}}
\@writefile{toc}{\contentsline {paragraph}{\cite  {theis2017lossy}}{27}{section*.39}}
\@writefile{toc}{\contentsline {paragraph}{\cite  {rippel2017real}}{27}{section*.40}}
\citation{balle2016end}
\citation{kodakdataset}
\citation{psnr}
\citation{msssim}
\citation{rippel2017real}
\citation{balle2018variational}
\citation{balle2016end}
\citation{balle2018variational}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Compression pipeline used by \cite  {rippel2017real}. The red boxes show the terms used in their loss function. (Image taken from \cite  {rippel2017real}.)\relax }}{28}{figure.caption.41}}
\newlabel{fig:rippel_pipeline}{{2.5}{28}{Compression pipeline used by \cite {rippel2017real}. The red boxes show the terms used in their loss function. (Image taken from \cite {rippel2017real}.)\relax }{figure.caption.41}{}}
\@writefile{toc}{\contentsline {paragraph}{\cite  {balle2018variational}}{28}{section*.42}}
\newlabel{eq:balle_var_train_objective}{{2.4}{28}{\cite {balle2018variational}}{equation.2.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.6}Evaluation}{28}{subsection.2.2.6}}
\@setckpt{Chapter2-RelatedWorks/chapter2}{
\setcounter{page}{30}
\setcounter{equation}{4}
\setcounter{enumi}{4}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{2}
\setcounter{subsection}{6}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{5}
\setcounter{table}{0}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{NAT@ctr}{0}
\setcounter{parentequation}{0}
\setcounter{Item}{13}
\setcounter{Hfootnote}{0}
\setcounter{Hy@AnnotLevel}{0}
\setcounter{bookmark@seq@number}{28}
\setcounter{ContinuedFloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{0}
\setcounter{ALG@line}{0}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{theorem}{2}
\setcounter{section@level}{2}
}
