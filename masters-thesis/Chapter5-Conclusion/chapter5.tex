%!TEX root = ../thesis.tex
%*******************************************************************************
%****************************** Second Chapter *********************************
%*******************************************************************************

\chapter{Conclusion}

\section{Discussion}
\par
In this work, we gave an introduction to image compression and machine learning
based compression. Based on the MDL principle and the Bits-back Argument, as
well as more recent work in information theory and neural network compression,
we developed a general lossy compression framework, and described how previous
quantization based approaches fit into it. We gave a comparative review of
recent influential works in the field of neural image compression. We
demonstrated the efficiency of the framework we developed by applying it to
image compression. We trained several Probabilistic Ladder Networks , optimized
for different rate-distrotion trade-offs, and achieved results competitive with
the current state of the art. We also present 3 coded sampling algorithms with
different advantages and disadvantages that may be used to compress data using
our framework. We present detailed analysis justifying our model choices.

\section{Future Work}
\par 
There are 
Several metrics to optimise for:
- compression quality
- compression size
- compression time
- compressor size
- compressor power consumption
- robustness of compressor (i.e. resistance to errors / adversarial attacks)
- security / privacy of compression
- scalability: image size, image quality


