\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces \textbf {a)} \texttt {kodim21.png} from the Kodak Dataset. \textbf {b)} A random sample from the VAE posterior. \textbf {c)} Posterior means in a randomly selected channel. \textbf {d)} Posterior standard deviations in the same randomly selected channel. We can see that there is a lot of structure in the latent space, on which the full indepenence assumption will have a detrimental effect. (We have examined several random channels and observed the similarly high structure. We present the above cross-section without preference.)\relax }}{28}{figure.caption.14}
\contentsline {figure}{\numberline {3.2}{\ignorespaces PLN network architecture. The blocks signal data transformations, the arrows signal the flow of information. \textbf {Block descriptions:} \textit {Conv2D:} 2D convolutions along the spatial dimensions, where the $W\times H \times C / S$ implies a $W \times H$ convolution kernel, with $C$ target channels and $S$ gives the downsampling rate (given a preceding letter ``d'') or the upsampling rate (given a preceding letter ``u''). If the slash is missing, it means that there is no up/downsampling. All convolutions operate in \texttt {same} mode with mirror padding. \textit {GDN / IGDN:} these are the non-linearities described in \cite {balle2016end}. \textit {Leaky ReLU:} elementwise non-linearity defined as $\qopname \relax m{max}\{x, \mitalpha x\}$, where we set $\mitalpha =0.2$. \textit {Sigmoid:} Elementwise non-linearity defined as $\frac {1}{1 + \qopname \relax o{exp}\{-x\}}$. We ran all experiments presented here with $N = 196, M = 128, F = 128, G = 24$.\relax }}{32}{figure.caption.18}
\contentsline {figure}{\numberline {3.3}{\ignorespaces We continue the analysis of the latent spaces induced by \texttt {kodim21} from the Kodak Dataset. Akin to Figure \ref {fig:vae_rand_posterior}, we have selected a random channel for both the first and second levels each and present the spatial cross-sections along these channels. \textbf {a)} Level 1 prior means. \textbf {b)} Level 1 posterior means. \textbf {c)} Level 1 prior standard deviations. \textbf {d)} Level 1 posterior standard deviations. \textbf {e)} Random sample from the Level 1 posterior. \textbf {f)} The sample from \textbf {e)} standardized according to the level 1 prior. Most structure from the sample is removed, hence we see that the second level has successfully learned a lot of the dependencies between the latents. We have checked cross-sections along several randomly selected channels and observed the same phenomenon. We present the above with no preference.\relax }}{34}{figure.caption.19}
\contentsline {figure}{\numberline {3.4}{\ignorespaces We continue the analysis of the latent spaces induced by \texttt {kodim21} from the Kodak Dataset. Akin to Figures \ref {fig:vae_rand_posterior} and \ref {fig:ladder_rand_posterior}, we have selected a random channel for both the first and second levels each and present the spatial cross-sections along these channels. \textbf {a)} Level 1 prior means. \textbf {b)} Level 1 posterior means. \textbf {c)} Level 1 prior standard deviations. \textbf {d)} Level 1 posterior standard deviations. \textbf {e)} Random sample from the Level 1 posterior. \textbf {f)} The sample from \textbf {e)} standardized according to the level 1 prior. We observe the same phenomenon, with no significant difference, as in Figure \ref {fig:ladder_rand_posterior}. We note that while the posterior sample may seem like it has more significant structure than the one in the previous Figure. This is only coincidence; some of the regular PLN's channels contain similar structure, and some of the $\mitgamma $-PLN's channels contain more noisy elements. \relax }}{36}{figure.caption.20}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.3}{\ignorespaces ladder on kodim21\relax }}{47}{figure.caption.29}
\contentsline {figure}{\numberline {4.1}{\ignorespaces Rate-Distorsion curves of several relevant methods. Please see Section \ref {sec:experimental_results} for the description of how we obtained each curve. We note that the MS-SSIM results are presented in decibels, where the conversion is done using the formula $-10 \cdot \qopname \relax o{log}_{10}\left ( 1 - \text {MS-SSIM}(\mathbf {x}, \hat {\mathbf {x}}) \right )$. The PSNR is computed from the mean squared error, using the formula $-10 \cdot \qopname \relax o{log}_{10}\text {MSE}(\mathbf {x}, \hat {\mathbf {x}})$.\relax }}{48}{figure.caption.27}
\contentsline {figure}{\numberline {4.2}{\ignorespaces Contribution of the second level to the rate, plotted agains the actual rate. \textbf {Left:} Contribution in BPP, \textbf {Right:} Contribution in percentages. We see that for lower bitrates there is more contribution from the second level and it quickly decreases for higher rates. It is also clear that on the same bitrates, the $\mitgamma $-PLN requires less contribution from the second level than regular PLN.\relax }}{49}{figure.caption.28}
\contentsline {figure}{\numberline {4.4}{\ignorespaces ladder on kodim21\relax }}{49}{figure.caption.30}
\contentsline {figure}{\numberline {4.5}{\ignorespaces ladder on kodim21\relax }}{50}{figure.caption.31}
\contentsline {figure}{\numberline {4.6}{\ignorespaces ladder on kodim21\relax }}{50}{figure.caption.32}
\contentsline {figure}{\numberline {4.7}{\ignorespaces ladder on kodim21\relax }}{51}{figure.caption.33}
\contentsline {figure}{\numberline {4.8}{\ignorespaces ladder on kodim21\relax }}{51}{figure.caption.34}
\contentsline {figure}{\numberline {4.9}{\ignorespaces Coding times of models plotted agains their rates. \textbf {Left:} Regular PLNs. \textbf {Right:} $\mitgamma $-PLNs. The striped lines indicate the concrete positions of our models in the rate line. While it seems that there is a linear relationship between rate and coding time, we do not have enough datapoints to conclude this.\relax }}{52}{figure.caption.35}
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
