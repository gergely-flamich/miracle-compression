\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Compressive Auto-Encoder architecture used by \cite {theis2017lossy}. Note that for visual clarity only 2 residual blocks are displayed, in their experiments they used 3. They use a 6-component Gaussian Scale Mixture model (GSM) to model the quantization noise during the training of the architecture. The normalization layer performs batch normalization separately for each channel, denormalization is the analogous inverse operation. (Image taken from their \cite {theis2017lossy}.)\relax }}{21}{figure.caption.20}
\contentsline {figure}{\numberline {2.2}{\ignorespaces Encoder architecture used by \cite {rippel2017real}. All circular blocks denote convolutions. (Image taken from \cite {rippel2017real}.)\relax }}{22}{figure.caption.22}
\contentsline {figure}{\numberline {2.3}{\ignorespaces Analysis and synthesis transforms $g_a$ and $g_s$ along with first level quantizer $Q(\mathbf {y})$ used in \cite {balle2016end}. This architecutre was then extended by \cite {balle2018variational} with second level analysis and synthesis transforms $h_a$ and $h_s$, along with second level quantizer $Q(\mathbf {z})$. This full architecture is also the basis of our model. A slightly strange design choice on their part is since they will wish to force the second stage activations to be positive (it will be predicting a scale parameter), instead of using an exponential or softplus ($\qopname \relax o{log}(1 + \qopname \relax o{exp}\{x\})$) activation at the end, they take the absolute value of the input to the first layer, and rely on the ReLUs never giving negative values. We are not sure if this was meant to be a computational saving, as taking absolute values is certainly cheaper then either of the aforementioned standard ways of forcing positive values, or it if it gave better results. (Image taken from \cite {balle2018variational})\relax }}{22}{figure.caption.24}
\contentsline {figure}{\numberline {2.4}{\ignorespaces Comparison of quantization error and its relaxations. \textbf {A)} Original image. \textbf {B)} Artifacts that result from using rounding as the quantizer. \textbf {C)} Stochastic rounding used by \cite {toderici2017full}. \textbf {D)} Uniform additive noise used by \cite {balle2016end} and \cite {balle2018variational}. (Image taken from \cite {theis2017lossy}.)\relax }}{23}{figure.caption.25}
\contentsline {figure}{\numberline {2.5}{\ignorespaces Compression pipeline used by \cite {rippel2017real}. The red boxes show the terms used in their loss function. (Image taken from \cite {rippel2017real}.)\relax }}{28}{figure.caption.41}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces \textbf {a)} \texttt {kodim21.png} from the Kodak Dataset. \textbf {b)} A random sample from the VAE posterior. \textbf {c)} Posterior means in a randomly selected channel. \textbf {d)} Posterior standard deviations in the same randomly selected channel. We can see that there is a lot of structure in the latent space, on which the full indepenence assumption will have a detrimental effect. (We have examined several random channels and observed the similarly high structure. We present the above cross-section without preference.)\relax }}{34}{figure.caption.44}
\contentsline {figure}{\numberline {3.2}{\ignorespaces PLN network architecture. The blocks signal data transformations, the arrows signal the flow of information. \textbf {Block descriptions:} \textit {Conv2D:} 2D convolutions along the spatial dimensions, where the $W\times H \times C / S$ implies a $W \times H$ convolution kernel, with $C$ target channels and $S$ gives the downsampling rate (given a preceding letter ``d'') or the upsampling rate (given a preceding letter ``u''). If the slash is missing, it means that there is no up/downsampling. All convolutions operate in \texttt {same} mode with mirror padding. \textit {GDN / IGDN:} these are the non-linearities described in \cite {balle2016end}. \textit {Leaky ReLU:} elementwise non-linearity defined as $\qopname \relax m{max}\{x, \mitalpha x\}$, where we set $\mitalpha =0.2$. \textit {Sigmoid:} Elementwise non-linearity defined as $\frac {1}{1 + \qopname \relax o{exp}\{-x\}}$. We ran all experiments presented here with $N = 196, M = 128, F = 128, G = 24$.\relax }}{38}{figure.caption.48}
\contentsline {figure}{\numberline {3.3}{\ignorespaces We continue the analysis of the latent spaces induced by \texttt {kodim21} from the Kodak Dataset. Akin to Figure \ref {fig:vae_rand_posterior}, we have selected a random channel for both the first and second levels each and present the spatial cross-sections along these channels. \textbf {a)} Level 1 prior means. \textbf {b)} Level 1 posterior means. \textbf {c)} Level 1 prior standard deviations. \textbf {d)} Level 1 posterior standard deviations. \textbf {e)} Random sample from the Level 1 posterior. \textbf {f)} The sample from \textbf {e)} standardized according to the level 1 prior. Most structure from the sample is removed, hence we see that the second level has successfully learned a lot of the dependencies between the latents. We have checked cross-sections along several randomly selected channels and observed the same phenomenon. We present the above with no preference.\relax }}{40}{figure.caption.49}
\contentsline {figure}{\numberline {3.4}{\ignorespaces We continue the analysis of the latent spaces induced by \texttt {kodim21} from the Kodak Dataset. Akin to Figures \ref {fig:vae_rand_posterior} and \ref {fig:ladder_rand_posterior}, we have selected a random channel for both the first and second levels each and present the spatial cross-sections along these channels. \textbf {a)} Level 1 prior means. \textbf {b)} Level 1 posterior means. \textbf {c)} Level 1 prior standard deviations. \textbf {d)} Level 1 posterior standard deviations. \textbf {e)} Random sample from the Level 1 posterior. \textbf {f)} The sample from \textbf {e)} standardized according to the level 1 prior. We observe the same phenomenon, with no significant difference, as in Figure \ref {fig:ladder_rand_posterior}. We note that while the posterior sample may seem like it has more significant structure than the one in the previous Figure. This is only coincidence; some of the regular PLN's channels contain similar structure, and some of the $\mitgamma $-PLN's channels contain more noisy elements. \relax }}{42}{figure.caption.50}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.3}{\ignorespaces ladder on kodim21\relax }}{53}{figure.caption.59}
\contentsline {figure}{\numberline {4.1}{\ignorespaces Rate-Distorsion curves of several relevant methods. Please see Section \ref {sec:experimental_results} for the description of how we obtained each curve. We note that the MS-SSIM results are presented in decibels, where the conversion is done using the formula $-10 \cdot \qopname \relax o{log}_{10}\left ( 1 - \text {MS-SSIM}(\mathbf {x}, \hat {\mathbf {x}}) \right )$. The PSNR is computed from the mean squared error, using the formula $-10 \cdot \qopname \relax o{log}_{10}\text {MSE}(\mathbf {x}, \hat {\mathbf {x}})$.\relax }}{54}{figure.caption.57}
\contentsline {figure}{\numberline {4.2}{\ignorespaces Contribution of the second level to the rate, plotted agains the actual rate. \textbf {Left:} Contribution in BPP, \textbf {Right:} Contribution in percentages. We see that for lower bitrates there is more contribution from the second level and it quickly decreases for higher rates. It is also clear that on the same bitrates, the $\mitgamma $-PLN requires less contribution from the second level than regular PLN.\relax }}{55}{figure.caption.58}
\contentsline {figure}{\numberline {4.4}{\ignorespaces ladder on kodim21\relax }}{55}{figure.caption.60}
\contentsline {figure}{\numberline {4.5}{\ignorespaces ladder on kodim21\relax }}{56}{figure.caption.61}
\contentsline {figure}{\numberline {4.6}{\ignorespaces ladder on kodim21\relax }}{56}{figure.caption.62}
\contentsline {figure}{\numberline {4.7}{\ignorespaces ladder on kodim21\relax }}{57}{figure.caption.63}
\contentsline {figure}{\numberline {4.8}{\ignorespaces ladder on kodim21\relax }}{57}{figure.caption.64}
\contentsline {figure}{\numberline {4.9}{\ignorespaces Coding times of models plotted agains their rates. \textbf {Left:} Regular PLNs. \textbf {Right:} $\mitgamma $-PLNs. The striped lines indicate the concrete positions of our models in the rate line. While it seems that there is a linear relationship between rate and coding time, we do not have enough datapoints to conclude this.\relax }}{58}{figure.caption.65}
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
