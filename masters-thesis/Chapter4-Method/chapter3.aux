\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{clic2018}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Method}{31}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:method}{{3}{31}{Method}{chapter.3}{}}
\citation{balle2016end}
\citation{rippel2017real}
\citation{balle2016end}
\citation{balle2018variational}
\citation{balle2015density}
\citation{balle2016end}
\citation{balle2016end}
\citation{theis2017lossy}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Dataset and Preprocessing}{32}{section.3.1}}
\newlabel{sec:dataset_preproc}{{3.1}{32}{Dataset and Preprocessing}{section.3.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Architectures}{32}{section.3.2}}
\newlabel{sec:architectures}{{3.2}{32}{Architectures}{section.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}VAEs}{32}{subsection.3.2.1}}
\newlabel{sec:method_vaes}{{3.2.1}{32}{VAEs}{subsection.3.2.1}{}}
\citation{jain1989fundamentals}
\citation{clic2018winner}
\citation{portilla2003image}
\citation{theis2017lossy}
\@writefile{toc}{\contentsline {paragraph}{Note:}{33}{section*.43}}
\@writefile{toc}{\contentsline {paragraph}{}{33}{section*.45}}
\@writefile{toc}{\contentsline {paragraph}{A note on the latent distributions}{33}{section*.46}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces \textbf  {a)} \texttt  {kodim21.png} from the Kodak Dataset. \textbf  {b)} A random sample from the VAE posterior. \textbf  {c)} Posterior means in a randomly selected channel. \textbf  {d)} Posterior standard deviations in the same randomly selected channel. We can see that there is a lot of structure in the latent space, on which the full indepenence assumption will have a detrimental effect. (We have examined several random channels and observed the similarly high structure. We present the above cross-section without preference.)\relax }}{34}{figure.caption.44}}
\newlabel{fig:vae_rand_posterior}{{3.1}{34}{\textbf {a)} \texttt {kodim21.png} from the Kodak Dataset. \textbf {b)} A random sample from the VAE posterior. \textbf {c)} Posterior means in a randomly selected channel. \textbf {d)} Posterior standard deviations in the same randomly selected channel. We can see that there is a lot of structure in the latent space, on which the full indepenence assumption will have a detrimental effect. (We have examined several random channels and observed the similarly high structure. We present the above cross-section without preference.)\relax }{figure.caption.44}{}}
\citation{zhao2015loss}
\citation{zhao2015loss}
\citation{girod1993s}
\citation{eskicioglu1994image}
\citation{zhao2015loss}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Image reconstruction quality comparison on the task of joint image denoising and demosaicing, for the same architecture optimized using different distortion metrics. \textbf  {a)-b)} Show the original image. \textbf  {c)} Show the input to the networks. \textbf  {d) - h)} Show reconstructions using various distrotion metrics. Mix is (approximately) defined as $(1 - \mitlambda ) L_1 + \mitlambda \text  {MS-SSIM}$ for $\mitlambda = 0.84$. The differences are best seen on the electronic version, zoomed in. We can clearly see the patchy artifacts introduced by Mean Squared Error (\textbf  {d)}), and how much better Mean Absolute Error (\textbf  {e)}) performs compared to it. (Image taken from \cite  {zhao2015loss}. We changed the fonts of their captions to a sans-serif font for better readabitity.)\relax }}{35}{figure.caption.47}}
\newlabel{fig:zhao_loss_comparison}{{3.2}{35}{Image reconstruction quality comparison on the task of joint image denoising and demosaicing, for the same architecture optimized using different distortion metrics. \textbf {a)-b)} Show the original image. \textbf {c)} Show the input to the networks. \textbf {d) - h)} Show reconstructions using various distrotion metrics. Mix is (approximately) defined as $(1 - \lambda ) L_1 + \lambda \text {MS-SSIM}$ for $\lambda = 0.84$. The differences are best seen on the electronic version, zoomed in. We can clearly see the patchy artifacts introduced by Mean Squared Error (\textbf {d)}), and how much better Mean Absolute Error (\textbf {e)}) performs compared to it. (Image taken from \cite {zhao2015loss}. We changed the fonts of their captions to a sans-serif font for better readabitity.)\relax }{figure.caption.47}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Data Likelihood and Training Objective}{35}{subsection.3.2.2}}
\newlabel{eq:regular_vae_elbo}{{3.1}{35}{Data Likelihood and Training Objective}{equation.3.2.1}{}}
\citation{balle2016end}
\citation{balle2016end}
\citation{sonderby2016train}
\citation{sonderby2016train}
\newlabel{eq:laplace_likelihood}{{3.2}{36}{Data Likelihood and Training Objective}{equation.3.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Probabilistic Ladder Network}{36}{subsection.3.2.3}}
\newlabel{sec:prob_ladder_networks}{{3.2.3}{36}{Probabilistic Ladder Network}{subsection.3.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces PLN network architecture. The blocks signal data transformations, the arrows signal the flow of information. \textbf  {Block descriptions:} \textit  {Conv2D:} 2D convolutions along the spatial dimensions, where the $W\times H \times C / S$ implies a $W \times H$ convolution kernel, with $C$ target channels and $S$ gives the downsampling rate (given a preceding letter ``d'') or the upsampling rate (given a preceding letter ``u''). If the slash is missing, it means that there is no up/downsampling. All convolutions operate in \texttt  {same} mode with mirror padding. \textit  {GDN / IGDN:} these are the non-linearities described in \cite  {balle2016end}. \textit  {Leaky ReLU:} elementwise non-linearity defined as $\qopname  \relax m{max}\{x, \mitalpha x\}$, where we set $\mitalpha =0.2$. \textit  {Sigmoid:} Elementwise non-linearity defined as $\frac  {1}{1 + \qopname  \relax o{exp}\{-x\}}$. We ran all experiments presented here with $N = 196, M = 128, F = 128, G = 24$.\relax }}{37}{figure.caption.48}}
\newlabel{fig:pln_architecture}{{3.3}{37}{PLN network architecture. The blocks signal data transformations, the arrows signal the flow of information. \textbf {Block descriptions:} \textit {Conv2D:} 2D convolutions along the spatial dimensions, where the $W\times H \times C / S$ implies a $W \times H$ convolution kernel, with $C$ target channels and $S$ gives the downsampling rate (given a preceding letter ``d'') or the upsampling rate (given a preceding letter ``u''). If the slash is missing, it means that there is no up/downsampling. All convolutions operate in \texttt {same} mode with mirror padding. \textit {GDN / IGDN:} these are the non-linearities described in \cite {balle2016end}. \textit {Leaky ReLU:} elementwise non-linearity defined as $\max \{x, \alpha x\}$, where we set $\alpha =0.2$. \textit {Sigmoid:} Elementwise non-linearity defined as $\frac {1}{1 + \exp \{-x\}}$. We ran all experiments presented here with $N = 196, M = 128, F = 128, G = 24$.\relax }{figure.caption.48}{}}
\citation{balle2018variational}
\citation{sonderby2016train}
\citation{sonderby2016train}
\citation{ioffe2015batch}
\@writefile{toc}{\contentsline {paragraph}{}{38}{section*.50}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Training}{38}{section.3.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces We continue the analysis of the latent spaces induced by \texttt  {kodim21} from the Kodak Dataset. Akin to Figure \ref  {fig:vae_rand_posterior}, we have selected a random channel for both the first and second levels each and present the spatial cross-sections along these channels. \textbf  {a)} Level 1 prior means. \textbf  {b)} Level 1 posterior means. \textbf  {c)} Level 1 prior standard deviations. \textbf  {d)} Level 1 posterior standard deviations. \textbf  {e)} Random sample from the Level 1 posterior. \textbf  {f)} The sample from \textbf  {e)} standardized according to the level 1 prior. Most structure from the sample is removed, hence we see that the second level has successfully learned a lot of the dependencies between the latents. We have checked cross-sections along several randomly selected channels and observed the same phenomenon. We present the above with no preference.\relax }}{39}{figure.caption.49}}
\newlabel{fig:ladder_rand_posterior}{{3.4}{39}{We continue the analysis of the latent spaces induced by \texttt {kodim21} from the Kodak Dataset. Akin to Figure \ref {fig:vae_rand_posterior}, we have selected a random channel for both the first and second levels each and present the spatial cross-sections along these channels. \textbf {a)} Level 1 prior means. \textbf {b)} Level 1 posterior means. \textbf {c)} Level 1 prior standard deviations. \textbf {d)} Level 1 posterior standard deviations. \textbf {e)} Random sample from the Level 1 posterior. \textbf {f)} The sample from \textbf {e)} standardized according to the level 1 prior. Most structure from the sample is removed, hence we see that the second level has successfully learned a lot of the dependencies between the latents. We have checked cross-sections along several randomly selected channels and observed the same phenomenon. We present the above with no preference.\relax }{figure.caption.49}{}}
\citation{balle2018variational}
\citation{dai2019diagnosing}
\citation{dai2019diagnosing}
\citation{goodfellow2014generative}
\citation{dai2019diagnosing}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Learning the Variance of the Likelihood}{40}{subsection.3.3.1}}
\newlabel{sec:learn_gamma}{{3.3.1}{40}{Learning the Variance of the Likelihood}{subsection.3.3.1}{}}
\citation{sonderby2016train}
\citation{havasi2018minimal}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Sampling and Coding}{41}{section.3.4}}
\newlabel{sec:coded_sampling}{{3.4}{41}{Sampling and Coding}{section.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces We continue the analysis of the latent spaces induced by \texttt  {kodim21} from the Kodak Dataset. Akin to Figures \ref  {fig:vae_rand_posterior} and \ref  {fig:ladder_rand_posterior}, we have selected a random channel for both the first and second levels each and present the spatial cross-sections along these channels. \textbf  {a)} Level 1 prior means. \textbf  {b)} Level 1 posterior means. \textbf  {c)} Level 1 prior standard deviations. \textbf  {d)} Level 1 posterior standard deviations. \textbf  {e)} Random sample from the Level 1 posterior. \textbf  {f)} The sample from \textbf  {e)} standardized according to the level 1 prior. We observe the same phenomenon, with no significant difference, as in Figure \ref  {fig:ladder_rand_posterior}. We note that while the posterior sample may seem like it has more significant structure than the one in the previous Figure. This is only coincidence; some of the regular PLN's channels contain similar structure, and some of the $\mitgamma $-PLN's channels contain more noisy elements. \relax }}{42}{figure.caption.51}}
\newlabel{fig:gamma_rand_posterior}{{3.5}{42}{We continue the analysis of the latent spaces induced by \texttt {kodim21} from the Kodak Dataset. Akin to Figures \ref {fig:vae_rand_posterior} and \ref {fig:ladder_rand_posterior}, we have selected a random channel for both the first and second levels each and present the spatial cross-sections along these channels. \textbf {a)} Level 1 prior means. \textbf {b)} Level 1 posterior means. \textbf {c)} Level 1 prior standard deviations. \textbf {d)} Level 1 posterior standard deviations. \textbf {e)} Random sample from the Level 1 posterior. \textbf {f)} The sample from \textbf {e)} standardized according to the level 1 prior. We observe the same phenomenon, with no significant difference, as in Figure \ref {fig:ladder_rand_posterior}. We note that while the posterior sample may seem like it has more significant structure than the one in the previous Figure. This is only coincidence; some of the regular PLN's channels contain similar structure, and some of the $\gamma $-PLN's channels contain more noisy elements. \relax }{figure.caption.51}{}}
\citation{harsha2007communication}
\citation{havasi2018minimal}
\citation{rissanen1981universal}
\citation{chen1999empirical}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Parallelized Rejection Sampling and Arithmetic Coding}{43}{subsection.3.4.1}}
\@writefile{toc}{\contentsline {paragraph}{Sampling}{43}{section*.52}}
\@writefile{toc}{\contentsline {paragraph}{Coding}{43}{section*.53}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Parallelized, bit-budgeted rejection sampling based on Algorithm \ref  {alg:harsha_rej_sampling}.\relax }}{44}{algorithm.1}}
\newlabel{alg:multivariate_rej_samp}{{1}{44}{Parallelized, bit-budgeted rejection sampling based on Algorithm \ref {alg:harsha_rej_sampling}.\relax }{algorithm.1}{}}
\citation{adel1962algorithm}
\@writefile{toc}{\contentsline {paragraph}{A note on the Arithmetic Coder}{45}{section*.54}}
\@writefile{toc}{\contentsline {paragraph}{Issues}{45}{section*.55}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Greedy Coded Sampling}{45}{subsection.3.4.2}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Greedy Coded Sampler\relax }}{46}{algorithm.2}}
\newlabel{alg:greedy_sampler}{{2}{46}{Greedy Coded Sampler\relax }{algorithm.2}{}}
\@writefile{toc}{\contentsline {paragraph}{A note on implementation}{47}{section*.56}}
\@writefile{toc}{\contentsline {paragraph}{Issues}{47}{section*.57}}
\citation{havasi2018minimal}
\citation{havasi2018minimal}
\citation{havasi2018minimal}
\citation{havasi2018minimal}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Adaptive Importance Sampling}{48}{subsection.3.4.3}}
\@writefile{toc}{\contentsline {paragraph}{}{48}{section*.58}}
\@writefile{toc}{\contentsline {paragraph}{Greedy Sampler}{48}{section*.59}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Adaptive Importance Sampler based on Algorithm \ref  {alg:miracle_imp_samp}, introduced by \cite  {havasi2018minimal}\relax }}{49}{algorithm.3}}
\newlabel{alg:adaptive_importance_sampler}{{3}{49}{Adaptive Importance Sampler based on Algorithm \ref {alg:miracle_imp_samp}, introduced by \cite {havasi2018minimal}\relax }{algorithm.3}{}}
\@setckpt{Chapter3-Method/chapter3}{
\setcounter{page}{51}
\setcounter{equation}{2}
\setcounter{enumi}{4}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{2}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{3}
\setcounter{section}{4}
\setcounter{subsection}{3}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{5}
\setcounter{table}{0}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{NAT@ctr}{0}
\setcounter{parentequation}{0}
\setcounter{Item}{17}
\setcounter{Hfootnote}{2}
\setcounter{Hy@AnnotLevel}{0}
\setcounter{bookmark@seq@number}{41}
\setcounter{ContinuedFloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{3}
\setcounter{ALG@line}{38}
\setcounter{ALG@rem}{38}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{theorem}{2}
\setcounter{section@level}{4}
}
