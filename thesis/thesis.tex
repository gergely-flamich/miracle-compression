
% ==============================================================================
% ==============================================================================
%
% HEADER
%
% ==============================================================================
% ==============================================================================

\documentclass{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphicx}

\usepackage[
backend=biber,
style=alphabetic,
%citestyle=authoryear
]{biblatex}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{float}
\usepackage{hyperref}
\usepackage{algorithm, algpseudocode}


\usepackage[a4paper, total={6in, 8in}]{geometry}


% ==============================================================================
% ==============================================================================
%
% DEFINITIONS
%
% ==============================================================================
% ==============================================================================
\addbibresource{cite.bib}


\renewcommand{\vec}[1]{\mathbf{#1}}
\renewcommand{\d}{\text{d}}

\renewcommand{\L}{\mathcal{L}}

\newcommand{\A}{\mathcal{A}}
\newcommand{\E}{\mathcal{E}}

\newcommand{\Oh}{\mathcal{O}}
\newcommand{\KL}{\mathrm{KL}}
\newcommand{\Exp}{\mathbb{E}}
\newcommand{\I}{\mathbb{I}}

\newcommand{\ImSpace}{\mathcal{X}}

\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Ints}{\mathbb{Z}}
\newcommand{\Nats}{\mathbb{N}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}


\title{Compression without Quantization}
\author{Gergely Flamich}

% ==============================================================================
% ==============================================================================
%
% START OF THESIS
%
% ==============================================================================
% ==============================================================================

\begin{document}

\input{titlepage.tex}

% ==============================================================================
%
% DECLARATION
%
% ==============================================================================

\vspace{2cm}

\begin{center}
\Huge
\textbf{Declaration}
\end{center}

\vspace{1cm}

\large
\noindent I, Gergely Flamich of St John's College, being a candidate for the
MPhil in Machine Learning and Machine Intelligence, hereby declare that this
report and the work described in it are my own work, unaided except as may be
specified below, and that the report does not contain material that has
already been used to any substantial extent for a comparable purpose.


\vspace{2cm}

\large
\noindent
Wordcount: \textbf{16384} words

\newpage

% ==============================================================================
%
% ACKNOWLEDGEMENTS
%
% ==============================================================================

\vspace{2cm}

\begin{center}
\Huge
\textbf{Acknowledgements}
\end{center}

\vspace{1cm}



\newpage

% ==============================================================================
%
% ABSTRACT
%
% ==============================================================================

\begin{abstract}
  We provide an implementation of our proposed method written in \texttt{Tensorflow}
  \cite{tensorflow2015-whitepaper} and \texttt{Sonnet} \cite{sonnetblog},
  available on GitHub \footnotemark.
\end{abstract}

\footnotetext{https://github.com/gergely-flamich/miracle-compression}

\newpage

\tableofcontents

\newpage

% ==============================================================================
% ==============================================================================
%
% START OF CONTENTS
%
% ==============================================================================
% ==============================================================================

\section{Introduction}
\subsection{Motivation}
- adaptive
- need for compression in a lot of new areas for which existing techniques might
not work well
lightfield cameras, 360 images / video, VR, video streams
- good handcrafted codecs are hard to design
- put the problem in a well-grounded mathematical framework, get theoretical guarantees

\subsubsection{Why Image Compression?}
\paragraph{}
Well studied problem, good literature availability, on handcrafted methods, NN
based methods and performance evaluation.

\subsubsection{Why Lossy?}
\paragraph{}
In lossless image compression we are limited by the true distribution of image
pixels in how much we can compress stuff.

In lossy compression we can make a huge saving, by only concentrating on details
that are perceptually important to the viewer.

\subsection{Our Goals}

Several metrics to optimise for:
- compression quality
- compression size
- compression time
- compressor size
- compressor power consumption
- robustness of compressor (i.e. resistance to errors / adversarial attacks)
- security / privacy of compression
- scalability: image size, image quality

\subsection{Our Contributions}
\paragraph{}

We present:
\begin{itemize}
\item a brief introduction to the technical background on neural-network
  based lossy image compression
\item a review of recent influential works in the area

\item a novel image compression algorithm trained on the CLIC 2018 dataset \cite{clic2018}

\item experiments and analysis of performance to confirm its theoretical properties
\end{itemize}

\subsection{Thesis Outline}
\paragraph{}

\section{Background}
\subsection{Image Compression}
Main goal: use the statistical / topological properties of images and properties
of the human visual system (HVS) to achieve
better compression rates than generic data compression algorithms

Types of image compression: lossless and lossy

lossy is suited for \textbf{natural images} -> imperceptible loss of quality can
lead to dramatic reduction in bit-rate

rate-distorsion
pnsr
\paragraph{}
The general setup for lossy image compression has two key, competing components:
the \textbf{distorsion} and the \textbf{rate} of the compressor. Given a space
of images $\ImSpace$, we select a distance metric $d(\cdot, \cdot): \ImSpace
\times \ImSpace \rightarrow \Reals^+$. Then, the distorsion of an image $\vec{x}
\in \ImSpace$ is defined as $d(\vec{x}, \hat{\vec{x}})$, where $\hat{\vec{x}}$
is the reconstruction of the image by the compression algorithm. The rate $R$
is the number of bits required to code $\vec{x}$.
\paragraph{}
It should 

\cite{psnr}
\cite{msssim}
\subsection{Metrics}
\paragraph{}
\subsubsection{PSNR}
\subsubsection{MS-SSIM}
\subsection{The Bits-Back Argument}
\paragraph{}
The bits-back argument was first introduced in \cite{hinton1993keeping}, and states
the following. Imagine the following communication problem: Given two parties,
Alice and Bob, Alice wants to send a sample $\vec{x}$ according to some
distribution 

\subsection{Neural Networks for Image Compression}
\paragraph{}

Why not compress images straight away?
- intractable due to high dimensionality
- ineffective due to spatial and cross-channel dependencies

solution:
- map into smaller dimensional representation and reduce dependencies between dimensions

\section{Related Work}
\paragraph{}
There have been several recent advances in neural network-based compression
techniques, most notably \cite{balle2016end}, \cite{theis2017lossy},
\cite{rippel2017real}, \cite{balle2018variational}, \cite{johnston2018cvpr},
\cite{mentzer2018cvpr} 
\paragraph{}
Notably, all previous VAE-based approaches have addressed the
non-differentiablility of quantization indirectly.
\paragraph{}
\cite{theis2017lossy} use an approximation for the derivative of the rounding
operator and optimize an upper bound on the error term introduced by the
quantiztion.
\paragraph{}
In \cite{balle2016end},\cite{balle2018variational} they model the quantizer by
adding uniform noise to the samples 
\subsection{VAE-based image compression}
\section{Method}

\subsection{Architecture}
\subsubsection{VAEs}
\paragraph{}
Unlike earlier work, since we do not require the quantization step at all, we
can train a classical VAE. On the data

\subsubsection{Hierarchical VAE}
\paragraph{}

\subsubsection{Probabilistic Ladder Network}
\paragraph{}

\subsubsection{Two-Stage VAE}
\paragraph{}
\cite{dai2019diagnosing}

\subsection{MIRACLE Coding}
\paragraph{}
\cite{havasi2018minimal}

\subsubsection{Rejection sampling}
\paragraph{}
\cite{harsha2007communication}

\subsection{Arithmetic Coding}
\paragraph{}
ac reference \cite{rissanen1981universal}

\section{Experimental Results}

\cite{zhao2015loss}
\section{Discussion}
\section{Conclusion}

\printbibliography
\end{document}