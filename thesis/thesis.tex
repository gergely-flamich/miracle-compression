
% ==============================================================================
% ==============================================================================
%
% HEADER
%
% ==============================================================================
% ==============================================================================

\documentclass{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphicx}

\usepackage[
backend=biber,
%style=alphabetic,
%citestyle=authoryear
]{biblatex}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{float}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algpseudocode}


\usepackage[a4paper, total={6in, 8in}]{geometry}


% ==============================================================================
% ==============================================================================
%
% DEFINITIONS
%
% ==============================================================================
% ==============================================================================
\addbibresource{cite.bib}


\renewcommand{\vec}[1]{\mathbf{#1}}
\renewcommand{\d}{\,\text{d}}


\newcommand{\A}{\mathcal{A}}
\newcommand{\E}{\mathcal{E}}
\newcommand{\F}{\mathcal{F}}
\renewcommand{\L}{\mathcal{L}}
\newcommand{\X}{\mathcal{X}}

\newcommand{\Enc}{\mathtt{Enc}}
\newcommand{\Dec}{\mathtt{Dec}}

\newcommand{\Oh}{\mathcal{O}}
\newcommand{\KL}[2]{\mathrm{KL}[\,#1\,\,||\,\,#2\,]}
\newcommand{\Exp}{\mathbb{E}}
\newcommand{\I}{\mathbb{I}}

\newcommand{\Hypos}{\mathcal{H}}
\newcommand{\Data}{\mathcal{D}}

\newcommand{\ImSpace}{\mathcal{X}}

\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Ints}{\mathbb{Z}}
\newcommand{\Nats}{\mathbb{N}}

\newcommand{\Norm}{\mathcal{N}}
\newcommand{\Unif}{\mathcal{U}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}


\title{Compression without Quantization}
\author{Gergely Flamich}

% ==============================================================================
% ==============================================================================
%
% START OF THESIS
%
% ==============================================================================
% ==============================================================================

\begin{document}

\input{titlepage.tex}

% ==============================================================================
%
% DECLARATION
%
% ==============================================================================

\vspace{2cm}

\begin{center}
\Huge
\textbf{Declaration}
\end{center}

\vspace{1cm}

\large
\noindent I, Gergely Flamich of St John's College, being a candidate for the
MPhil in Machine Learning and Machine Intelligence, hereby declare that this
report and the work described in it are my own work, unaided except as may be
specified below, and that the report does not contain material that has
already been used to any substantial extent for a comparable purpose.


\vspace{2cm}

\large
\noindent
Wordcount: \textbf{16384} words

\newpage

% ==============================================================================
%
% ACKNOWLEDGEMENTS
%
% ==============================================================================

\vspace{2cm}

\begin{center}
\Huge
\textbf{Acknowledgements}
\end{center}

\vspace{1cm}



\newpage

% ==============================================================================
%
% ABSTRACT
%
% ==============================================================================

\begin{abstract}
  We provide an implementation of our proposed method written in \texttt{Tensorflow}
  \cite{tensorflow2015-whitepaper} and \texttt{Sonnet} \cite{sonnetblog},
  available on GitHub \footnotemark.
\end{abstract}

\footnotetext{https://github.com/gergely-flamich/miracle-compression}

\newpage

\tableofcontents

\newpage

% ==============================================================================
% ==============================================================================
%
% START OF CONTENTS
%
% ==============================================================================
% ==============================================================================

\section{Introduction}

High level description of every section

\subsection{Motivation}
- adaptive
- need for compression in a lot of new areas for which existing techniques might
not work well
lightfield cameras, 360 images / video, VR, video streams
- good handcrafted codecs are hard to design
- put the problem in a well-grounded mathematical framework, get theoretical guarantees

\subsubsection{Why Image Compression?}
\paragraph{}
Well studied problem, good literature availability, on handcrafted methods, NN
based methods and performance evaluation.

\subsubsection{Why Lossy?}
\paragraph{}
In lossless image compression we are limited by the true distribution of image
pixels in how much we can compress stuff.

In lossy compression we can make a huge saving, by only concentrating on details
that are perceptually important to the viewer.

\subsection{Our Goals}

Several metrics to optimise for:
- compression quality
- compression size
- compression time
- compressor size
- compressor power consumption
- robustness of compressor (i.e. resistance to errors / adversarial attacks)
- security / privacy of compression
- scalability: image size, image quality

\subsection{Our Contributions}
\paragraph{}

We present:
\begin{itemize}
\item a brief introduction to the technical background on neural-network
  based lossy image compression
\item a review of recent influential works in the area

\item a novel image compression algorithm trained on the CLIC 2018 dataset \cite{clic2018}

\item experiments and analysis of performance to confirm its theoretical properties
\end{itemize}

\subsection{Thesis Outline}
\paragraph{}

\section{Background}
\par
As compression is not a standard topic in machine learning, it will be useful to
first spend some time establishing the main concepts and familiarize ourselves
with the jargon. We first go through some notation that will be used throughout
this work. Then we go through a brief introduction to compression in general,
and lossy compression and transform coding in particular. Third, we examine the
motivating line of research to our project, the MDL framework, the bits-back
argument and MIRACLE. Although our work is inspired by and based on earlier
work, it differs from them in several significant ways. We will point out these
differences throughout.
\subsection{Notation and Basic Concepts}
\paragraph{}
It will be useful to clarify some of the notation throughout this work.
\begin{itemize}
\item Vectors will be denoted by boldface lowercase letters: $\vec{u}, \vec{x}, ...$
\item Matrices will be denoted by uppercase letters: $A, M, ...$
\item Probability mass functions will be denoted by uppercase letters: $P(x),
  Q(z), ...$
\item Probability density functions will be denoted by lowercase letters: $p(y),
  q(u), ...$
\item $\Exp_{p(x)}[f(x)]$ denotes the expected value of $f(x)$ with respect to
  the mass / density $p(x)$, i.e.:
  \[
    \Exp_{p(x)}[f(x)] = \int_\Omega f(x) \d p(x),
  \]
  where $\Omega$ is the sample space. As $\Omega$ will usually denote $\Reals^n$
  or will be understood from context, it will be omitted, and the integral will
  be rewritten as
  \[
    \Exp_{p(x)}[f(x)] = \int f(x)p(x) \d x.
  \]
\item $H[X]$ denotes the Shannon entropy of the random variable $X$. If $X$ is
  discrete, then it is defined as
  \[
    -\sum_{X=x}P(X=x)\log P(X=x).
  \]
  If it is continuous, then it will refer to the \textit{differential entropy}
  of $X$, namely
  \[
    -\int_{\X}\log p(x) \d p(x),
  \]
  where $\X$ denotes the support of $X$.
  \paragraph{Note:} we used the natural logarithm in our definition of entropy,
  and hence its units are \textbf{nats}. If we used the base 2 logarithm
  instead, the units would be \textbf{bits}.
\item $\KL{q(x)}{p(x)}$ denotes the Kullback-Leibler divergence between two
  distributions and is defined as
  \[
    \KL{q(x)}{p(x)} = \Exp_{q(x)}\left[\log\frac{q(x)}{p(x)}\right].
  \]
\item $I[X : Y]$ denotes the mutual information between random variables $X$ and
  $Y$ and is defined as
  \[
    I[X : Y] = \KL{p(x, y)}{p(x)p(y)},
  \]
  where $(X, Y) \sim p(x, y)$ and $p(x)$ and $p(y)$ denote the marginals.
\end{itemize}
\subsection{Image Compression}
\paragraph{Source Coding}
From a theoretical point of view, given some source $S$, a sender and a
receiver, compression may be described as the aim of the sender communicating an
arbitrary sequence $X_1, X_2, \hdots, X_n$ taken from $S$ to the receiver in as few bits
as possible such that the receiver may recover relevant information from the message.
If the receiver can always recover all the information from the message of the sender, we
call the algorithm \textbf{lossless}, otherwise we call it \textbf{lossy}. 
\par
At first it might seem non-sensical to allow for lossy compression, and in some
domains this is definitely true, e.g. in text compression. However, 
human's audio-visual perception is neither completely aligned with the range of
what can be digitally represented, nor does it always scale the same way. Hence,
there is a huge opportunity for compressing media in a lossy way by discarding
information with the change being imperceptible for a human observer, while
making huge gains in size reduction.
\paragraph{Lossy Compression}
As the medium of interest in lossy compression is generally assumed to be a
real-valued vectory $\vec{x} \in \Reals^N$, such as RGB pixel intensities in an
image or frequency coefficients in an audio file, the usual pipeline consists of 
an encoder $C \circ \Enc$ map a point $\vec{x} \in \Reals^N$ to a string of bits and a
decoder mapping from bitstrings to reconstruction $\vec{\hat{x}}$. The
factors of the encoder $\Enc$ and $C$ can be understood as a map from $\Reals^N$ to a
finite symbol set $\A$, called a \textbf{lossy encoder} and a map from $\A$ to a
string of bits called a \textbf{lossless code} \cite{goyal2001theoretical}.
We will examine both $\Enc$ and $C$ in more detail shortly. The decoder then can be
thought of as inverting the code first and then using an approximate inverse of
$\Enc$ to get the reconstruction $\vec{\hat{x}}$: $\Dec \circ C^{-1}$.
\par
It is important to be able to quantify
\begin{itemize}
\item the \textbf{distorsion} of the compressor: on average, how closely does
  $\vec{\hat{x}}$ resemble $\vec{x}$?
\item the \textbf{rate} of the compressor: on average, how many bits are
  required to communicate $\vec{x}$? We want this to be as low as possible of course.
\end{itemize}

\paragraph{Distorsion}
In order to measure ``closeness'' in the space of interest $\ImSpace$,
a distance metric $d(\cdot, \cdot): \ImSpace
\times \ImSpace \rightarrow \Reals$ is introduced. Then, the distortion $D$ is 
is defined as
\[
  D = \Exp_{p(\vec{\hat{x}})}[d(\vec{x}, \vec{\hat{x}})].
\]
A popular choice of $d$, across many domains of compression is the normalized $L_2$ metric
or MSE, defined as
\[
  d(\vec{x}, \vec{\hat{x}}) = \frac{1}{N} \sum_{i}^N (x_i - \hat{x}_i)^2, \quad
  \ImSpace = \Reals^N.
\]
It is a popular metric as it is simple, easy to implement and has nice
interpretations in both a Bayesian \cite{bishop2013pattern} and the MDL
(\cite{hinton1993keeping}, to be introduced in Section \ref{sec:mdl}) settings.
In the image compression setting, however, the MSE is problematic, since it is
optimizing for such metric does not necessarily translate to obtaining pleasant-looking
reconstructions \cite{zhao2015loss}, and hence more appropriate, so-called \textit{perceptual
  metrics} were developed. The ones relevant to our discussion are Peak
Signal-to-Noise Ratio (PSNR) \cite{psnr}, \cite{gupta2011modified} and the
Structural Similarity Index (SSIM) \cite{wang2004image} and its multiscale
version (MS-SSIM) \cite{msssim}. Crucially, these
two metrics are not only the most popular, but are also differentiable, which
means they lend themselves for gradient-based optimization.
PSNR is not a good metric \cite{girod1993s} \cite{eskicioglu1994image}.
% TODO --------------------------------------------------
\textbf{TODO: More discussion on metrics}

\paragraph{Rate}
We noted above that the code used after the lossy encoder is lossless. To
further elaborate, in virtually all cases it is an \textbf{entropy code}
\cite{goyal2001theoretical}. This means that we assume that each symbol
in the representation $\vec{z} = \Enc(\vec{x})$ has some probability mass
$P(z_i)$. A fundamental result by Shannon states that $\vec{z}$ may not be
encoded losslessly in fewer than $H[\vec{z}]$ nats
\cite{shannon1998mathematical}. Entropy codes, such as Huffman codes
\cite{huffman1952method} or Arithmetic Coding \cite{rissanen1981universal} can
get very close to this lower bound. We will coding methods further in Section
\ref{sec:mir_coding}. The rate (in nats) of the compression algorithm is defined
as the average number of nats required to code a single dimension of the input, i.e.
\[
  R = \frac{1}{N} H[\vec{z}].
\]
\paragraph{Transform Coding}
The issue with source coding is that coding $\vec{x}$ might have a lot of
dependencies across its dimensions. For images, this manifests on multiple
scales and semantic levels, e.g. a pixel being blue might indicate that most
pixels around it are blue as the scene is depicting the sky or a body of water;
a portrait of a face will also imply that eyes, a nose and mouth are probably
present, etc. Modelling and coding this dependence structure in very high
dimensions is highly non-trivial or perhaps even impossible, and hence we need
to make simplifying assumptions about it to proceed.
\par
\textit{Transform coding} attempts to solve the above problem by decomposing the
encoder function $\Enc = Q \circ T$ into a so-called \textbf{analysis transform}
$T$ and a \textbf{quantizer} $Q$. The idea is that to transform the input into a
domain, such that the dependencies between the dimensions are removed, and hence
they can be coded individually. The decoder inverts the steps of the encoder,
where the inverse operation of $T$ is called the \textbf{synthesis transform}
\cite{gupta2011modified}.
\par
In \textit{linear transform coding}, $T$ is an invertible linear trasformation,
such as a discrete cosine transformation (DCT), as it is in the case of JPEG
\cite{wallace1992jpeg}, or discrete wavelet transforms in JPEG 2000
\cite{rabbani2002overview}. While simple, fast and elegant, linear transform
coding has the key limitation that it can only at most remove correlations (i.e.
first-order dependencies), and this can severly limit its efficiency
\cite{balle2016endtrans}. Instead, \cite{balle2016endtrans} propose a method for
\textit{non-linear transform coding}, where $T$ is replaced by a highly
non-linear transformation, and its inverse is now replaced by an approximate
inverse, which is a separate non-linear transformation. Both $T$ and its
approximate inverse are learnt, and the authors show that with a more
complicated transformation they can easily surpass the performance of the much
more fine-tuned JPEG codecs.
\par
Our work also falls into this line of research, although with signifcant
differences, which will be pointed out later.
\subsection{The MDL principle and the Bits-Back Argument}
\label{sec:mdl}
\paragraph{MDL Principle} 
Our approach is based on the Minimum Description Length (MDL) Principle
\cite{rissanen1986stochastic}. In essence, it is a formalization of Occam's
Razor, i.e. the simplest model that describes the data well is the best model of
the data \cite{gr√ºnwald2007minimum}. Here, ``simple'' and ``well'' need to be
defined, and these definitions are precisely what the MDL principle gives us.
Informally, it asserts that given a class of hypotheses $\Hypos$ (e.g. a certain
statistical model and its parameters) and some data $\Data$, if a particular
hypothesis $H \in \Hypos$ can be described with at most $L(H)$ bits and the using the
hypothesis the data can be described with at most $L(\Data \mid H)$ bits, then the
minimum description length of the data is
\begin{equation}
\label{eq:min_desc_princ}
  L = \min_{H \in \Hypos}\{ L(H) + L(\Data \mid H) \},
\end{equation}
and the best hypothesis is the $H$ that minimizes the above quantity.
\par
Crucially, the MDL principle can thus be interpreted as telling us that
\textbf{the best model of the data is the one that compresses it the most}.
This makes Eq \ref{eq:min_desc_princ} a very appealing learning objective for
optimization-based compression methods, ours included.
Below, we briefly review how this has been applied so far and how it translates
to our case.
\paragraph{Bits-Back Argument}
First, we begin with the bits-back argument, introduced in
\cite{hinton1993keeping}, which is a direct application of the above. The main
goal of this work was to develop a regularisation technique for neural networks
by framing the training of a neural network as a communication problem, where
the training input and the fixed network architecture is public, but the weights,
the network's output given a particular input, and the training targets are
only available to the sender, and the task is to communicate the
\textit{training targets} with minimal bits.
\par
Concretely, they train a Bayesian Neural Network, by equipping the weights
$\vec{w}$ with a prior $p_\theta(\vec{w})$ and a posterior $q_\phi(\vec{w})$
(parameterized by $\theta$ and $\phi$, respectively) and maximize
the \textit{evidence lower bound} (ELBO) given a likelihood $p(\Data \mid \vec{w})$:
\begin{equation}
  \label{eq:elbo_target}
  \Exp_{q_\phi}[\log p(\Data \mid \vec{w})] - \KL{q_{\phi}}{p_{\theta}}.
\end{equation}
Given a sufficiently finely quantized likelihood, the minimum description length
of the data given this model is $\Exp_{q_\phi}[-\log p(\Data \mid \vec{w})]$
\cite{shannon1998mathematical}, and hence the first term in Eq
\ref{eq:elbo_target} corresponds to $-L(\Data \mid H)$. In their work then,
\cite{hinton1993keeping} show that the second term in Eq \ref{eq:elbo_target}
is equal to $-L(H)$, which establishes a link between the variational training
objective of the BNN and the MDL principle.
\par
To do this, the encoder
\begin{enumerate}
\item trains the neural network, optimising Eq \ref{eq:elbo_target}.
\item draws a random sample $\vec{\hat{w}} \sim q_{\phi}(\vec{w})$. This
  represents a message of $\Exp_{q_\theta}[- \log q_\phi]$ nats.
\item $\vec{\hat{w}}$ is then used to calculate the residuals $\vec{r}$ between
  the network's output and the targets.
\item $\vec{r}$ is coded with $\vec{\hat{w}}$ and then $\vec{\hat{w}}$ is coded
  using its prior $p_\theta$. The total length of the message is hence $\Exp_{q_\phi}[-\log
  p(\Data \mid \vec{\hat{w}})] + \Exp_{q_\phi}[-\log p_{\theta}]$.
\end{enumerate}
Once everything has been communicated, the decoder can recover the true training
targets, but then they can also run the same training algorithm that the encoder
used to then recover the posterior $q_\phi$. This means that the code of
$\vec{\hat{w}}$ is ``free bits'' in the sense that the decoder can recover them
exactly given what they already have. 
Hence, the whole cost of drawing $\vec{\hat{w}}$ should be
subtracted from the original cost, yielding
$\Exp_{q_\phi}[-\log p_{\theta}] - \Exp_{q_\phi}[-\log q_{\phi}] =
\KL{q_{\phi}}{p_\theta}$ nats. This ``recovery'' is the namesake for the
bits-back argument.
\paragraph{MIRACLE}
Inspired by the above idea, \cite{havasi2018minimal} asked a natural question:
\textit{is it possible to communicate only the weights of a network at
  bits-back efficiency?}
\par
If the above were true, it would give a method for compressing neural networks
rather efficiently. It is clear that the coding must be different than it was in
\cite{hinton1993keeping}, as their method focused on the regularisation aspect
of the KL-divergence and is very inefficient for actual communication of the
model parameters.
\par
A second, important question that arises in conjunction with the first, natural
for compression algorithms:
\textit{is it possible trade off accuracy of a fixed neural network architecture
  for better compression rates, and vice versa?}
\par
Luckily, the answer to both of the above questions is yes, and we shall begin by
addressing the latter first. Fix a network architecture, and some data
likelihood given a weight set $p(\Data \mid \vec{\hat{w}})$. Akin to
\cite{hinton1993keeping}, we will actually train a BNN with weight prior
$p(\vec{w})$ and posterior $q(\vec{w})$. Then, given a budget of $C$ nats, we
hope to maximize the following constrained objective:
\begin{equation}
\label{eq:miracle_hard_train_target}
\Exp_{q_\phi}[\log p(\Data \mid \vec{w})] \quad \text{subject to }
\KL{q_{\phi}}{p_{\theta}} < C.
\end{equation}
We can rewrite Eq \ref{eq:miracle_hard_train_target} as its Lagranagian
relaxation under the KKT conditions \cite{karush2014minima},
\cite{kuhn2014nonlinear}, \cite{higgins2017beta} and get:
\[
  \F(\theta, \phi, \beta, \Data, \vec{\hat{w}}) = 
  \Exp_{q_\phi}[\log p(\Data \mid \vec{\hat{w}})] - \beta (\KL{q_{\phi}}{p_{\theta}} - C).
\]
By the KKT conditions if $C \geq 0$ then $\beta \geq 0$, hence discarding the last
term in the above equation will provide a lower bound for it:
\begin{equation}
\label{eq:miracle_train_target}
\F(\theta, \phi, \beta, \Data, \vec{\hat{w}}) \geq
\L(\theta, \phi, \beta \Data, \vec{\hat{w}}) =
\Exp_{q_\phi}[\log p(\Data \mid \vec{\hat{w}})] - \beta \KL{q_{\phi}}{p_{\theta}}.
\end{equation}
Notice, that this is the same as Eq \ref{eq:elbo_target}, but with the addition of
the parameter $\beta$ that will control the regularisation term and eventually
the compression cost of the weights. It is also intimately related to the
training target of $\beta$-VAEs \cite{higgins2017beta}, except for where they
regularise the distributions of activations on a stochastic layer, here the
regularisation is for the distributions of weights.
\par
Now, to answer the first question, we first need to establish the right setting
for the task, which will be another communications problem.
Concretely, given a dataset $\Data$ sampled from a distribution $p(D)$, and
$q_\phi(\vec{w})$, our trained weight posterior for a given $\beta$, what are
the bounds on the minimum description length for the posterior $L(q_{\phi})$?
\par
Under some mild assumptions, it can be shown \cite{harsha2007communication} that
in fact
\[
  \Exp_{p(D)}[L(q_{\phi})] \geq \Exp_{p(D)}[\KL{q_{\phi}}{p_{\theta}}],
\]
i.e. in this probabilistic setting bits-back efficiency is the best we can hope
for. Now, if we make the further assumption that the sender and the receiver are
allowed to \textit{share a source of randomness} (e.g. a random number generator
and a seed for it), then a rather tight upper bound can also be derived, also
due to \cite{harsha2007communication}:
\begin{equation}
\label{eq:miracle_ub}
  \Exp_{p(D)}[L(q_{\phi})] \leq I[\Data : \vec{w}] + 2 \log \left( I[\Data :
    \vec{w}] + 1 \right) + \Oh(1)
\end{equation}
where $I[D : \vec{w}] = \Exp_{p(D)}[\KL{q_{\phi}}{p_{\theta}}]$ is the
mutual information between the distribution of datasets and the weights.
\par
Eq \ref{eq:miracle_ub} is proven by exposing an algorithm that achieves the
postualted coding efficiency, an adaptive rejection sampling algorithm, which we
detail in the Appendix A. This turns out to be infeasible in the case of
MIRACLE, and instead the authors propose an importance sampling-based
approximate sampling algorithm, which is also discussed in further detail in
Appendix A. They are important, as we have used both in our project.

\paragraph{Our method} Our project is based upon the simple observation, that
the MIRACLE framework may be utilised for compression of any data where in our
model a public prior distribution $p_{\theta}$ and a learned $q_{\phi}$ is
available and we are allowed to share a source of randomness. We have already
noted the extreme similarity of the original BNN training objective to that of
the $\beta$-VAE \cite{higgins2017beta}, and indeed they are precisely the model
that we shall use for the compression of images in our case.

\section{Related Work}
\par
Here we give a brief overview of the history of using machine learning for image
compression. Then, we focus on recent advances in lossy image compression and
describe and compare their methods to each other as well as ours.
\subsection{Machine Learning-based Image Compression}
\par

First attempt by \cite{bottou1998high}

\subsection{Comparison of Recent Works}
\label{sec:lit_comparison}
\par

There have been several recent advances in neural network-based compression
techniques, most notably \cite{toderici2015variable}, \cite{balle2016end},
\cite{toderici2017full}, \cite{theis2017lossy}, \cite{rippel2017real},
\cite{balle2018variational}, \cite{johnston2018cvpr}, \cite{mentzer2018cvpr}. 
An interesting commonality between these approaches is that there is very little
commonality between them, for a multidue of reasons, on which we hope to shed
some lite in this section. Instead of analyzing them in a historical order, we
will instead go through the compression pipeline and compare them head-to-head
in each compartment separately. 
\subsubsection{Datasets and Input Pipelines}
\par
Somewhat surprisingly it appears that there is no canonical dataset (yet) for
the task at hand, namely a set of high-resolution, variable-sized losslessly
encoded colour images,
although CLIC \cite{clic2018} seems to be an emerging one. Perhaps the reason is
that generally in other domains, such as image-based classification cropping and
rescaling images can effectively side-step the need to deal with variable-sized
images. However, when it comes to compression, if we hope to build anything
useful, we must account for this.
\par
On the other hand most authors have used the Kodak dataset \cite{kodakdataset}
for testing / reporting results.
\begin{itemize}
\item \cite{toderici2015variable} trained on 216 million $32 \times 32$ pixel
  colour images, however, they offer little detail about how this dataset was
  obtained. Since their architecture was created with compressing only
  $32 \times 32$ in mind, there is no further preprocessing.
\item \cite{balle2016end} trained on 6507 images, selected from ImageNet
  \cite{deng2009imagenet}. They removed images with excessive saturation and
  since their method is based on dithering, they added unifrom noise to the
  remaining images to imitate the noise introduced by quantization. Finally,
  they downsampled and cropped images to be $256 \times 256$ pixels in size.
  They only kept images whise resampling factor was 0.75 or less, in order to
  avoid high frequency noise.
\item \cite{toderici2017full}
  used two datasets, first the one they described in \cite{toderici2015variable}
  and the second one (they call the ``High Entropy'' dataset) was created by
  first scraping 6 million images from the web, then resizing them to $1280
  \times 720$ pixels. Then, they decomposed these into $32 \times 32$ pixel
  tiles and selected the 100 tiles from each with the worst compression ratio under
  the PNG algorithm.
\item \cite{theis2017lossy} used 434 high resolution images from \url{flickr.com}
  under the creative commons license. As \texttt{flickr} store its images as
  JPEGs, they downsampled all images to be below $1536 \times 1536$ in
  resolution and saved them as PNGs in order to reduce the effects of the lossy
  compression. Then, they extracted several $128 \times 128$ patches from each
  image and trained on those.
\item \cite{rippel2017real} took images from the Yahoo Flickr Creative Commons
  100 Million dataset, with $128 \times 128$ patches randomly sampled from the
  images. They do not state whether they used the whole dataset or just a
  subset, neither do they describe further preprocessing steps.
\item \cite{balle2018variational} scraped $\approx$ 1 million colour JPEG images
  of dimensions at most $3000 \times 5000$. They filtered out images with
  excessive saturation similarly to \cite{balle2016end}. They also
  downsampled images by random factors such that the image's height and width
  stayed above 640 and 1200 pixels, respectively. Finally, they use several
  randomly cropped $256 \times 256$ pixel patches extracted from each image.
\item \cite{johnston2018cvpr} similarly to the ``High Entropy'' dataset in
  \cite{toderici2017full}, they scrape 6 million JPEG images from the web and
  extract $128 \times 128$ patches from them to train on.
\item \cite{mentzer2018cvpr} train on the ILSVRC2012 ImageNet dataset 
  \cite{russakovsky2015imagenet}. They take extract $160 \times 160$ pixel image
  patches and randomly flipped them.
\end{itemize}

\subsubsection{Architectures}
\par
This is the most diverse aspect of recent approaches, and so we will only
discuss them on a very high level. We took inspiration from most of these papers
as well as others, these will be emphasized in Section \ref{sec:our_method}.

\subsubsection{Quantization}
\par

\subsubsection{Coding}
\par

\subsubsection{Training}
\par

\subsubsection{Evaluation}
\par

Notably, all previous VAE-based approaches have addressed the
non-differentiablility of quantization indirectly.
\paragraph{}
\cite{theis2017lossy} use an approximation for the derivative of the rounding
operator and optimize an upper bound on the error term introduced by the
quantiztion.
\paragraph{}
In \cite{balle2016end},\cite{balle2018variational} they model the quantizer by
adding uniform noise to the samples 
\subsection{VAE-based image compression}

\cite{rippel2017real} use GANs \cite{goodfellow2014generative}
GDN \cite{balle2015density} \cite{ioffe2015batch}
\section{Method}
\label{sec:our_method}
\par
In this section we describe the models we have tried. We present it in a similar
way as the layout of Section \ref{sec:lit_comparison}. Within a section we
persent ideas we tried in chronological order

Crucially, the rate term in our optimization objective is different from
previous approaches, in that in previous approaches the distribution of the
codewords was estimated separately. In our case, however, it is an integral part
of the optimization process.

In \cite{theis2017lossy}, they demonstrate that it is precisely the quantization
step that introduces the noisy artifacts into the image, and not the
reconstruction procedure. This is fundamentally different from our case, where
the majority of the quality degradation comes from the fact that VAEs generally
produce blurry reconstructions.
\subsection{Architecture}
\subsubsection{VAEs}
\paragraph{}
Unlike earlier work, since we do not require the quantization step at all, we
can train a classical VAE. On the data

\subsubsection{Hierarchical VAE}
\paragraph{}

\subsubsection{Probabilistic Ladder Network}
\paragraph{}

\subsubsection{Two-Stage VAE}
\paragraph{}
\cite{dai2019diagnosing}

\subsection{MIRACLE Coding}
\label{sec:mir_coding}
\paragraph{}
\cite{havasi2018minimal}

\subsubsection{Rejection sampling}
\paragraph{}
\cite{harsha2007communication}

\subsection{Arithmetic Coding}
\paragraph{}
ac reference \cite{rissanen1981universal}

\section{Experimental Results}

\cite{zhao2015loss}
\section{Discussion}
\section{Conclusion}

\cite{townsend2019practical}
\printbibliography

\newpage

\section*{Appendix A: Sampling algorithms}
\subsection*{Rejection Sampling}
\par
The rejection sampling algorithm presented here is due to
\cite{harsha2007communication}.

\begin{algorithm}
  \caption{Rejection sampling presented in \cite{harsha2007communication}.}
  \label{alg:harsha_rej_sampling}
  \begin{algorithmic}[1]
    \Procedure{Rej-Sampler}{$P, Q, \langle x_i \sim Q \mid i \in \Nats \rangle$}
    \Comment $P$ is the prior
    \Statex
    \Comment $Q$ is the posterior
    \Statex
    \Comment $x_i$ are i.i.d. samples from $Q$
    \State $p_0(x) \gets 0 \quad \forall x \in \X$.
    \State $p_0^* \gets 0$.
    \For{$i \gets 1, \hdots \infty$}

    \State
    $\alpha_i(x) \gets \min{P(x) - p_{i - 1}(x), (1 - p_{i - 1}^*)Q(x)}\quad
    \forall x \in \X$

    \State $p_i(x) \gets p_{i - 1}(x) + \alpha_i(x)$
    
    \State $p_i^* \gets \sum_{x \in \X}p_i(x)$

    \State $\beta_i(x_i) \gets \frac{\alpha_i(x)}{(1 - p_i^*)Q(x)}$

    \State Draw $u \sim \Unif(0, 1)$

    \Statex

    \If{$u < \beta_i(x_i)$}

    \State\Return $i, x_i$

    \EndIf
    
    \EndFor
    \EndProcedure
  \end{algorithmic}
\end{algorithm}
\subsection*{Importance Sampling}
\section*{Appendix B: Images}
\end{document}