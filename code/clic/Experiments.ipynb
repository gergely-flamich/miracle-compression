{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Imports\n",
    "# ==============================================================================\n",
    "\n",
    "# This is needed so that python finds the utils\n",
    "import sys\n",
    "sys.path.append(\"/home/gf332/miracle-compession/code\")\n",
    "\n",
    "from imageio import imwrite\n",
    "\n",
    "import argparse\n",
    "import os, glob\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Needed for compression as the common source of randomness\n",
    "from sobol_seq import i4_sobol_generate\n",
    "from scipy.stats import norm\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "tfe = tf.contrib.eager\n",
    "tfs = tf.contrib.summary\n",
    "tfs_logger = tfs.record_summaries_every_n_global_steps\n",
    "\n",
    "from architectures import ClicCNN\n",
    "from utils import is_valid_file, setup_eager_checkpoints_and_restore\n",
    "from load_data import load_and_process_image, create_random_crops, download_process_and_load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Predefined Stuff\n",
    "# ==============================================================================\n",
    "\n",
    "models = {\n",
    "    \"cnn\": ClicCNN\n",
    "}\n",
    "\n",
    "\n",
    "optimizers = {\n",
    "    \"sgd\": tf.train.GradientDescentOptimizer,\n",
    "    \"momentum\": lambda lr:\n",
    "                    tf.train.MomentumOptimizer(learning_rate=lr,\n",
    "                                               momentum=0.9,\n",
    "                                               use_nesterov=False),\n",
    "    \"adam\": tf.train.AdamOptimizer,\n",
    "    \"rmsprop\": tf.train.RMSPropOptimizer\n",
    "}\n",
    "\n",
    "# ==============================================================================\n",
    "# Auxiliary Functions\n",
    "# ==============================================================================\n",
    "def clic_input_fn(dataset, buffer_size=2000, batch_size=8):\n",
    "    dataset = dataset.shuffle(buffer_size)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(1)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(config_path=None,\n",
    "        model_key=\"cnn\",\n",
    "        is_training=True,\n",
    "        model_dir=\"/tmp/clic_test\"):\n",
    "\n",
    "    # ==========================================================================\n",
    "    # Configuration\n",
    "    # ==========================================================================\n",
    "\n",
    "    config = {\n",
    "        \"training_set_size\": 93085,\n",
    "\n",
    "        \"batch_size\": 8,\n",
    "        \"num_epochs\": 20,\n",
    "\n",
    "        \"loss\": \"neg_elbo\",\n",
    "        \"beta\": 0.1,\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"optimizer\": \"adam\",\n",
    "\n",
    "        \"log_freq\": 200,\n",
    "        \"checkpoint_name\": \"_ckpt\",\n",
    "    }\n",
    "\n",
    "    if config_path is not None:\n",
    "        config = json.load(config_path)\n",
    "\n",
    "    num_batches = config[\"training_set_size\"] // config[\"batch_size\"]\n",
    "\n",
    "\n",
    "    print(\"Configuration:\")\n",
    "    print(json.dumps(config, indent=4, sort_keys=True))\n",
    "    print(\"Num batches: {}\".format(num_batches))\n",
    "\n",
    "    # ==========================================================================\n",
    "    # Load dataset\n",
    "    # ==========================================================================\n",
    "\n",
    "    train_dataset, valid_dataset = download_process_and_load_data()\n",
    "    \n",
    "    train_dataset = clic_input_fn(train_dataset,\n",
    "                                  batch_size=config[\"batch_size\"])\n",
    "\n",
    "    # ==========================================================================\n",
    "    # Create VAE model\n",
    "    # ==========================================================================\n",
    "\n",
    "    model = models[model_key]\n",
    "\n",
    "    vae = model(prior=\"laplace\")\n",
    "\n",
    "    # Connect the model computational graph by executing a forward-pass\n",
    "    vae(tf.zeros((1, 256, 256, 3)))\n",
    "\n",
    "    optimizer = optimizers[config[\"optimizer\"]](config[\"learning_rate\"])\n",
    "\n",
    "    # ==========================================================================\n",
    "    # Define Checkpoints\n",
    "    # ==========================================================================\n",
    "\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "    trainable_vars = vae.get_all_variables() + (global_step,)\n",
    "    checkpoint_dir = os.path.join(model_dir, \"checkpoints\")\n",
    "\n",
    "    checkpoint, ckpt_prefix = setup_eager_checkpoints_and_restore(\n",
    "        variables=trainable_vars,\n",
    "        checkpoint_dir=checkpoint_dir,\n",
    "        checkpoint_name=config[\"checkpoint_name\"])\n",
    "\n",
    "    # ==========================================================================\n",
    "    # Tensorboard stuff\n",
    "    # ==========================================================================\n",
    "\n",
    "    logdir = os.path.join(model_dir, \"log\")\n",
    "    writer = tfs.create_file_writer(logdir)\n",
    "    writer.set_as_default()\n",
    "\n",
    "    # ==========================================================================\n",
    "    # Train VAE\n",
    "    # ==========================================================================\n",
    "\n",
    "    beta = config[\"beta\"]\n",
    "\n",
    "    if is_training:\n",
    "\n",
    "        for epoch in range(1, config[\"num_epochs\"] + 1):\n",
    "\n",
    "            with tqdm(total=num_batches) as pbar:\n",
    "                for batch in train_dataset:\n",
    "\n",
    "                    # Increment global step\n",
    "                    global_step.assign_add(1)\n",
    "\n",
    "                    with tf.GradientTape() as tape, tfs_logger(config[\"log_freq\"]):\n",
    "\n",
    "                        # Predict the means of the pixels\n",
    "                        output = vae(batch)\n",
    "\n",
    "                        log_prob = vae.log_prob\n",
    "                        kl_div = vae.kl_divergence\n",
    "\n",
    "                        if config[\"loss\"] == \"neg_elbo\":\n",
    "                            # Cross-entropy / MSE loss (depends on )\n",
    "                            B = batch.shape.as_list()[0]\n",
    "                            loss = (-log_prob + beta * kl_div) / B\n",
    "\n",
    "                        elif config[\"loss\"] == \"psnr_kl\":\n",
    "                            # PSNR: the Gaussian negative log prob is the MSE\n",
    "                            psnr = 20 * tf.log(config[\"max_pixel_value\"]) - 10 * tf.log(-log_prob)\n",
    "\n",
    "                            loss = -psnr + beta * kl_div\n",
    "\n",
    "                        else:\n",
    "                            raise Exception(\"Loss {} not available!\".format(config[\"loss\"]))\n",
    "\n",
    "                        output = tf.cast(output, tf.float32)\n",
    "\n",
    "                        # Add tensorboard summaries\n",
    "                        tfs.scalar(\"loss\", loss)\n",
    "                        tfs.image(\"Reconstruction\", output)\n",
    "\n",
    "                    # Backprop\n",
    "                    grads = tape.gradient(loss, vae.get_all_variables())\n",
    "                    optimizer.apply_gradients(zip(grads, vae.get_all_variables()))\n",
    "\n",
    "                    # Update the progress bar\n",
    "                    pbar.update(1)\n",
    "                    pbar.set_description(\"Epoch {}, Loss: {:.2f}, KL: {:.2f}, Log Prob: {:.4f}\".format(epoch, loss, kl_div, log_prob))\n",
    "\n",
    "            checkpoint.save(ckpt_prefix)\n",
    "\n",
    "    else:\n",
    "        print(\"Skipping training!\")\n",
    "\n",
    "\n",
    "    # ==========================================================================\n",
    "    # Compress images\n",
    "    # ==========================================================================\n",
    "\n",
    "\n",
    "    print(vae.encode(tf.convert_to_tensor(test_data[:1, ...] / 255., dtype=tf.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "{\n",
      "    \"batch_size\": 8,\n",
      "    \"beta\": 0.1,\n",
      "    \"checkpoint_name\": \"_ckpt\",\n",
      "    \"learning_rate\": 0.0001,\n",
      "    \"log_freq\": 200,\n",
      "    \"loss\": \"neg_elbo\",\n",
      "    \"num_epochs\": 20,\n",
      "    \"optimizer\": \"adam\",\n",
      "    \"training_set_size\": 93085\n",
      "}\n",
      "Num batches: 11635\n",
      "Processing Training Data!\n",
      "Data already processed!\n",
      "Processing Validation Data!\n",
      "Data already processed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/11635 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: nan, KL: nan, Log Prob: nan:  51%|█████     | 5931/11635 [40:11<36:31,  2.60it/s]                                                         \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-b12f72f49e00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cnn\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     model_dir=\"/tmp/clic_test\")\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-f34278b55131>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(config_path, model_key, is_training, model_dir)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                     \u001b[0;31m# Backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m    944\u001b[0m         \u001b[0mflat_sources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m     70\u001b[0m       \u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_Conv2DBackpropInputGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m     59\u001b[0m           \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"padding\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m           data_format=op.get_attr(\"data_format\"))\n\u001b[0m\u001b[1;32m     62\u001b[0m   ]\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, data_format, dilations, name)\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strides\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m         \u001b[0;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"padding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 987\u001b[0;31m         \"data_format\", data_format, \"dilations\", dilations)\n\u001b[0m\u001b[1;32m    988\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run(config_path=None,\n",
    "    model_key=\"cnn\",\n",
    "    is_training=True,\n",
    "    model_dir=\"/tmp/clic_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
