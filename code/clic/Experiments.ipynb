{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Imports\n",
    "# ==============================================================================\n",
    "\n",
    "# This is needed so that python finds the utils\n",
    "import sys\n",
    "sys.path.append(\"/home/gf332/miracle-compession/code\")\n",
    "\n",
    "from imageio import imwrite\n",
    "\n",
    "import argparse\n",
    "import os, glob\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Needed for compression as the common source of randomness\n",
    "from sobol_seq import i4_sobol_generate\n",
    "from scipy.stats import norm\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "tfe = tf.contrib.eager\n",
    "tfs = tf.contrib.summary\n",
    "tfs_logger = tfs.record_summaries_every_n_global_steps\n",
    "\n",
    "from architectures import ClicCNN\n",
    "from utils import is_valid_file, setup_eager_checkpoints_and_restore\n",
    "from load_data import load_and_process_image, create_random_crops, download_process_and_load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Predefined Stuff\n",
    "# ==============================================================================\n",
    "\n",
    "models = {\n",
    "    \"cnn\": ClicCNN\n",
    "}\n",
    "\n",
    "\n",
    "optimizers = {\n",
    "    \"sgd\": tf.train.GradientDescentOptimizer,\n",
    "    \"momentum\": lambda lr:\n",
    "                    tf.train.MomentumOptimizer(learning_rate=lr,\n",
    "                                               momentum=0.9,\n",
    "                                               use_nesterov=False),\n",
    "    \"adam\": tf.train.AdamOptimizer,\n",
    "    \"rmsprop\": tf.train.RMSPropOptimizer\n",
    "}\n",
    "\n",
    "# ==============================================================================\n",
    "# Auxiliary Functions\n",
    "# ==============================================================================\n",
    "def clic_input_fn(dataset, buffer_size=2000, batch_size=8):\n",
    "    dataset = dataset.shuffle(buffer_size)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(1)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(config_path=None,\n",
    "        model_key=\"cnn\",\n",
    "        is_training=True,\n",
    "        model_dir=\"/tmp/clic_test\"):\n",
    "\n",
    "    # ==========================================================================\n",
    "    # Configuration\n",
    "    # ==========================================================================\n",
    "\n",
    "    config = {\n",
    "        \"training_set_size\": 93085,\n",
    "\n",
    "        \"batch_size\": 8,\n",
    "        \"num_epochs\": 20,\n",
    "\n",
    "        \"loss\": \"neg_elbo\",\n",
    "        \"beta\": 0.1,\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"optimizer\": \"adam\",\n",
    "\n",
    "        \"log_freq\": 200,\n",
    "        \"checkpoint_name\": \"_ckpt\",\n",
    "    }\n",
    "\n",
    "    if config_path is not None:\n",
    "        config = json.load(config_path)\n",
    "\n",
    "    num_batches = config[\"training_set_size\"] // config[\"batch_size\"]\n",
    "\n",
    "\n",
    "    print(\"Configuration:\")\n",
    "    print(json.dumps(config, indent=4, sort_keys=True))\n",
    "    print(\"Num batches: {}\".format(num_batches))\n",
    "\n",
    "    # ==========================================================================\n",
    "    # Load dataset\n",
    "    # ==========================================================================\n",
    "\n",
    "    train_dataset, valid_dataset = download_process_and_load_data()\n",
    "    \n",
    "    train_dataset = clic_input_fn(train_dataset,\n",
    "                                  batch_size=config[\"batch_size\"])\n",
    "\n",
    "    # ==========================================================================\n",
    "    # Create VAE model\n",
    "    # ==========================================================================\n",
    "\n",
    "    model = models[model_key]\n",
    "\n",
    "    vae = model(prior=\"laplace\")\n",
    "\n",
    "    # Connect the model computational graph by executing a forward-pass\n",
    "    vae(tf.zeros((1, 256, 256, 3)))\n",
    "\n",
    "    optimizer = optimizers[config[\"optimizer\"]](config[\"learning_rate\"])\n",
    "\n",
    "    # ==========================================================================\n",
    "    # Define Checkpoints\n",
    "    # ==========================================================================\n",
    "\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "    trainable_vars = vae.get_all_variables() + (global_step,)\n",
    "    checkpoint_dir = os.path.join(model_dir, \"checkpoints\")\n",
    "\n",
    "    checkpoint, ckpt_prefix = setup_eager_checkpoints_and_restore(\n",
    "        variables=trainable_vars,\n",
    "        checkpoint_dir=checkpoint_dir,\n",
    "        checkpoint_name=config[\"checkpoint_name\"])\n",
    "\n",
    "    # ==========================================================================\n",
    "    # Tensorboard stuff\n",
    "    # ==========================================================================\n",
    "\n",
    "    logdir = os.path.join(model_dir, \"log\")\n",
    "    writer = tfs.create_file_writer(logdir)\n",
    "    writer.set_as_default()\n",
    "\n",
    "    # ==========================================================================\n",
    "    # Train VAE\n",
    "    # ==========================================================================\n",
    "\n",
    "    beta = config[\"beta\"]\n",
    "\n",
    "    if is_training:\n",
    "\n",
    "        for epoch in range(1, config[\"num_epochs\"] + 1):\n",
    "\n",
    "            with tqdm(total=num_batches) as pbar:\n",
    "                for batch in train_dataset:\n",
    "\n",
    "                    # Increment global step\n",
    "                    global_step.assign_add(1)\n",
    "\n",
    "                    with tf.GradientTape() as tape, tfs_logger(config[\"log_freq\"]):\n",
    "\n",
    "                        # Predict the means of the pixels\n",
    "                        output = vae(batch)\n",
    "\n",
    "                        log_prob = vae.log_prob\n",
    "                        kl_div = vae.kl_divergence\n",
    "\n",
    "                        if config[\"loss\"] == \"neg_elbo\":\n",
    "                            # Cross-entropy / MSE loss (depends on )\n",
    "                            B = batch.shape.as_list()[0]\n",
    "                            loss = (-log_prob + beta * kl_div) / B\n",
    "\n",
    "                        elif config[\"loss\"] == \"psnr_kl\":\n",
    "                            # PSNR: the Gaussian negative log prob is the MSE\n",
    "                            psnr = 20 * tf.log(config[\"max_pixel_value\"]) - 10 * tf.log(-log_prob)\n",
    "\n",
    "                            loss = -psnr + beta * kl_div\n",
    "\n",
    "                        else:\n",
    "                            raise Exception(\"Loss {} not available!\".format(config[\"loss\"]))\n",
    "\n",
    "                        output = tf.cast(output, tf.float32)\n",
    "\n",
    "                        # Add tensorboard summaries\n",
    "                        tfs.scalar(\"loss\", loss)\n",
    "                        tfs.image(\"Reconstruction\", output)\n",
    "\n",
    "                    # Backprop\n",
    "                    grads = tape.gradient(loss, vae.get_all_variables())\n",
    "                    optimizer.apply_gradients(zip(grads, vae.get_all_variables()))\n",
    "\n",
    "                    # Update the progress bar\n",
    "                    pbar.update(1)\n",
    "                    pbar.set_description(\"Epoch {}, Loss: {:.2f}, KL: {:.2f}, Log Prob: {:.4f}\".format(epoch, loss, kl_div, log_prob))\n",
    "\n",
    "            checkpoint.save(ckpt_prefix)\n",
    "\n",
    "    else:\n",
    "        print(\"Skipping training!\")\n",
    "\n",
    "\n",
    "    # ==========================================================================\n",
    "    # Compress images\n",
    "    # ==========================================================================\n",
    "\n",
    "\n",
    "    print(vae.encode(tf.convert_to_tensor(test_data[:1, ...] / 255., dtype=tf.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "{\n",
      "    \"batch_size\": 8,\n",
      "    \"beta\": 0.1,\n",
      "    \"checkpoint_name\": \"_ckpt\",\n",
      "    \"learning_rate\": 0.0001,\n",
      "    \"log_freq\": 200,\n",
      "    \"loss\": \"neg_elbo\",\n",
      "    \"num_epochs\": 20,\n",
      "    \"optimizer\": \"adam\",\n",
      "    \"training_set_size\": 93085\n",
      "}\n",
      "Num batches: 11635\n",
      "Processing Training Data!\n",
      "Data already processed!\n",
      "Processing Validation Data!\n",
      "Data already processed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/11635 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 181650.75, KL: 21319.67, Log Prob: -1451074.0000:  15%|█▍        | 1689/11635 [11:32<1:07:10,  2.47it/s]"
     ]
    }
   ],
   "source": [
    "run(config_path=None,\n",
    "    model_key=\"cnn\",\n",
    "    is_training=True,\n",
    "    model_dir=\"/tmp/clic_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
