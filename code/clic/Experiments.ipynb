{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Imports\n",
    "# ==============================================================================\n",
    "\n",
    "# This is needed so that python finds the utils\n",
    "import sys\n",
    "sys.path.append(\"/Users/gergelyflamich/Documents/Work/MLMI/miracle-compession/code\")\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Needed for compression as the common source of randomness\n",
    "from sobol_seq import i4_sobol_generate\n",
    "from scipy.stats import norm\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "tfe = tf.contrib.eager\n",
    "tfs = tf.contrib.summary\n",
    "tfs_logger = tfs.record_summaries_every_n_global_steps\n",
    "\n",
    "from architectures import ClicCNN\n",
    "from utils import is_valid_file, setup_eager_checkpoints_and_restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Predefined Stuff\n",
    "# ==============================================================================\n",
    "\n",
    "models = {\n",
    "    \"cnn\": ClicCNN\n",
    "}\n",
    "\n",
    "\n",
    "optimizers = {\n",
    "    \"sgd\": tf.train.GradientDescentOptimizer,\n",
    "    \"momentum\": lambda lr:\n",
    "                    tf.train.MomentumOptimizer(learning_rate=lr,\n",
    "                                               momentum=0.9,\n",
    "                                               use_nesterov=False),\n",
    "    \"adam\": tf.train.AdamOptimizer,\n",
    "    \"rmsprop\": tf.train.RMSPropOptimizer\n",
    "}\n",
    "\n",
    "# ==============================================================================\n",
    "# Auxiliary Functions\n",
    "# ==============================================================================\n",
    "def mnist_input_fn(data, batch_size=128, shuffle_samples=5000):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "    dataset = dataset.shuffle(shuffle_samples)\n",
    "    dataset = dataset.map(mnist_binary_parse_fn)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def mnist_binary_parse_fn(data, normalizing_const=255.):\n",
    "    return tf.cast(data, tf.float32) / normalizing_const"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(config_path=None,\n",
    "        model_key=\"cnn\",\n",
    "        is_training=True,\n",
    "        model_dir=\"/tmp/clic_test\"):\n",
    "\n",
    "    # ==========================================================================\n",
    "    # Configuration\n",
    "    # ==========================================================================\n",
    "\n",
    "    config = {\n",
    "        \"training_set_size\": 60000,\n",
    "        \"max_pixel_value\": 1.,\n",
    "\n",
    "        \"num_latents\": 40,\n",
    "        \"hidden_units\": 300,\n",
    "        \"data_likelihood\": \"gaussian\",\n",
    "\n",
    "        \"batch_size\": 128,\n",
    "        \"num_epochs\": 20,\n",
    "\n",
    "        \"loss\": \"neg_elbo\",\n",
    "        \"beta\": 1,\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"optimizer\": \"momentum\",\n",
    "\n",
    "        \"log_freq\": 250,\n",
    "        \"checkpoint_name\": \"_ckpt\",\n",
    "    }\n",
    "\n",
    "    if config_path is not None:\n",
    "        config = json.load(config_path)\n",
    "\n",
    "    num_batches = config[\"training_set_size\"] // config[\"batch_size\"]\n",
    "\n",
    "\n",
    "    print(\"Configuration:\")\n",
    "    print(json.dumps(config, indent=4, sort_keys=True))\n",
    "    print(\"Num batches: {}\".format(num_batches))\n",
    "\n",
    "    # ==========================================================================\n",
    "    # Load dataset\n",
    "    # ==========================================================================\n",
    "\n",
    "    ((train_data, _),\n",
    "    (test_data, _)) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "    train_dataset = mnist_input_fn(train_data[:num_batches * config[\"batch_size\"]],\n",
    "                                   batch_size=config[\"batch_size\"])\n",
    "\n",
    "    # ==========================================================================\n",
    "    # Create VAE model\n",
    "    # ==========================================================================\n",
    "\n",
    "    model = models[model_key]\n",
    "\n",
    "    vae = model(hidden_units=config[\"hidden_units\"],\n",
    "                num_latents=config[\"num_latents\"],\n",
    "                data_likelihood=config[\"data_likelihood\"])\n",
    "\n",
    "    # Connect the model computational graph by executing a forward-pass\n",
    "    vae(tf.zeros((1, 28, 28)))\n",
    "\n",
    "    optimizer = optimizers[config[\"optimizer\"]](config[\"learning_rate\"])\n",
    "\n",
    "    # ==========================================================================\n",
    "    # Define Checkpoints\n",
    "    # ==========================================================================\n",
    "\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "    trainable_vars = vae.get_all_variables() + (global_step,)\n",
    "    checkpoint_dir = os.path.join(model_dir, \"checkpoints\")\n",
    "\n",
    "    checkpoint, ckpt_prefix = setup_eager_checkpoints_and_restore(\n",
    "        variables=trainable_vars,\n",
    "        checkpoint_dir=checkpoint_dir,\n",
    "        checkpoint_name=config[\"checkpoint_name\"])\n",
    "\n",
    "    # ==========================================================================\n",
    "    # Tensorboard stuff\n",
    "    # ==========================================================================\n",
    "\n",
    "    logdir = os.path.join(model_dir, \"log\")\n",
    "    writer = tfs.create_file_writer(logdir)\n",
    "    writer.set_as_default()\n",
    "\n",
    "    # ==========================================================================\n",
    "    # Train VAE\n",
    "    # ==========================================================================\n",
    "\n",
    "    beta = config[\"beta\"]\n",
    "\n",
    "    if is_training:\n",
    "\n",
    "        for epoch in range(1, config[\"num_epochs\"] + 1):\n",
    "\n",
    "            with tqdm(total=num_batches) as pbar:\n",
    "                for batch in train_dataset:\n",
    "\n",
    "                    # Increment global step\n",
    "                    global_step.assign_add(1)\n",
    "\n",
    "                    with tf.GradientTape() as tape, tfs_logger(config[\"log_freq\"]):\n",
    "\n",
    "                        # Predict the means of the pixels\n",
    "                        output = vae(batch)\n",
    "\n",
    "                        log_prob = vae.log_prob\n",
    "                        kl_div = vae.kl_divergence\n",
    "\n",
    "                        if config[\"loss\"] == \"neg_elbo\":\n",
    "                            # Cross-entropy / MSE loss (depends on )\n",
    "                            B = batch.shape.as_list()[0]\n",
    "                            loss = -log_prob + beta * kl_div\n",
    "\n",
    "                        elif config[\"loss\"] == \"psnr_kl\":\n",
    "                            # PSNR: the Gaussian negative log prob is the MSE\n",
    "                            psnr = 20 * tf.log(config[\"max_pixel_value\"]) - 10 * tf.log(-log_prob)\n",
    "\n",
    "                            loss = -psnr + beta * kl_div\n",
    "\n",
    "                        else:\n",
    "                            raise Exception(\"Loss {} not available!\".format(config[\"loss\"]))\n",
    "\n",
    "                        output = tf.cast(tf.expand_dims(output, axis=-1), tf.float32)\n",
    "\n",
    "                        # Add tensorboard summaries\n",
    "                        tfs.scalar(\"loss\", loss)\n",
    "                        tfs.image(\"Reconstruction\", output)\n",
    "\n",
    "                    # Backprop\n",
    "                    grads = tape.gradient(loss, vae.get_all_variables())\n",
    "                    optimizer.apply_gradients(zip(grads, vae.get_all_variables()))\n",
    "\n",
    "                    # Update the progress bar\n",
    "                    pbar.update(1)\n",
    "                    pbar.set_description(\"Epoch {}, Loss: {:.2f}, KL: {:.2f}, Log Prob: {:.4f}\".format(epoch, loss, kl_div, log_prob))\n",
    "\n",
    "            checkpoint.save(ckpt_prefix)\n",
    "\n",
    "    else:\n",
    "        print(\"Skipping training!\")\n",
    "\n",
    "\n",
    "    # ==========================================================================\n",
    "    # Compress images\n",
    "    # ==========================================================================\n",
    "\n",
    "\n",
    "    print(vae.encode(tf.convert_to_tensor(test_data[:1, ...] / 255., dtype=tf.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(config=None,\n",
    "    model=\"cnn\",\n",
    "    is_training=True,\n",
    "    model_dir=\"/tmp/clic_test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
