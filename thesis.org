#+TITLE: Thesis Stuff

Azure: https://portal.azure.com/#blade/Microsoft_Azure_Education

* Needs work [1/4]
** TODO Implement different losses [3/5]
*** DONE Bernoulli + KL for binary MNIST
    CLOSED: [2019-05-19 Sun 14:22]
*** DONE MSE + KL
    CLOSED: [2019-05-19 Sun 14:21]
*** DONE PSNR + KL
    CLOSED: [2019-05-19 Sun 14:21]
*** TODO VGG
*** TODO GAN + VGG?


** DONE Implement (basic) rejection sampling
   CLOSED: [2019-05-19 Sun 14:20]

** TODO Implement block-based RS?

** TODO Upscale stuff


* Questions [7/10]
** DONE What is the equivalent of the block constraint here?
   CLOSED: [2019-05-31 Fri 10:52]
   - Is there a connection to conditional sampling?
** DONE How much data should we be training on? Good datasets?   
   CLOSED: [2019-05-31 Fri 11:52]
   CC images from FLICKR -> downsample them save as png
   narrow by camera type lens type.

** DONE Is there a reason to further experiment with loss functions?
   CLOSED: [2019-05-31 Fri 12:03]
   How about VGG loss / L1 / L2 only?
   Just use L2 for comparison
   Use GAN for nice pictures

** DONE Is there a good justification for Laplace latent distributions?
   CLOSED: [2019-06-13 Thu 17:55]
** DONE Good image reconstruction activations? Is GDN good?
   CLOSED: [2019-05-31 Fri 11:45]

** TODO Is there a way to get rid of the edge artifacts?
** DONE Use VALID or mirrored SAME padding?
   CLOSED: [2019-06-13 Thu 17:55]
** TODO Using stacked CNN layers instead of one big one?
** TODO Residual CNN architecture?

** DONE Is a Laplace prior justified or should we attempt to use the non-informative one?
   CLOSED: [2019-06-13 Thu 17:55]

     
* How to choose the beta
** report the pareto frontier
** Gaussianise the latent space
** Decorrelate latent representation
** autoregressive structure on Gaussianized network
** Try YUV

   
* Things to do next
** 

* Ideas
** Denoising VAEs -> compress similar images to the same latent representation?

** Permute quarters of the image and add them up?
   do something fancier to connect disconnected parts of the image?

* A* sampling

* Issues [1/4]
** DONE Check edges of images -> use valid padding instead of same maybe?
   CLOSED: [2019-06-05 Wed 14:51]

** TODO A* sampling log difference requires the sufficient statistics of the target distribution for the search to work!
   - Combine Arithmetic coding with A* sampling?
   - find 2^-i wide interval and resample
** TODO A* Log difference for normals can be concave or linear
   - use scale mixture priors instead?
** TODO Vanishing mass on truncated regions


* Papers [2/9]
** TODO Deep Feature Consistent Variational Autoencoder (Hou et al.)
   [[file:papers/deep_feature_consistent_vae.pdf][Paper]] 
** TODO Denoising Criterion for VAEs (Im et al.)
   [[file:papers/denoising_vaes.pdf][Paper]]
** TODO End-to-end Optimised Image Compression (Balle et al.)
   [[file:papers/ete_image_compression.pdf][Paper]]
** DONE How to Train Deep Variational Autoencoders and Probabilistic Ladder Networks (Sonderby et al.)
   CLOSED: [2019-05-19 Sun 14:36]
   [[file:papers/how_to_train_vaes.pdf][Paper]]
** DONE Loss Functions for Image Restoration with Neural Networks (Zhao et al.)
   CLOSED: [2019-05-19 Sun 14:36]
   [[file:papers/nn_img_loss_fns.pdf][Paper]]
** TODO Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network
   [[file:papers/srgans_mssim.pdf][Paper]]
** TODO VAE-GANs for Probabilistic Compressive Image Recovery: Uncertainty Analysis (Edupuganti et al.)
   [[file:papers/vae_gans.pdf][Paper]]
** TODO Variational Autoencoder for Low Bit-rate Image Compression (CLIC 2018 winner)
   [[file:papers/clic2018_winner.pdf][CLIC 2018 Winner]]

** TODO Variational Image Compression with Scale Hyperprior (Balle et al.)
   [[file:papers/var_comp_with_hyperprior.pdf][Paper]]


* Meeting on 20 May [3/4]
** DONE What is the appropriate way to train the network?
   CLOSED: [2019-05-20 Mon 15:56]
   - In [[file:papers/clic2018_winner.pdf][clic winner]] and [[file:papers/var_comp_with_hyperprior.pdf][Balle 2018]] they suggest downloading stuff from Flickr and using 256x256 cutouts
   *Answer*: just try what's in the paper
** DONE How to make the network work for arbitrary image sizes?
   CLOSED: [2019-05-20 Mon 15:56]
   Would a purely CNN + ResNet based architecture work?
   *Answer*: yes, a pure cnn + resnet is probably the answer
** DONE Is -PSNR + KL the appropriate loss to use?
   CLOSED: [2019-05-20 Mon 15:56]
   - Criticised in [[file:papers/nn_img_loss_fns.pdf][here]]
   *Answer*: just use MSE (Gaussian likelihood)

* Meeting on 27 May

  https://en.wikipedia.org/wiki/Elias_delta_coding
  http://brahma.tcs.tifr.res.in/~prahladh/papers/HJMR/HJMR2010.pdf
* Meeting on 3 June

* Meeting on 10 June

* Meeting on 17 June / Industry day


* Poster & Presentation
** Stuff to say
   - Lossy image compression using hieararchical VAE:
   - Principled:
     - Can recast former VAE-esque methods using quantization as using a mixture of dirac deltas for coding
     - Using a technique similar to the original MIRACLE paper, use a continuous coding distribution -> can be reparametrised
   - Efficient: 
     - encoding length can be bounded by KL between decoding and encoding distribution
   - Differentiable:
     - Since we do away the only non-differentiable part of the architecture, we can now train end-to-end just like a regular VAE with the ELBO
   
   - Fully convolutional architecture based on previous architecture by Johannes Balle

   - Train on CLIC 2018 dataset

** Stuff in poster
  Titles:
   - Introduction
   - Coding
   - Architecture
   - Results
   - Conclusion


   

* Building the dataset
** Find popular cameras, search for those
   - sony a7 mark3
   - canon eos 5d
** Exposure triangle

* Industry day talk

Hi my name is Greg, and I am working on compression without quantization.
In particular, I am working on a novel image compression algorithm.

The 3 key "selling points" of our approach is that it is principled, 
efficient and differentiable.

Principled, as our approach is using the mathematical framework of information
theory. In particular, it gives us direct control over the the compression rate,
and it also gives upper bound on how badly the algorithm might do in the worst 
case for given settings.

Our method is efficient, because the upper bound it quite close to the theoretical lower bound 
in our framework.

Finally differentiable is nice, as it makes it easy to train using standard optimizers. This makes
training compression architectures widely available.

To illustrate, here's a comparison between our method and JPEG. 

73 KB is as 
The original image size was 1.8 MB



* Things to do [/]
** 

* Penultimate meeting with Marton [4/23]:

** DONE put everything in the camb phd thesis template
   CLOSED: [2019-08-08 Thu 20:59]
** DONE Change Section headers to Chapters
   CLOSED: [2019-08-08 Thu 20:59]
** TODO write everything in present tense
** TODO Check out LaTeX notation section
** TODO Reduce / cut background section
** DONE More discussion on perceptual metrics not needed
   CLOSED: [2019-08-08 Thu 21:02]
** TODO Write out Shannon's source coding theorem
** TODO Citation rule of thumb: Author Year
** TODO Expand bits back argument, maybe make extra graphic
*** Note: We can't get the actual bits back. 
   We can encode an auxiliary message into the random bits
** TODO Change MIRACLE section to be about quantizationless  
   coding: go from bits-back as argument to harsha rejection
** TODO Small Introductiong description paragraph
** TODO Related Works: these are the works that we are comparing,  
   as there are no standard techniques or datasets or bencchmarks developed yet
** TODO Get architecture / reconstruction images from related works
** TODO More general discussion of related work sections
** TODO Differentiate Quantization methods by name
** TODO Emphasize problems with Quantization
** TODO Training section in Related works needs more general discussion
** TODO Evaluation Section is good for explaining perceptual metric formualae 
/ what they do.
** TODO Write out abbreviations: VAE, MSE
** TODO Find better notation for laplace - ELBO loss mathcal L problem
** DONE WRite first level second level into the graph, 
   CLOSED: [2019-08-08 Thu 21:04]
** TODO To reduce clutter, paramtericze normals with stdev
** TODO Condition on sample, not the RV

